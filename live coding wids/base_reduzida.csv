id_trabalho,traducao,keyword
1206,"The purpose of this poster is to present instruments for inference for mixture models with spatial dependence between variables using MV and/or Bayesian methodology. Among the aspects to be addressed is the selection of models, estimation via MCMC, as well as performance verification. Simulation studies will be carried out to verify the performance of the proposed methods and comparison with methods proposed in the literature. The proposed methods will also be applied to real data",Bayesian inference; spatial data; mixing distributions; MCMC
1157,"Partially replicated designs, or p-rep designs, are widely used in experiments in plant breeding programs, whose main objective is to maximize genetic gain. Thus, a quality selection of the evaluated genetic material is necessary. Such designs were originally proposed as solvable designs in rows and columns, being obtained through optimal designs according to the maximization of the expected genetic gain, which is equivalent to minimizing the variance of pairs of errors in the prediction of treatment effects. The present study evaluated the influence of the effect of treatments, whether fixed or random, and the design characteristic, whether solvable or non-resolvable, on the selection of genetic material. The results show that the p-rep designs are robust, and the genetic gain is little influenced by such assumptions made regarding the design model, for a fixed size of experiment.",Line and column designs; Spatial optimized designs; Mean of the variances of the differences between treatment effects pairs; Solvable designs; Genetic gain.
1348,"Currently, there are several analytical queuing models that are often used to represent the queuing systems at supermarket checkouts. The main objective is to estimate the waiting time of customers in the checkout line, important information for making management decisions. A very common occurrence, not only in supermarkets, is the exchange of lines by users. There are few models that consider switching and, when they do, they are for few queues (small queue systems). These should be compared, in efficiency in predicting waiting time in line, to other models that are frequently used. To analyze and compare the performance of these models, a case study was carried out in a company in the interior of São Paulo. With the information and data from the case study, it was possible to compare the results of the model with changes in relation to other models used for predictions. As a result, it was found that the model that best represented the real system was the queuing model with exchanges. The results highlight that, possibly, the model with exchanges is more realistic and efficient for situations in which customers have this behavior. However, this model presents computational capacity problems when one intends to use it with a larger number of queues. This is due to the fact that, to add to the exchanges, this situation leads to a system of balance equations with tens of thousands of equations, which is not determined. This work then brings another possibility, an approach that allows, through the division into smaller blocks of parallel queues, to use larger queue systems composed of blocks, expanding the analysis capacity of the queue system with exchanges.","queuing model with changes, queuing limitation, system performance."
1104,"In this paper our interest relies on analyze the performance of undergraduate students in Calculus I and Linear Algebra and identify which characteristics may be associated with the chances of a student be approved in one of those disciplines. The database is from the University of Campinas, one of the top  Public Research Universities in Brazil, with records of 9,130 students who enrolled at the university between 2009 and 2015 in courses such that Calculus I or Linear Algebra were required disciplines. The variable of interest is the number of times a student take each one of those course until he/she is approved, treating this as discrete survival variable, some students may take 7 or more time the discipline until be approved.  We proposed a bivariate discrete survival model to analyses jointly the time until pass in both discipline taking account the strong correlation among them via a gaussian random component.","student performance, survival analysis, discrete time, censored data, mixed model"
1217,"In this work we introduce a dynamic class of model for time series taking values on the unit interval and following a beta distribution. The proposed model follows a generalized linear model approach where the mean specification includes covariates and also an extra additive term presenting chaotic behavior. The extra term is defined through the iteration of a chaotic transformation and can present a wide variety of behaviors including short and long range dependence, fixed points, periodic behavior, attracting and/or repelling points, etc.",chaotic processes; time series; beta regression
1133,"The present work was born from a proposal to carry out a pedagogical training workshop aimed at teachers at the Federal Institute of Basic, Technical and Technological Education of Brasília (IFB). Professionals from the most diverse specialties would participate, from engineers, chemists, physicists and mathematicians to biologists, historians, professionals of letters, administrators and pedagogues, a very diverse audience with diverse needs, but with a focus on continuing education and improving their practices. Six workshops were selected, five of which were based on pedagogical activity in the classroom and the last one, the present proposal, whose main objective was to disseminate to participants the possibilities of applying statistical tools and practices for both the evaluation and analysis of data. collected and for the improvement and comparison of learning processes in different situations. Raising important aspects of teachers&#39; pedagogical practice and associating them with statistical procedures and tools via the presentation of several examples of application in the school context, several issues were addressed such as appropriate profiles for assessments, grade scales based on position measures and variability, grading scales based on statistical distributions, the Hypothesis Test to compare means obtained from the same sample and to compare means from independent samples. The result was satisfactory considering the short period of time and the variability of interests and backgrounds involved. The teachers not only worked on different contents, but also saw new possibilities and got excited, that is, they were motivated to apply the methods presented in their research and in their daily professional practice.",Continuing Education. Statistics and Education. Qualitative and Quantitative Research.
840,"The data set of respondents to the complete questionnaire of the 2010 IBGE Demographic Census was considered, which involved 20800804 respondents. The influence of the fertility index was evaluated as a function of variables such as type of disability, expanded education level, income, race and type of work, through the application of a cross-sectional study and an analysis of crosses between number of children and federation unit depending on the other variables, the fertility index is calculated considering women over 10 years of age as fertile. Conclusion, it was possible to show that this index is usually higher in groups considered more vulnerable in terms of disability, level of education, housing conditions, greater distance from urban centers, among others.","fertility rate, people with disabilities, race, increased level of education, crossing of variables."
1227,"We introduce a new three-parameter model called the logistic Nadarajah-Haghighi distribution. The proposed distribution can produce an unimodal density overcoming a Nadarajah-Haghighi limitation. It is also more flexible than the baseline hazard rate function, allowing constant, increasing, decreasing and upsidedown-bathtub forms. We present the maximum likelihood estimators for the model parameters. A simulation study is carried out and we illustrate the usefulness of the new distribution by means of two applications to real data sets.",Hazard rate function; lifetime data; logistic-X family; maximum likelihood; Nadarajah-Haghighi distribution.
1251,"Economic assessment is an essential part of planning forestry projects. These assessments usually take into account the calculation of revenue, which is usually linked to wood production and its sale price. The Bayesian methodology can serve as an alternative to obtain better estimates of growth and production and also of price. This study aimed to carry out the economic evaluation of a forestry project using the Bayesian approach and extreme values theory (EVT). The starting point for the study was the definition of a project for planting eucalyptus to be implemented in the State of São Paulo. For the economic evaluation of the project, the net present value (NPV) was used. Since the economic performance of the project will be between the extremes where the price of wood can be the best or worst possible for the sale period, its revenues were calculated for two situations. The first considered the sale of wood at the maximum price expected for the final year of the project. In the second, the sale was made at the minimum price expected for the same period. Wood production was estimated by the non-linear Schumacher model. The minimum and maximum prices were estimated by the Gumbel distribution. Both production and price were estimated in the form of probability density via the Bayesian approach. As a result, an HPD95% region could be built for the NPV. Thus, the NPV calculated for the minimum price situation had 95% HPD varying between R$ 2,050.11 and R$ 5,409.07 per hectare, with an average equal to R$ 3,771.60 per hectare. The NPV calculated for the maximum price situation had 95% HPD ranging from R$ 7,766.77 to R$ 9,070.29 per hectare, with an average of R$ 8,398.13 per hectare.","Probability distribution, nonlinear model, eucalyptus, a priori information"
1121,"We consider the modeling of the interaction between a continuous and a binary regressor in a mixed effects model in which the nature of the relationship between the response variable and the continuous regressor is nonlinear and can be approximated by a curve. The representation of curves including low-order or high-order power terms for a regressor raises several well-known issues, such as bounded curve shapes or not fitting the data well due to extreme values. We propose an alternative approach based on the Fractional Polynomial Models (PFs) family, more specifically we extend the MFPI (Multivariable Fractional Polynomials Interaction) procedure to include random effects components. Therefore, a strategy for the analysis of fractional polynomial mixed effects models was proposed and a subroutine in R language was developed to calculate the deviances (deviation function) of these models and thus be able to select the best PF model. To illustrate and motivate our proposal we used a dataset analyzed by other authors who used polynomial mixed effects models, including an unstructured covariance matrix for random effects and modeling the error structure. In this case, in particular, the matrix associated with the random effects was generated by selecting certain columns from the matrix associated with the fixed parameters, that is, where the polynomial terms are. The analysis using the mixed effects fractional polynomial model proved to be a very parsimonious alternative, besides having the same advantages of the usual polynomials.",nothing
1027,"The study aims to describe the profiles of students entering the University of Brasília with a view to subsidizing performance monitoring and student assistance policies. The data refer to freshmen in 2016 extracted from the database of the survey “Students Profile at the University of Brasília – Registration Stage” conducted by the Observatório da Vida Estudantil since 2012. This survey has an estimated coverage of 95% of freshmen and collection sociodemographic data, previous school trajectory, admission to UnB, and future perspectives. With the help of the Grade of Membership – GoM method, three extreme profiles were identified with the following characteristics: 1) entry by quotas, blacks or browns, low income, high school in public schools, low prestige courses; 2) entering by quotas, black or brown, intermediate income, secondary education in public school, high prestige courses, and 3) entering by the universal system, white or yellow, high income, high school in private school, high prestige courses. It is concluded that even though the University of Brasília has expanded access to low-income students through the implementation of social and racial quotas, inequalities in access to more prestigious courses persist. Furthermore, data not shown in this survey indicate that inequalities in the permanence and completion of the intended course still challenge the academic trajectory of students in a more vulnerable situation.",gom; Grade of Membership; Universidade de Brasília; desigualdade
919,"This work presents kernel-based hard clustering methods for interval data, based on adaptive distances, which are obtained as sums of squared Euclidean distances between observations and prototypes, calculated individually for each variable and for each group, through functions of Kernel. The advantage of this approach over the conventional approach of Kernel-based clusters consists in assigning weights to the variables during the clustering process, in order to differentiate their relevance for the construction of clusters and consequently obtain better results.",Grouping; Kernel Functions; adaptive distances;
1255,"Companies involved as a defendant in a high number of lawsuits are known as major litigants, including banks and telephone service providers. Given the current large number of lawsuits in progress, this research aims to help the strategy used by the plaintiff, in order to reduce the amount and time of actions. The database used originates from the Consultation of First Degree Proceedings of the Court of Justice of São Paulo and is composed of judgments, approved in 2014, in the first instance of lawsuits filed against large litigants and is composed of 12,616 observations and 12 variables, the result of the action being the response variable. Therefore, predictive models are proposed, based on the multinomial distribution and on the support vector machine (SVM) with a Gaussian kernel. Models based on the multinomial distribution are fitted by the maximum likelihood method and the evaluation of relevant features is done by the likelihood ratio test. The results of the adjustment of this model showed that the company involved, jurisdiction, type of court and damage, indications of the presence of a defendant&#39;s lawyer, discussions about credit protection agencies, presence of the word &quot;third party&quot;, discussions about the relationship of consumption and respect of free legal aid are relevant characteristics for the outcome of the action. The model achieved 53% accuracy on the test basis. The method based on SVM, on the other hand, is built by the geometric interpretation of the problem, placing the observations in a hyperplane and maximizing the subspace margins of this hyperplane that better separate the observations. For this method, all the covariates in the database were considered and an accuracy of 54% was obtained in the test base. Both approaches presented similar performances, which indicates that the problem for stock classification is due to the characteristics of the data, which have very similar covariates for different results.",nothing
1355,"The present work presents some regularity conditions in the regression model with general parameterization to guarantee the asymptotic normality of the maximum likelihood estimators. A theorem is stated and some steps for proof are suggested. In order to exemplify, we consider violating one of the conditions considering a particular model, the heteroskedastic model with error in the variables and in the equation. A brief analytical and simulation study is presented, both of which indicate that the violation of the condition compromises the theoretical result presented in the Theorem.",Regression model with general parameterization; regularity conditions; asymptotic normality
1011,"In experiments based on randomization, simple random allocations can result in very different experimental groups regarding certain covariates. Furthermore, the use of classical control procedures is only feasible for a modest number of covariates. An answer to this problem is the re-randomization procedure, which consists of repeated randomizations to ensure that the final allocation obtained is balanced. However, despite the potential advantages of realignment, the number of realignments needed to maintain the desired balance grows exponentially with the number of covariates. In this work, we propose random intentional allocation, a method based on intentional allocation (to guarantee balanced groups with high probability) extended with random perturbations (to avoid systematic biases). Numerical experiments on real and simulated data show a remarkable superiority of random allocation over reallocation, both in terms of balance between groups and in terms of power of inference.",Random Sampling Realignment Experimental Planning
1012,"Intentional allocation methods are non-probabilistic procedures for the selection and allocation of individuals, with the aim of achieving representativeness and balancing criteria. Such an approach is suitable for exploratory research or pilot studies in which rarity of individuals, ethical or cost constraints severely limit sample sizes and prevent the adoption of traditional random sampling. In previous works, we presented the random intentional allocation, an allocation method based on the optimal balance of the convariables of interest, combined with random perturbations. In this work, we extend the random allocation approach and present a case study in software evaluation. Numerical experiments show that random allocation provides well-balanced experimental groups, even in the presence of a low number of participants.",Random Allocation Intentional Allocation Experiment Planning
1080,"The maximum likelihood method and the method of moments are commonly used for Gumbel parameter
estimation.The maximum likelihood estimator cannot be expressed in closed-form. Estimates are
typically obtained by numerically maximizing the log likelihood. An alternative estimation method
which is less well-known is the probability weighted moments method. The main goal of this paper
is to compare the performances of such estimation methods. To do this, we used Monte Carlo
simulation on Gumbel distribution and evaluate comparative measures based on average estimates,
average biases and average standard errors using parametric bootstrap replications.","Estimation methods, Probability weighted moments method, Maximum likelihood method, Method of moments, Monte Carlo simulation"
1144,"This study aimed to analyze the effect of brushing and isolation on the behavior of sheep. Data were collected in an experiment with 20 animals classified as reactive or not to social isolation and evaluated in 3 experimental sessions, and in 3 different moments: before, during and after brushing. The analysis was performed using Additive Generalized Models for Location, Scale and Shape (GAMLSS). Two random effects were included: animal and animal within session. The insertion of these effects is due to the need to incorporate the correlations between the measurements in the same animal and the animal in the same session. The analysis was performed using the R software, gamlss package, and the variable analyzed was the proportion of time the animal remained with its ears raised or asymmetrical, thus being a variable restricted to the interval [0,1]. The distributions considered were the beta and its inflated version, implemented in the gamlss package. The inflated beta distribution admits values equal to 0 or 1 for the response, unlike the original distribution. The parameter associated with inflation was also modeled as a function of experimental factors in the inflated beta distribution. The results showed that the effects of session, moment and interactions between session with moment and strain with moment were significant in the mean time, while in the probability of the animal remaining in the posture all the time, the significant variables were experimental session and strain. The diagnosis of the models, based on randomized quantile residues, showed that the model with an inflated beta distribution was the most suitable for the study.","Animal behavior, proportion data, random effects, beta distribution, inflated beta distribution"
1049,"The form of land occupation in rural areas, such as crops, pastures, reforestation and other areas, can be an indication of the productivity of the land production factor and the value of agricultural production. The products that make up the value of agricultural production have different land use due to their own productive characteristic. The general objective of this work was to measure the association between the value of production of groups of agricultural products with the different uses of the rural area in the production of municipalities in the state of São Paulo. In this research, 52 agricultural products were used, grouped into five variables of production value and nine variables of area use in the production of São Paulo municipalities that had production in this sector in 2008. The multivariate statistical technique of canonical correlation was used for to measure the association between the group of variables of production value products with the group of variables of land use in agricultural activities. It was concluded that there is a strong correlation (94.3%) in the first pair of canonical variables representing the value of production and use of the area, allowing the formation of groups of municipalities at different stages of development in agricultural production. It can be seen that 61.8% of the municipalities in the state were below the average in the production and area use group and that only 4.8% were above the average for the production variables group and with values below the average in use of the area.",Grouping. Canonical Correlation. Heterogeneity.
1218,"Music is one of the most important forms of art for human beings, especially for the Brazilian people. Brazilian music is very diverse as it is influenced by different cultures such as European, Indian, African and American. This work aims to verify which are the main words that differentiate each Brazilian musical style and which genres are most similar in relation to the lyrics of their songs. For this, a database of songs, singers and styles was set up using Web Scraping and text mining techniques were used in the lyrics of these songs, such as removing stop words and stemming. The lyrics of the songs were treated and grouped by eight different musical styles, each style was considered a document. For each word in each musical genre, the measure tf-idf (frequency of the inverse-term frequency in the documents) was calculated. Based on this measure, the cosine similarity was used in order to calculate the similarity between the styles. Finally, a hierarchical cluster analysis was performed using Ward&#39;s method to group these genres. Considering this measure of similarity, the most similar styles were funk and hip hop/rap while the least similarity occurred between gospel and funk genres. Regarding the analysis of clusters, we can separate these genres into four groups: axé, samba and mpb; funk and hip hop/rap; pagode and sertanejo and a group formed only by the gospel genre.",nothing
1029,"In the climatological scenario of agriculture, a factor that directly influences the development of animals and plants is the relative air temperature. At its highest levels, it is one of the most important climatic elements for the growth, development and productivity of many crops, such as rice, soybeans and corn. Bearing in mind the importance of knowing the possible maximum temperatures, this work aimed to apply the Theory of Extreme Values (EVT) in order to provide information on the occurrence of maximum temperatures in the city of Uruguaiana - RS. The maximum temperature data were obtained from 1950 to 2016, for all months of the respective years under study, provided by the Meteorological Database for Education and Research (BDMEP) and separated into monthly maximum series. The R software was used, together with the evd and extremes packages, to estimate the parameters of the GVE and Gumbel distributions, the Kolmogorov-Smirnov adherence test, the Ljung-Box test and calculation of the maximum temperature return levels. The results show that the generalized distribution of extreme values is adequate to study the behavior of the maximum temperature in the months of January, February, April, May, June, July, September and December, and for the months of March, August, October and November , the Gumbel distribution is adequate, in the municipality of Uruguaiana-RS. The quantile-quantile charts prove this fact. From May to August, the lowest temperatures forecast for all return levels occurred, and from December to February, the highest temperatures forecast for all calculated return levels occurred.","climatology, meteorology, return level, adhesion test, Gumbel distribution"
1347,"Understanding the factors that condition the adoption of crop-livestock (ILP) and livestock-forest (IPF) integration systems can be decisive in directing strategies that enable dissemination by the agribusiness sectors in Brazil. This study aimed to evaluate the profile of producers in the state of São Paulo who adopt integration systems. The survey was carried out with the application of a questionnaire to 175 producers. The chi-square test was used to verify the existence of an association between factors and integration systems and multiple correspondence analysis to characterize the profile of adopters. Variables related to the characteristics of the rural producer (experience, profile and access to credit), the rural property (size, relief and texture of the soil), the production system (livestock production cycle), as well as access to technical information (participation in agricultural events) characterize the three groups analyzed: ILP adopters; adopters of the IPF, and non-adopters.","Integration systems, correspondence analysis."
1328,"Longitudinal data analysis are studies of characteristics of individuals or experimental units measured over time. According to Littell et al. (1997), the analysis of repeated measures data requires special attention to the covariance structure due to the sequential nature of the data. Ignoring covariance may result in incorrect conclusions in the statistical analysis of the data. The mixed linear model allows for the solution of the problem considering two steps in its realization: the covariance structure model and the analysis of time trends for treatments by estimation and comparison of means. This means describing the observations over time and explaining the correlation structure of the data, indicating the degree of variation of the observation in the specified group. The objective of this work is to adjust mixed linear models in the data of protein content contained in cow&#39;s milk, presented by Diggle, Liang, and Zeger (1994), in order to characterize changes in proteins and factors that influence these changes, in which will be emphasized in the development of the models using the Lmer library of the R statistical software and in the interpretation of the data. Therefore, the purpose of this research is to select the mixed model with the covariance structure that best fits the data and test the effects of diet.",Longitudinal data analysis; Mixed Linear Models.
1265,"The environmental theme discussed in recent years gains more notoriety as discoveries are made year after year in different areas of human knowledge. The study of ways to minimize the environmental impact of various human actions is of great collaborative importance towards sustainable development. Today we live in the information age and one of the great characteristics of our age is the immense mass of data generated, offered, transported and stored daily. And one of the great areas that generate such a volume of data is the environmental area, where scholars and professionals make data available to provide collaborative work between people around the world. Agriculture is one of the pillars of the economy of several countries and is also responsible for a large portion of the total emissions of greenhouse gases, making it an important object of study. The objective of this work is to analyze patterns of emission of greenhouse gases (greenhouse gases) from Agriculture, specifically related to energy use, and to seek relationships between these patterns and GDP (Gross Domestic Product). Among several pattern discovery techniques, the Multivariate Statistical Principal Component Analysis (ACP) technique was chosen. The Principal Component Analysis technique proved to be a useful tool in the identification of expected patterns, with three components (dimensions) being identified related to three different aspects related to total equivalent CO2 emissions by energy use, namely: transport, irrigation and electrical efficiency. A relationship of these patterns with GDP was observed, as expected.",nothing
1268,"The shift point problem initially arose in the context of quality control and before the introduction of the shift point hypothesis associated with models, researchers faced difficulties in establishing a single model for some datasets. In fact, the switching point problem has been a topic of permanent interest in the statistical literature. In particular, many authors have studied the switching point problem associated with regression models under the usual assumptions of normal and uncorrelated errors. However, in many situations, inferences under normality are inappropriate as is the assumption of uncorrelated errors. A possible approach to treating the correlation of errors is to treat them as being autoregressive. Thus, we propose to study the switching point problem associated with regression models with symmetric and autoregressive errors, contributing positively to the development in the field of statistical research.",nothing
997,"There are two main models for repairable systems: renewal process (PR) and non-homogeneous Poisson process (PPNH) that uses the Weibull distribution to study the distribution of failure times. The objective of this work was to model the failure times of pieces of sugarcane harvesters (Chopper blade) using a Weibull regression model to better predict the durability of these systems. The analysis of repairable systems was carried out through a particular case of the PPNH, known as power law, using time truncation (the collection ended after one harvest period). Given the amount of covariates capable of influencing the failure time (response variable), a regression model was used and due to the behavior of the data, the Weibull distribution was chosen. The event of interest was the breakage of the Chopper blade and the response variable was the time to failure, counted in days. The covariates are the agent causing the failure (natural wear or breakage) and the State where the unit is located (São Paulo or Paraná). The Duane graphs and the total time under test (TTT-plot) showed an indicative of a non-constant failure intensity function. Comparison of the Weibull regression model with the exponential regression model, using the likelihood ratio test, showed that the best fit occurred using the first model. From this adjustment, it was possible to describe the behavior of the system during the harvest, which is a deterioration over time. With this information, mills and Chopper blade manufacturers will be able to better predict the durability of their products.","Repairable systems, non-homogeneous Poisson process, Weibull regression"
819,"In the wide statistical literature we can find several works on linear regression using the least squares method, which was proposed by Adrien-Marie Legendre in 1805. Such method will present an adequate result under certain conditions. However, in several cases such conditions are not satisfied, such as: cases where the errors do not have a normal distribution, or have heavy tails, or we have the presence of aberrant values, the least squares regression is not satisfactory. In the latter case, even in situations where the conditions are met, we need some robust procedure that accommodates the presence of these values, or techniques that allow us to identify them. When we investigate a little more the statistical literature on the subject, we realize that the least squares estimation process for regression coefficients has an ancestor, which precedes by at least half a century the one introduced by Legendre in 1805. Such ancestor, known as Minimum Sum of Absolute Errors regression or simply L1 Regression is considered a good robust alternative even for cases where the conditions for least squares are satisfied. In this work, we will reanalyze the real estate data presented by Narula and Wellington (1977) in the light of L1 regression. We will illustrate the main inferential results: such as model interpretation, construction of confidence intervals and hypothesis testing for the parameters, analysis of measures of model goodness of fit, and we will also use diagnostic measures to highlight influential observations. Among the influence measures we will use the likelihood displacement and the conditional displacement likelihood.",L1 regression; Likelihood displacement; Measures of Influence.
1241,"At the end of 2014, the state of Paraíba presented a worrying number with regard to the Municipal Social Security Regimes (RPPS): 92.86% of them showed financial and actuarial deficits, a fact that requires investigation and solution. In view of this reality and the national scenario of debates and intended reforms in the public service social security scope, the objective is to analyze the Actuarial Result (AR) of the municipal RPPS of Paraíba, based on cross-sectional data (2014) and through Generalized Additive Models for Location, Scale and Form (GAMLSS). Among the main results, we have that: the actuarial result has asymmetric and continuous distribution, with negative mean and median and large standard deviation; the T Family distribution (μ,σ,ν), according to the values indicated by the information criteria, provided the best adjustment for the actuarial result variable; the final model, which showed excellent goodness of fit, indicates a significant effect of the covariates Actuarial Balance (small and negative effect), Financial Balance (small and positive effect) and Total Retirees and Pensioners (large and negative effect). Regarding the limitations and suggestions, the need to continue this research is highlighted, considering the quality of management, the temporal effect (that is, the use of panel data) and segregation, which are fundamental for achieving even more results. robustness and consequent contributions.","Paraíba municipal RPPS. Actuarial Result. Generalized Additive Models for Location, Scale and Shape."
936,"Several elements influence the life expectancy of a country&#39;s population, such as environmental sanitation services, food, public safety, environmental protection, health services, education, among others. Thus, the increase in life expectancy is directly linked to the improvement of the population&#39;s living conditions and the Municipal Human Development Index (IDHM) is an important indicator for measuring these aspects. The objective of this work is to spatially describe this indicator (2010) for the Brazilian state of Tocantins and, in addition, to study its possible relationships with some characteristic variables, such as percentage of total employed persons, expected years of study, among others. For this, spatial statistical techniques such as Moran&#39;s Index and spatial regression models were used.","IDHM, Expectativa de Vida, Regressão Espacial."
1093,"Using data from the RAIS (Annual Report of Socioeconomic Information), from 2010 to 2015, an exploratory analysis was performed on the data for the State of Bahia. The RAIS databases are composed of a little more than twenty variables, but after the initial analysis, only six of them were considered useful for further research. The absolute number of companies in the state of Bahia has grown over the years, but on the other hand, the number of companies that are active has decreased. Among Bahian companies, we can highlight companies that belong to the trade and services sector, as they are the majority in the years analyzed. It can also be highlighted that, in 2013, commercial companies were surpassed in absolute number by service companies. To continue the work, the databases will be separated by mesoregions and the regression analysis will be tested. In addition, other data analysis techniques will be used, such as cluster analysis.","Bahia, companies, activity"
1308,"In this work, the techniques of multivariate analysis, factor analysis and cluster analysis were used to synthesize the information, and facilitate the understanding of the temporal and spatial variability of precipitation, in the municipalities of the State of Sergipe in Brazil. The months from March to October are the highest levels of precipitation, and the different groups formed are concordant municipalities in the coastal, agreste and sertão regions of the state.","precipitation, factor analysis, cluster analysis"
1018,"It was descriptively studied the temporal evolution (period of 10 years) of the maximum, average and minimum daily temperatures, considering two chosen months: January (summer) and July (winter). Data are from the Climatological Station of CEAPLA/IGCE/UNESP. The objective was to carry out an exploratory analysis of the data, looking for possible temporal trends, as well as comparing the two months. No trend was found over time, and it was concluded that, in general, temperature asymmetries are negative. The values of the coefficients of variation were practically the same, being slightly higher in July. In relation to minimum temperatures, the value found in July was much higher than the others.","Temperatura, Estatística Descritiva, Séries Temporais."
1005,"Economic data published on municipal agricultural production of bananas in the state of São Paulo in 2008 (ten years before the start of the current 2017 Agricultural Census) were analyzed using geostatistics in order to support the development of technical and political guidelines with the objective to guide the development of the banana production system that is the economic base of the region with less economic and social development in the state of São Paulo. The results showed that the region of Registro had the highest data on area destined to harvest, harvested area, quantity produced and commercial value. The highest average yield data obtained were concentrated in the regions of Sorocaba and Campinas. The largest data on average price paid to producers were concentrated in the region of Sorocaba, Presidente Prudente and São José do Rio Preto. These differences indicated the need for studies in the area of economics and commercialization in order to characterize in detail the differences in the productive systems of these regions.",geoestatística agricultura banana
1314,"Multivariate Profile Analysis (MPA) is an excellent way to analyze data measured in time, but also in space, aiming to test hypotheses (parallelism, coincidence and horizontality) about the average response profiles of the different treatments and compare them each other. In the case of this project, the Multivariate Profile Analysis methodology was used to analyze a data set of repeated measures over time from an experiment with different cacao progenies (Theobroma cacao) and their resistance/susceptibility to known disease as “Ceratocystis wilt”, caused by the pathogen Ceratocystis cacaofunesta. Thus, this experiment evaluated the average profiles described over time by these progenies in relation to the variable percentage of dead seedlings (%PM). From the results obtained, it was observed that, for the variable under study, the parallelism hypothesis was not rejected, so that the profiles are parallel to each other, implying that there is no interaction effect between the treatment and time factors. However, the coincidence and horizontality hypotheses were both rejected, so that the profiles are not coincident and horizontal, implying that there is no effect of the treatment factor and the time factor, respectively. Contrasts were also constructed to compare the average profiles two by two. Thus, it was observed that the average profile of the CCN51 progeny had the highest values of percentage of dead seedlings (%PM) over time in this experiment, which shows its greater susceptibility to the disease of &quot;Ceratocystis wilt&quot; . As for the average profile of the FCB01 progeny, the lowest %PM values were observed over time in this experiment, which indicates its greater resistance to the disease.",Multivariate Profile Analysis; repeated measures data; Theobroma cacao; Ceratocystis cacao disastrous
969,"Eucalyptus leaf bacteriosis is an important disease in forest nurseries, characterized by leaf lesions in seedlings. This disease can affect more than 3 million seedlings per year, resulting in estimated losses of approximately 2.4 million reais per year. Given this situation, this work aims to detect bacterial infection in eucalyptus seedlings in nurseries, before the appearance of visible symptoms on the leaves, thus allowing the researcher to have greater control in combating the infestation. For this purpose, multivariate techniques of principal components and Fisher&#39;s discriminant linear function were applied, as well as the non-decimated discrete wavelet transform. Principal component and Fisher&#39;s linear discriminant function techniques were useful to identify bacteriosis in clones of Eucalyptus spp. with a 75% hit rate. The non-decimated discrete wavelet transforms combined with generalized linear models suggest that to diagnose bacteriosis, we can work with leaf reflectances with wavelengths greater than 1126 nm. The infrared band in which Spearman&#39;s correlation between leaf reflectance and leaf bacteriosis severity was significant occurred between wavelengths from 1403.15 nm to 1529.04 nm. A multiple linear regression model of covariate selection (stepwise) was fitted, which estimates the severity of bacteriosis from wavelengths (nm) in the range of 1,194,016 to 1,644,467 nm.","componentes principais, função linear discriminante, transformada wavelet, Xantomonas."
953,"There is an increase in the quantity and replacement of the quality of food consumed due to the increase in income in a given population (Cirera and Masset, 2010), which in economics is called Bennett&#39;s Law (Bennett, 1954 in apud Godfray, 2011). In general, populations with lower income generally have a predominant dietary pattern for starch-rich foods, while populations with higher income generally have greater variability and caloric content, with a predominance of foods of animal origin and foods from vegetable processing and fruits (Godfray et al., 2010). Such behavior generally occurs worldwide, but may have some specific variations depending on the location. It may be beneficial at first, for example, in reducing levels of malnutrition, however, it may present problems if not controlled in the long term by the science of nutrition (Godfray et al., 2010). For example, it generates greater use of resources, making it less sustainable from an environmental point of view (Godfray, 2011), and can also be harmful to health (FAO, 2017). In this context, analyzes are needed to try to understand the patterns of food availability that are more related to income in a population context, and for this, statistical techniques such as multivariate analyzes have been increasingly applied in the literature (Shills, 2016). This work aims to apply the multivariate statistical technique of Principal Component Analysis to search for patterns in food group availability data taking into account 171 countries from the FAO (Food and Agriculture Organization) data repositories, in order to identify at least one world-wide food availability pattern that can be related to GDP (measure of income). With the multivariate statistical method applied, the expected pattern was obtained, with behavior similar to that observed in Bennett&#39;s Law and in FAO (2017).",Availability of food; Nutrition; Principal component analysis; FAO; Bennett&#39;s Law
905,"The semi-competing risks situation is a generalization of competing risks and usually one considers only two events, one terminal and one non-terminal. In this situation the terminal event censors the non-terminal event, while the occurrence of the non-terminal event does not prevent the terminal event from occurring. Typically, the two events are correlated. We consider an illness–death model with shared frailty, suggested Xu et. al (2010), which in its most restrictive form is identical to the semi-competing risks model. In their model the dependency between the terminal and non-terminal failure time is incorporated through the use of a shared frailty, which gives a model with conditional transition rates possessing the Markov property. We introduce the use of a parametric model for the conditional transition rates. Maximum likelihood estimation is performed to fit the model to data set. Then the model is applied to a data-set on a colon cancer clinical trial (Moertel et al., 1990).",Copula; Dependent censoring; Frailty; Illness–death model; Proportional hazards; Semi-competing risks data; Terminal event.
1117,"Young and Bakir (1987) proposed the class of Generalized Log-Gamma Linear Models (MLGGG) to analyze survival data. In this work, we extend the class of models proposed by Young and Bakir (1987) allowing a non-linear structure for the regression parameters. The new class of models is called Generalized Log-Gamma Nonlinear Models (MNLGGG). In this work, expressions in matrix notation were obtained from the Bartlett and Bartlett-type correction factors to the likelihood ratio, score and gradient statistics, respectively, in the MNLGGG. We evaluated and numerically compared the performance of the proposed tests through Monte Carlo simulation in relation to size and power, in finite samples.",Bartlett&#39;s correction; Type-Bartlett Correction; Chi-square distribution; Likelihood Ratio Test; Test Score; Gradient Test.
950,"This study used data from the Federal District Victimization Survey, granted by the Federal District&#39;s Secretary of State for Security to build Classical Regression (OLS) and Geographically Weighted Regression (RGP) models that explain the Feeling of Insecurity Indicator (ISI) of the population interviewed from predictors elaborated with survey data. The objectives of this work are: To identify the influence of space on the ISI, to evaluate the gains from the use of Geographically Weighted Spatial Regression (RGP) models in relation to the use of the Classical Regression (OLS) model, and to study some of the results that show the importance of applying a local regression model to the detriment of a global regression model. To compare the models produced, the Akaike information criterion, the evaluation of the produced waste and the identification of results achieved exclusively by RGP were used. The RGP models showed that there is indeed an influence of space on the ISI and allowed a more qualified view of the results through local estimates of the parameters. The evaluation of the models revealed a reduction in the Moran index applied to the residues, when the RGP was used. The results show the spatial distribution of the ISI and the varied way in which other factors relate to them in the different regions surveyed, revealing that social inequality in the DF is also characterized by different perceptions of the level of security.","Victimization, Feeling of insecurity, Geographically Weighted Regression."
1264,"This work aimed to obtain an equation through multiple linear regression that correlates annual values (from the year 2016) of the average yield of guaraná production and factors (amount produced, production variation and harvested area) in 6 Brazilian states. Calculations were performed using SPSS and Minitab statistical software.",Keywords: Regression; Average yield; Guarana.
1201,"Stomach cancer is one of the most frequent among men in Brazil and has different causes and prognostic factors (ZILBERSTEIN et al., 2013). In this sense, statistical methods such as survival analysis can be used to study how such factors are associated with outcomes of interest, such as death and recurrence, for example. In this work, we present the application of a survival model with cure proportion, called Weibull promotion time, to the stomach cancer data available in the Cancer Genome Atlas (TCGA). The estimation of the model parameters was made considering Classical and Bayesian inferential procedures. From a Bayesian point of view, we use approximation by Monte Carlo methods via the Markov Chain (MCMC).","Survival Analysis, Cure Ratio, Bayesian Inference, Classical Inference, Weibull promotion time model, TCGA."
1001,"The usual distribution of food consumption in a population group is of great interest to researchers in the field of nutrition and presents a challenge for statistical modeling, as it requires a model that accommodates the so-called inter- and intrapersonal variability. The first is attributed to the variation in consumption among individuals that make up a given population, and the second is given by the variability that an individual can present in consumption on different days of the week. This work aimed to estimate the usual distribution of vitamin intake in a population, through a model that uses a robust estimation procedure, in order to incorporate the aforementioned variability. Data came from a longitudinal study of 20 days of food consumption of 285 individuals living in the city of Rio de Janeiro, Brazil. The selected variables were the consumption of vitamins B1, B2 and B6, analyzed according to the gender of the participants. The Box-Cox t model with random effects associated with the median and coefficient of variation parameters was used to accommodate the variability between individuals over time. The estimation procedure was performed via the PQL (penalized quasi-likelihood) method and the gamlss routine of the R program was used to estimate the parameters. The results showed a good performance of the Box-Cox t model with random effects adjusted to the data set, being an interesting alternative for data modeling with outliers and accentuated presence of asymmetry.","habitual consumption, Box-Cox t distribution, inter- and intrapersonal variability."
1303,"In this work, we proposed a new  long-term lifetime distribution with four parameters
inserted in a risk competitive scenario  with decreasing, increasing and unimodal hazard rate functions, namely the  Weibull-Poisson long-term distribution.
This new distribution  arised  from scenario of competitive latent  risk, in which the lifetime associated to the particular risk is not observable, rather only the minimum lifetime value among all risks is noticed in a context of long-term. But it can be used in any other situation as long as it fits the data well.
The Weibull-Poisson long-term distribution presented  as particular cases the new  exponential-Poisson long-term distribution and the  Weibull long-term distribution. The properties of the proposed distribution were discussed, including its
probability density, survival and hazard functions.
The selection criteria AIC and likelihood ratio test were used for the model selection.
The relevance of the approach was illustrated on two real datasets, when the new model was compared with its particular cases observing its potential and competitiveness.","Competing  risks, Cure fraction model,  Likelihood, Weibull-Poisson distribution, Survival Analysis"
1325,"We investigate the square root diffusion process, also named the CIR process. It is a stochastic differential equation which ensures mean reversion of the state variable towards a long run level and avoids the possibility of negative values of the process. These are interesting properties for a number of practical applications, especially when two CIR processes are correlated. We developed analytical approximations to convert the correlated CIR into an affine-diffusion process to find closed-form solutions for the Laplace Transform via Riccati Equations. The final result is intended to be applied to two real-world financial situations: we model the default probability of emerging market bonds issued in foreign currency and we price bonds splitting the nominal interest rates as a combination of real interest rates and actual inflation. 

The paper developed two analytical approximations to convert the correlated CIR into an affine-diffusion process to find closed-form solutions for the Laplace Transform via Riccati Equations. The result is intended to be applied to calculate the default probability of emerging market bonds issued in foreign currency and to price bonds splitting the nominal interest rates as a combination of real interest rates and actual inflation.

The results are a very good approximation to the correlated case when compared to Monte Carlo simulation results. Moreover, if the correlation parameter is set equal to zero the results match when compared to the non-correlated formula.","Default Probability, Stochastic Multifactor Process, Square Root Diffusion."
1006,"The skew-normal distribution is one of the first skewed distributions developed by Azzalini, which allows modeling the asymmetry present in the data as opposed to the normal distribution. In several areas, such as health, economics and biology, where the skew-normal distribution is often used, it is necessary to make use of hypothesis tests to reach conclusions about characteristics of the population of interest. When the sample size is small, and here small refers to a sample size smaller than 50, the conclusions obtained from the tests tend to be imprecise. A commonly used test is the signed likelihood ratio test, which asymptotically has a normal distribution, but for small sample sizes the approximation can be imprecise. In order to improve the accuracy of the conclusions obtained from the hypothesis tests, we developed a correction according to the methodology developed by Fraser et al. (which we will refer to as Fraser-Reid-Wu type correction) for the signed likelihood ratio test on the parameters of skew-normal regression models. Empirical results show great performance of corrected tests.",Asymmetry; Fraser-Reid-Wu fix; Centered parameterization; Signed likelihood ratio; Skew normal
1197,"The aim of this study was to analyze the behavior of the sampling distribution of the range and the degree of spatial dependence (DE). Also, the objective was to evaluate the correlation between these measures and the spatial dependence index (SDI). It was possible to identify that reach, in general, presented a positive asymmetric behavior. The ND had a negative skewed distribution and, consequently, the relative nugget effect (RPE) showed a positive skewed distribution. The range has better correlations with the IDE. When comparing the classifications of spatial dependence, from the DE and the SDI, it was possible to verify that the latter is more coherent because it also considers the spatial variability due to the range parameter, and not just the variability explained by contribution and level.",nothing
904,"The objective of this work is to increase the sample size of the water potential data of different eucalyptus species through their archetypes, aiming to increase the precision in the statistical inference procedure. For this purpose, six eucalyptus species were evaluated: E. grandis, E. urophylla, E. cloeziana, E. citriodora, E. camaldulensis and E. brassiana, with four replications, and two response variables were measured: minimum water potential and maximum. Then, for each species, two successive increases of a dice were performed, increasing the number of repetitions from four to six and, consequently, the sample size from 24 to 36 (50% increase in n), without changing the probability distribution. from which comes the initial sample and neither its parameters. The sample increase corroborated to evaluate the effect of the six eucalyptus species on the minimum and maximum water potential with greater precision.","Archetype Analysis, Data Augmentation, Agronomy"
1225,"The use of geostatistics in the field of soil sciences has been increasingly intensified to help study the behavior of spatial dependence. Indexes that measure spatial dependence in a summarized way are widely used as a way to complement information that the semi-variogram provides. Although there are several indexes in the literature, the need to further formalize the interpretations of them is evident. The present work aimed to assume the spatial dependence indices IDE and DE as random variables and based on data from physical and chemical soil attributes, adjust theoretical probability distributions and, based on the best fits, enable more complete inferences about the indices based on probability distributions.",nothing
1250,"A product (or a service) can be evaluated by checking the satisfaction of m 3 1 characteristics. At least two different evaluation scenarios are possible: (1) the evaluators attribute, to each item, a value between 0 in corresponding to the count of characteristics considered satisfactory, or (2) the evaluators attribute, to each item, a vector of m dichotomous variables corresponding to each characteristic. The evaluators belong to different populations that consider different levels of importance for the characteristics or randomly assign the answers for each characteristic (without judging the satisfaction of that category). This summary shows some simple models to handle these 2 aforementioned assessment scenarios.",product evaluation; binomial distribution; mix models
1260,"This work aims to investigate the effect of the quota policy on the chance of a student entering higher education given the various factors that characterize him as a quota holder or not. For this, we used the microdata from ENEM 2016, available on the INEP website, and the cutoffs of 3 different courses historically highly sought after because they have prospects for financially successful careers and, consequently, social ascension: Law, Civil Engineering and Medicine. The cutoff scores used as a parameter were those of the Federal University of Rio de Janeiro in the reference year, 2016. Through the proposed models, we had evidence that the existence of quotas increases the chance of admission to universities for those who are covered, but they reveal also that these effects are very unequal when compared between the quota categories. In particular the candidate awarded for the racial quota, but not awarded for public education or income. This has a 254% chance of admission that a candidate who does not have these characteristics as a quota student by race, but who is also a quota student for having studied in a public school, for example, has a 15% chance when compared to an outsider of this group.","ENEM, MLG, Generalized Linear Model, Logistics Regression, Affirmative Action"
1198,"An accurate forecast of extreme events is a crucial condition for public policies to be adopted in order to mitigate the effects caused by meteorological and climatic phenomena. The use of quantitative methods to perform weather forecasting through numerical simulation has been highlighted in meteorology. In general, these models use several physical parameterizations, generating possible combinations in quantities that are not feasible to be carried out. Thus, the main objective of this work is to apply experiment planning techniques to assist in the selection of adequate parameterization settings for the Weather Research and Forecasting (WRF) model to improve minimum and maximum temperature forecasts in Northeastern Brazil.",Previsão de tempo; Planejamento de experimento; Weather Research and Forecasting.
1193,"Multitype events (or competing risks) models for a repairable system subject to multiple failure types are discussed. Under minimal repair, it is assumed that each failure type has a power law intensity. An orthogonal reparametrization is used to obtain an objective Bayesian prior which is invariant under relabelling of the failure modes. The resulting posterior is a product of gamma distributions and has 
appealing properties: one-to-one invariance, consistent marginalization and consistent sampling properties. Moreover, the resulting Bayes estimators have closed-form expressions and are naturally unbiased for all the parameters of the model. The methodology is applied in the analysis of (i) a previously unpublished dataset about recurrent failure history of a sugarcane harvester and (ii) records of automotive warranty claims. A simulation study was carried out to study the efficiency of the methods proposed.","Bayesian analysis, competing risks, power law process, reference prior, Jeffreys prior, repair"
1163,"Count data models, especially in a time series context, have not yet been fully explored in the literature. In this work, we are interested in count data with excess zeros in a time series context. Real data sets may have this characteristic, as shown in the application with congenital rubella data. In this work we present the generalized Poisson ARMA(r,q) zero modified model and perform a Bayesian analysis using a rubella data set.    
","Bayesian inference, Generalized Poisson distribution, Zero modification"
1137,"In this work we consider some criteria that assess the goodness of fit of k-modified regression models. The k-modification consists of the inclusion of a parameter in the probability mass function of traditional discrete distributions, capable of modeling the inflation or deflation of observation k in the data set. In this sense, modification becomes essential when, in many practical situations, a certain observation k of the data set occurs with a higher or lower frequency than expected when considering a particular discrete distribution. In the context of regression models, we want to explain the k-modified count data set using explanatory variables both for the mean of the variable of interest (response variable) and for the parameter responsible for modifying observation k. In this work, we will consider the following k-modified models: Poisson, Binomial and Geometric",Counting data; Goodness of Fit; Inflation; Deflation; Regression Models.
1363,"This work aims to carry out an exploratory characterization of Brazilian pastures through cluster analysis using the DTW algorithm and the GAM model in NDVI series obtained from the MODIS satellite. Being able to characterize the patterns of pastures represents a great advance for decision making related to the development and quality of the production process of pastures, which can also represent benefits for the environment and possibly for the reduction of deforestation, due to better management of information and of land use.",nothing
1317,"Poker is considered a mental skill game, included in the Mind Sports Olympiad, which involves both knowledge of strategy and probabilities. Through the study, it is possible to maximize the winning probabilities in a game, thus contradicting the theory of games of chance, which states that the random player goes bankrupt with probability one, when the number of games tends to infinity. The study of poker has strengthened the development of students in higher education courses in several universities around the world and this work presents a practical study regarding the realization of tournaments at the Federal University of Santa Maria. Theoretical classes and tournaments held have proven to be motivating tools for students entering the Statistics course and other courses at UFSM.","probability, strategy, mind sports, poker, statistics"
1344,"Given the necessary importance on the use and development of comparison criteria between estimators, this work seeks to present an additional criterion that summarizes arbitrary performance information in graphic profiles that facilitate the perception of the quality of the methods proposed to solve a given problem. To illustrate the use of the tool, a small simulation study was considered and its results discussed.",Comparison of estimators. Performance radius. Cumulative probability function.
962,"The estimation of Consumer Price Indices (CPIs) is generally carried out from samples of places of purchase and products, and such samples can be considered as coming from a two-dimensional population. Conglomerate Sampling (CA) is commonly used in surveys conducted to estimate CPIs. Two-dimensional Sampling (AB) can be a convenient alternative in some research situations. In the context of IPCs, AB would be the crossing of samples from places of purchase with product samples. However, this methodology (AB) is sometimes confused with CA, it is not well known, there is little literature and few empirical studies on the variance of its estimators. The way of building records of places of purchase and products in Brazil implies a sampling process of places (and later, of products) that does not correspond exactly to the Conglomerate Sampling method in 2 stages (AC2). It is at this point related to sampling that this work proposes to contribute, comparing a process of CA with AB for the selection of places and products to have their prices collected. In this work, we take AC2 as the &#39;standard&#39; method against which the AB alternative will be compared. The data source for this work comes from the World Food Program project. It is a price base composed of 15,606,420 records representing the 60 monthly observations, from January 2012 to December 2016, of prices for 263 selected products in each of the 989 eligible purchase locations. We will use the R software. The comparison between the sampling plans will be performed by calculating the variance of the CPI estimator each month. For this purpose, approximate expressions of the variance of the Laspeyres Price Index estimator were obtained for each type of sampling. As results, we will present values of variances for each sample scenario studied. From these we will be able to conclude in which situations one plan is preferable to the other.","Two-Dimensional Sampling, Conglomerate Sampling, Consumer Price Indices."
1205,"Co-authorship among members of a research group can commonly be represented by a graph G with k vertices on edges. The researchers that make up this group are represented by the vertices, and the connections (common publications or co-authored works) between two researchers are represented by the edges. The objective of this work was to study the reliability measure of networks considering the vertices or researchers perfectly reliable and the edges or unreliable co-authorship relationships (or prone to failures). Specifically, a Bayesian approach is proposed for the reliability of a co-authorship network represented by a research group from UNESP, registered with the CNPq, obtaining the Bayesian estimates and the respective credibility intervals for the individual components (edges or relations of co-authorship) and for the co-authorship network. For that, an informative a priori density proposed by Lee (1993) was assumed, and the a posteriori summaries were obtained through Monte Carlo simulation methods via Markov Chains (MCMC). The results showed the relevance of an inferential approach for the reliability of scientific co-authorship networks, emphasizing that the contribution of each researcher is essential for the maintenance of a research group. Furthermore, the Bayesian methodology was viable and easy to implement in computation, allowing the incorporation of a priori information in the estimation process.","MCMC methods, social media, graph theory."
1229,"In this work, factors related to the consumption of cigarettes and alcohol by students were studied. The study was carried out based on microdata from the National Adolescent Health Survey (PeNSE) for the year 2015 in a sub-sample from the North region. The sample profile was drawn and an analysis performed using generalized linear models to estimate the prevalence of factors associated with the consumption of these substances. The results showed that cigarette consumption in the North region is in line with national indicators and the consumption of alcoholic beverages is lower in the North compared to Brazil. In the modeling it was found that the fact of living with father and mother is related to a lower prevalence of cigarette consumption, but does not influence the consumption of alcoholic beverages. As for the characteristics of the school, it was observed that there is no difference regarding the consumption of alcoholic beverages in the administrative spheres, but regarding the use of cigarettes there is a higher prevalence of consumption in the state and federal spheres in relation to the private sphere.","Northern region, Teenagers, Cigarette, Alcoholic beverage."
1190,"The present work proposes robust estimators for the parameters of the beta distribution, useful to model continuous data in the range $(0,1)$ in the presence of outliers outliers. Weighted maximum likelihood estimators (EMVP) with two weighting proposals were considered. To correct the bias of these estimators we use the parametric \textit{bootstrap} method adapted for outlier data. The performances of the proposed estimators were evaluated using Monte Carlo simulations, compared to the usual maximum likelihood estimators. As specific robustness measures, breakpoint and sensitivity were adopted. Numerical evaluation showed that EMVPs have less sensitivity to outliers and higher breakpoints. Estimators corrected by the proposed bootstrap method showed less bias compared to their uncorrected versions. An application to real data is also presented and discussed.","bootstrap, beta distribution, robust estimators, outliers, weighted likelihood"
951,"In this work, an analysis and comparison of the main robust criteria for selecting models in robust linear regression was performed. The advantages and disadvantages will be presented through different scenario simulations and data analysis.",nothing
1167,"In binary regression, data imbalance refers to the presence of zeros or ones in a significantly greater proportion than the corresponding ones or zeros. In this work, we study two methods developed to deal with unbalance and compare them with the use of asymmetric links, specifically we work with some links proposed in Lemonte and Bazán (2017) and Bazán et al. (2017). The results show that these methods do not adequately correct the bias in the estimates of the regression coefficients and that the models with asymmetric power and power reverse links considered produce better results for a certain type of imbalance. Additionally, we present an application for unbalanced data identifying the best model among several proposed models. Parameter estimation is performed under a Bayesian approach considering the Monte Carlo Hamiltonian estimation method using the No-U-Turn Sampler algorithm and model comparisons are developed using different model selection criteria.","asymmetry, asymmetric links, unbalance, binary regression"
1237,"One of the challenges in the production of second generation (2G) ethanol is the development of efficient technologies to convert xylose to ethanol in the sugar fermentation process. The investigation of xylose assimilation pathways by yeasts can provide relevant information regarding the limiting steps in the metabolism of this pentose conversion and help in the construction of genetically modified strains. Metabolomics-based technology has been widely used to answer important biological questions, in view of being able to identify and quantify metabolites present in a biological system. However, the large volume of information generated in a metabolomics experiment makes it necessary to use a mathematical and biologically representative model for the analysis and interpretation of the data obtained. The objective of this work was to develop a statistical analysis protocol to interpret the data obtained in the quantification of the main intracellular metabolites related to the conversion pathways of xylose to ethanol. The xylose fermenting yeast S. arborariae was evaluated under two different growth conditions: aerobic and microaerobic, during the exponential growth phase. The statistical model used is based on analysis of variance (ANOVA) with repeated measures. This model will be added to the metabolomics protocol established and optimized by the group and can be applied to data obtained in metabolomics experiments using other xylose fermenting yeast species.","ANOVA, METABOLOMICS, QUANTIFICATION, XYLOSIS, YEAST"
1212,"In this article, partial results of a research project that seeks to develop resources for the assessment of statistical literacy of students in the early years of elementary school will be presented. It is proposed a collaborative game in RPG form that aims to help the teacher to identify if the child understood what is expected about the contents of Statistics in accordance with the Brazilian curriculum guidelines. The game in development involves an adventure in medieval times in which the children together progress through the various stages to find an orchid and save the king&#39;s life. Through the game, it is intended to verify the children&#39;s knowledge about graphics and possibilities.",Statistical Education; Early years; Cooperative games; Assessment.
1219,"In this work, via fuzzy theory, a method for determining regression models was obtained using the fuzzy inference system proposed by Takagi and Sugeno, with the data inputs being transformed into fuzzy sets and the system outputs being non-fuzzy sets. This model is combined with the network structure of the Adaptive Neuro-Fuzzy Inference System (ANFIS). The ANFIS model takes a fuzzy inference system and associates it with a back-propagation algorithm, based on a data set, and its structure facilitates the calculation of the gradient for the parameters. In this way, a set of fuzzy rules is obtained by the fuzzy subtractive grouping technique, used to partition the input and output space of the dataset. Fuzzy inference systems are used based on a set of conditional rules, representing the inputs and outputs of the model, associating the learning ability of artificial neural networks with the linguistic interpretation power of fuzzy inference systems, which are capable of treating knowledge incomplete, uncertain or even conflicting. The proposed mathematical model used input variables from observations, obtaining outputs of the type, y = f(x), where x is the input and y the output of the inference system and f a quantitative representation of the mapping. Usually the function f is a linear combination of input variables, whose coefficients are estimated using the least squares method. In the development and result of the work, an application is presented where the observed data and the data estimated by the model were compared using statistical methods, and these were shown to be well correlated, where the efficiency and accuracy of the neuro-fuzzy model obtained can be verified. .",nothing
1342,"The elimination of points is the most traditional sensitivity method, which consists of evaluating the impact of removing an observation on the estimates of the parameters of a model. On the other hand, Cook (1986) presented a general method to assess the local influence of small perturbations on the statistical model or on the data. However, Cook&#39;s methodology may not detect locally influential and masked observations. In this work, we propose a methodology for obtaining locally influential masked data in the context of linear regression.","Local Influence, Regression Models, Diagnosis, Masked"
1159,"Smallholder agriculture, now defined by law as family farming, has always been on the sidelines of the benefits and incentives offered by the Brazilian government, thus hindering its development. Currently, the main public policy to strengthen this category is the National Program for Strengthening Family Agriculture - Pronaf. Thus, the objective of this study is to verify the current distribution of the program&#39;s resources regarding the location and profile of its beneficiaries, identifying regional differences among credit users. It was found that there are regional differences in the distribution of resources, having a concentrating history in the southern region of the country, to the detriment of the Northeast region, which concentrates the vast majority of farming families. The hypothesis of this work considers that the disproportionate distribution of Pronaf resources may occur due to the socioeconomic profile of the farmer. For this, a study was carried out based on the information collected by PNAD 2014 in its Productive Inclusion supplement. All people who responded that they had received some credit from Pronaf in the reference period of the research were considered and the sociodemographic and economic characteristics of the profile of the beneficiaries and their activities were analyzed. The results are presented for all regions, but only the two relevant regions in this study were analyzed. If we consider as a measure the participation of regions in the total of family establishments, in the personnel employed in agriculture or in the rural population, these distributions do not present the same proportion observed for the resources granted by Pronaf. Differences were observed in the profile of family farmers in the South and Northeast regions: while the former have higher per capita household income, more access to information and technical assistance and are better educated, the latter receive more seeds and inputs.",nothing
931,"This paper aims to present some of the results of the identification of the contextual questionnaires of the 2015 Basic Education Assessment System (SAEB) questionnaires. The methodology for carrying out the factor analysis is based on procedures proposed by Laros (2012) and Johnson &amp; Wichern (1998), among them, the standardization of variables, verification of the factorability of the correlation matrix, identification of factors to be extracted, type of rotation to be used, inspection of factor loadings, reliability indices and calculation of factor scores. The results indicated a reduction from 361 items that make up the contextual questionnaires, to 209 items distributed in 24 factors, grouped into four databases.","SAEB, Contextual Questionnaires, Factor Analysis, Dimension Reduction."
1069,"The inappropriate use of natural resources due to food production has caused great degradation, especially in the soil, which directly suffers the main impacts. The objective of this work is to apply the multivariate analysis of variance (MANOVA) and linear discriminant in order to evaluate different land uses in the Southern Amazon, as a function of physical, chemical and biological attributes, thus allowing to detect differences between the types of land use. management, according to the variables responsible for this discrimination. The study was conducted on the rural property Fazenda Maringá, located in the municipality of Alta Floresta, with different land uses: native forest (FN), degraded pasture (PD), renewed pasture (PN); permanent preservation area under recovery (APPR); crop area (LA); forage sugarcane area (CA) and area reforested with eucalyptus (EU). The variables considered for the analysis were: soil microbial biomass carbon (CB), aluminum (Al), CaMg, which corresponds to the sum of calcium (Ca) and magnesium (Mg), and clay. According to the results obtained, it is concluded that the areas of tillage, native forest and degraded pasture were identified as similar to each other based on the carbon of the microbial biomass, clay, aluminum, calcium and magnesium content, indicating that such attributes are able to distinguish the different types of management and point out their similarities.",nothing
1136,"In this work, a Bayesian approach is proposed for estimating the parameters of distributions for k-Modified count data. A modification is understood as the inclusion of a parameter in the mass probability function of traditional discrete distributions, capable of modeling the inflation or deflation of observation k in the data set. Modification becomes essential when, in many practical situations, a particular observation k of the data set occurs with a higher or lower frequency than expected when considering a particular discrete distribution. Applications considering real datasets are presented.",Counting data; Discrete Distribution; Inflation; Deflation; Bayesian Inference.
958,"Over time, there has always been an attempt to increase the accuracy of experimental designs, mainly due to the increase in their practice in the most diverse areas of knowledge. The use of optimal designs with their different optimization criteria is fundamental to obtain results that maximize the information in the study. However, the assumption of heteroscedasticity is not always satisfied and can be resolved by a transformation. Therefore, the objective of this work is to develop a methodology for the construction of exact D-optimal designs for experiments in which, a priori, it is suspected that the variance of the responses is not constant, for two factors, when the model is linear, and using a transformation of the Box-Cox family into the response variable.","Optimal design, Box-Cox transformation, criteria."
1024,"Statistics is a useful and sometimes necessary and essential tool in work carried out by various professionals such as geographers, biologists, administrators, psychologists, etc. Normally, for training in statistics, the curriculum of these courses is composed of a single subject, which makes it difficult for the undergraduate to actually learn the concepts. In David Ausubel&#39;s Learning Theory, a key concept refers to the meaningful learning that occurs when new information (new knowledge) is related in a non-arbitrary and substantive (non-literal) way to the learner&#39;s cognitive structure, that is, the content taught becomes meaningful to the individual. In our work carried out between 2012 and 2015, a software prototype was built, written using HTML and javascript, named EDH (Statistics in Homeopathic Doses) to teach some concepts of descriptive statistics in order to provide the occurrence of significant learning of concepts by the learner when using the software. The focus of EDH for content transmission is the primary use of imagery language. An image that is didactically effective is an image with low cognitive load, that is, an image that is easy to assimilate and, therefore, enhances meaningful learning. Several HTML pages were built dealing with concepts such as variance, standard deviation, Gini index, etc., presented primarily through images that a priori were designed to have low cognitive load. EDH has been used on several occasions with Geography undergraduates to evaluate the software. As a general result, several students praised the use of images as a tool for transmitting descriptive statistics contents, which seems to indicate that the transmission of such contents through images with low cognitive load can provide meaningful learning.","Meaningful learning; EDH software, image with low cognitive load"
1353,"In order to compare the progress of Youth and Adult Education (EJA) in São Carlos and its corresponding micro-region (São Carlos micro-region) with the State of São Paulo, this paper surveys data on illiteracy and its characteristics. sociodemographic characteristics of the PEA15+ (Economically Active Population aged 15 or over) from the 2010 Demographic Census, the 2016 School Census and the 2015 National Household Sample Survey (PNAD). so far in the appropriate locations with regard mainly to the PEA15+ schooling and to present suggestions for a progressive reduction (and in the future to achieve the eradication) of illiteracy in the region of São Carlos.","Illiteracy, Official data, Demand, EJA, Descriptive statistics."
1147,"In many situations, the use of samples instead of evaluating an entire population is a common action given the limitations of survey costs, the time available for data collection and the difficulty of accessing them. A common way of selecting units that will constitute a sample is simple random sampling (AAS). In addition to AAS, there are several other ways to carry out the sampling process, including sampling designs based on ordered sets, with the particular case of Ordered Set Sampling (ACO). The OAC is useful in situations where measuring the variable of interest implies high cost or is difficult to access, but it is possible to order the sampling units using a concomitant variable or based on a subjective, accessible and low-cost criterion. The designs by ordered sets have several applications in the context of Statistical Quality Control (CEQ), especially in the construction of control charts, producing efficiency superior to AAS. In this work, a simulation study was carried out with the objective of evaluating the performance of control charts for the mean through the mean sequence length (CMS), considering samples selected via ACO and processes with distributions presenting different levels of asymmetry and kurtosis. We used the normal and generalized normal skew distributions in the generation of the samples as they allow incorporating different levels of asymmetry and kurtosis, respectively. As a result, the CMS tends to be smaller as skewness increases and larger as kurtosis increases, regardless of sample size and whether or not the process is under control. Furthermore, the CMS for samples with normal and generalized normal skew distribution tends to be higher for the process operating under control when compared to the process out of control.",Monte Carlo simulation; Sampling by Ordered Sets (ACO); Control Charts;
1263,"The application of quantitative methods, especially statistics, in the study of law and its institutions is called jurismetry. Despite its relevance for the development of strategic actions both in the procedural sphere and for the formulation of public policies, few Higher Education Institutions offer specific subjects on the subject in the curriculum. In this context, the present work aims to present actions that help students and professionals working in the field of legal sciences with regard to the application of statistics and processing of quantitative data. The development of activities was carried out in an interdisciplinary way, involving professors, students and administrative technicians from the Department of Statistics of the Federal University of Santa Maria in partnership with the Citizen Security Nucleus (NUSEC) of the Law School of Santa Maria (FADISMA). The actions include assistance in the planning, execution and quantitative analysis of academic work carried out by NUSEC, preparation of teaching material for the introduction of basic notions of descriptive statistics and the realization of an extension course. The target audience of the course comprises FADISMA students, as well as other academics and professionals working in the area. It is expected that, at the end of the extension actions, the members of NUSEC will be able to elaborate quantitative research instruments applied to law and the other participants of the extension actions will have improved their understanding of the subject.",Statistical consultancy; Jurimetry
963,"There is a growing interest in modeling series that have the characteristics of long dependence and seasonality. Series of monetary aggregates and financial income are examples of data that can exhibit these characteristics. Another growing interest is to model series that present high variability. Thus, it is interesting to study the processes that also take this property into account as well. The $k$-Factor GARMA $(p,\boldsymbol{u},\boldsymbol{\lambda},q)$ processes with symmetric $\alpha$-stable innovations, denoted by $k$-Factor GARMA $(p, \boldsymbol{u},\boldsymbol{\lambda},q)\_{S\alpha S}$, allow us to fit data that have the characteristics of long dependence, seasonality and high variability. The objective of this work is to propose estimators for the parameters of these processes. We extend the estimator proposed by Ndongo et al. [2010], for processes SARFIMA${(p,d,q)\times(P,D,Q)_s}\_{S\alpha S}$, for processes $k$-Factor GARMA $(p ,\boldsymbol{u},\boldsymbol{\lambda},q)\_{S\alpha S}$. We use the smoothed normalized periodogram and smoothed periodogram of correlations functions as estimators of the transfer power function [Stein, 2012]. Monte Carlo simulations were performed to verify the accuracy of the estimates of the $k$-Factor GARMA process parameters $(p,\boldsymbol{u},\boldsymbol{\lambda},q)\_{S\alpha S}$ and for that, the bias, the mean square error and the variance of the estimates were analyzed. We found that both the proposed estimators presented good estimates, in the sense of low bias, mean square error and variance for all parameters in the analyzed cases.","Long Dependency, Parameter Estimation, Stable Distributions."
1088,"The maximum likelihood estimation method is one of the most used, but it is sensitive to outliers. In some cases, an atypical observation is enough for the maximum likelihood estimator to have a severe bias and lead to erroneous conclusions about the characteristics of interest. Ferrari &amp; La Vecchia (2012) proposed a robust estimation procedure based on the minimization of an empirical approximation of q-entropy. The resulting estimator aims to balance robustness and efficiency according to the value assumed by a tuning constant q. In this work, this estimation procedure was applied to the beta distribution with parameterization indexed by the mean and by a precision parameter. The beta distribution is the most used in the context of regression modeling for continuous data observed in the unit interval. The performance of the robust estimator was evaluated together with the maximum likelihood estimator through Monte Carlo simulation studies in the presence and absence of data contamination. The results are preliminary and will be extended, in particular, to beta regression models.",Beta distribution; Robustness; Influence function; q-entropy.
1105,"Several methods have been proposed for the reconstruction of Gaussian networks with high-dimensional data, that is, in which the quantity of variables or nodes in the network is greater than the quantity of the sample. In this work we analyze three different methods for recovering networks from data with this feature. We use the method known as Graphical Lasso, which applies regularization processes based on LASSO, the Graphical Ridge method, which uses Ridge regularization and a new method that we are going to present, called LPC (Local Partial Correlation), or Local Partial Correlation. The evaluation was performed on data generated from random networks (Erdös-Renyi, Barabási-Albert, Watts-Strogatz ), in which we used the Characteristic Operation Receiver, or ROC curve, as a comparison measure for the retrieved networks. We also applied the methods presented in the reconstruction of the gene co-expression network with data from cervical cancer tumors","Network Reconstruction, Regularization, Gaussian Networks, Partial Correlation"
1329,"The use of multivariate techniques in experimental results represents a growing responsibility on the researcher, in the sense of understanding, evaluating and interpreting the results, especially the more complex ones. An aid in these tasks is understanding the basic characteristics of the data and their relationships. The starting point in interpreting the results is to characterize the form of their distribution. In this work, two statistical algorithms were studied: combination of histograms and two-step. The studies were carried out using a database of 161 samples in which the mass fractions of K, La, Lu, Na, U, Yb, CE, Cr, Cs, Eu, Fe, Hf, SC, Tb and Th were determined. using the neutron activation analysis method. The results were compared using histogram plots, normal probability and the Shapiro-Wilks Test. The results showed that the two-step algorithm is superior.","Data normalization, histogram, Shapiro-Wilks test"
1039,"Given the growing need to mitigate the environmental impacts caused by anthropic uses, reduce greenhouse gas emissions, recover degraded areas and, in return, increase food production, it is necessary to constantly monitor the use and transformations of the surface terrestrial. Time series of remote sensing data are critical to tracking surface changes over time and at large scales, and automated methods are needed to track different land use changes. In this study, the efficiency of two of these methods, BFAST (Breaks For Additive Seasonal and Trend) and DBEST (Detecting Breakpoints and Estimating Segments in Trend), in detecting different regime changes in NDVI/MODIS time series were evaluated. The results suggest that the methods are effective in detecting changes of greater magnitude, such as deforestation, and present a lot of uncertainty in changes of lesser magnitude, such as conversion from pasture to soybean or soybean to soybean and corn. It is suggested that further studies testing different method parameters be conducted.",nothing
1081,"In general, there are factors that influence the variation of prices charged for airline tickets, such as day of the week, flight time, the company chosen to fly and the advance of the ticket purchase in relation to the flight, etc. In this way, the objective of this work is to study the daily variation of air ticket prices in a period of 30 days, of the sections Londrina/PR - Curitiba/PR and Londrina/PR - São Paulo/SP considering the three largest airlines that make these excerpts. Linear and non-linear regression models were used to explain the relationship between ticket prices and the time taken to purchase tickets. It was observed that, in fact, the closer to the flight date, the higher the amount charged for the ticket, but this relationship was non-linear. Therefore, purchasing a ticket in advance can bring savings to the consumer.",nonlinear regression; airfare pricing; Software R
939,"Attention Deficit Hyperactivity Disorder (ADHD) is a psychiatric disorder that originates in childhood and may persist into adulthood, characterized by symptoms of inattention and hyperactivity. This work aims to study the possible confounding between ADHD symptoms and other psychological measures. A survey was conducted in which 309 people were interviewed, 196 women and 113 men. Respondents are students from 8 consecutive periods of the medical course at UFRJ. The variables of interest were patient-reported hyperactivity and inattention scores. As explanatory variables, we used physician-measured inattention and hyperactivity scores, as well as patient-reported anxiety scores. For such analysis, we propose the use of generalized linear models and Bayesian inference. ADHD, Hyperactivity, Inattention, Anxiety, Poisson Regression, Generalized Linear Models.","ADHD, Hyperactivity, Inattention, Anxiety, Poisson Regression, Generalized Linear Models."
1338,"A variety of authors have addressed the topic of calculating medians for multivariate data. Small (1990) presents several options for calculating the multivariate median. This paper discusses some of the options available in the literature, in particular calculations available in the R software package, Gmedian, which is the G1 median, also known as the L1 median. Simulation studies are presented to assess the robustness of this measure and compare it to the mean as a measure of the center of a distribution.","Gmedian, Robust Methods, Outlier, Monte Carlo Simulation, Software R."
921,"The univariate financial option pricing process generated and generates several works in the literature where there is criticism about the Black and Scholes model assumption that the asset-object volatility is considered constant over time. In addition, when financial options have more than one asset, care must be taken in the choice of their joint distribution, where the pioneer work considered the normal multivariate distribution, implying a linear correlation between the assets. Aiming to make the pricing process more realistic, this work aims to price bivariate financial options considering the Brazilian stock market through the DGARCH marginal process for asset-objects and copulas functions for the construction of the joint distribution. As a result, through tests and statistical criteria, a good adjustment of the marginal process and the joint distribution of the underlying assets was obtained, and in addition, the difference was obtained around 1% between the observed prices and the estimated considering the function payoff as the sum of the underlying assets. Another point to emphasize is the importance of choosing copula in the process of obtaining the fair price of the options. Finally, it justifies the importance of this work by the new approach of bivariate pricing and for being the first scientific application in the Brazilian stock market.",DGARCH; Copulas; Bivariate Option; Pricing.
1170,"Abstract: Traffic accidents are a serious problem in Brazil, as it is one of the main causes of death in the population. In this sense, this study aims to know the frequency of traffic accidents that occurred in the State of Pará, from January to December 2017, as well as to verify and analyze the existence of associations between the Accident Classification (with or without victims) and Type and Cause of the Accident, based on the use of the Statistical Correspondence Analysis Technique, based on data from Federal Highway Police records.",Correspondence Analysis; Deaths; Federal Highway Police.
1245,"The transition of a young person from high school to higher education is accompanied by a change in the teaching environment that can trigger profound changes in the student. This set of transformations does not include only those related to intellectualization and professionalization. No less important are the transformations in social and psychological relationships. In this work, we propose a method of investigation of social, psychological and motivational dimensions for performance and dropout in higher education, focusing on the reality of the Statistics course at the National School of Statistical Sciences.",nothing
943,"To define criteria based on measurements with a remote sensing device (RSD) to identify Otto cycle LDV [light duty vehicles] with high emissions of carbon monoxide, hydrocarbons or nitrogen monoxide, we used data from 179,142 vehicles in metropolitan São Paulo, with complete measurements. We adjusted statistical models of GAMLSS - Generalized Additive Models for Location, Scale and Shape class to test the influence of fuel type, VSP [vehicle specific power] and Proconve [Brazilian vehicle emission control program] phases on measurements of CO, HC and NO emission rates. The emissions were then conceptually subdivided into two groups: vehicles with normal and abnormal emission, this for the various pollutants in vehicles of L3, L4 and L5 phases. Latent variables were defined to indicate the distribution of vehicles in relation to those groups and phases. The algorithm EM-Expectation - Maximization was employed to identify all the parameters of the distributions. To determine the limits values for vehicles with high emissions of pollutants and Proconve phase, we use the 98% percentiles of the distributions set for vehicles of groups with normal emissions, therefore, the type I error was set at 2% and this percentage has been established considering the type II error of indicating the vehicle as normal emission when in fact it is a High emitter. Indicative values were determined with high emissions vehicles according to the pollutant and Proconve phase. 

","Key words: Vehicle emission, on-road measurement, vehicle specific power, remote sensing devices, I/M program, high emitters, carbon monoxide, hydrocarbons, nitrogen monoxide."
961,"Within the statistical techniques, regression stands out for its applicability to real problems in several areas by analyzing the possible associations between different variables in order to understand the behavior of these variables. In addition to predicting a variable of interest, using regression it is possible to interpret the relationships between variables. In this context, linear regression models are widely used. In several applications of linear regression models, in the most diverse areas, the constancy in the error variance (homoscedasticity) is assumed, in addition to the normality assumption. However, in most applications the assumption of homoscedasticity of errors cannot be assumed. In the most diverse areas, in diverse applied problems, it is also common to see that the distribution of errors is not known. In this class of problems, we can consider linear regression models with heteroskedasticity in an unknown way. These models take into account the ordinary least squares method to obtain estimates of the parameters of the linear regression model. However, a major challenge concerns the theoretical difficulties of obtaining interval inferences and hypothesis testing for the parameters that index heteroskedastic linear regression models. In this context, inferences via bootstrap resampling have become increasingly common in these models. In order to help the resampling process, the hcci package, version 1.0.0, in R, was implemented to construct interval estimates in linear models with heteroskedasticity of unknown form and to perform a quasi-F and quasi-t hypothesis test. The package provides HC, Tboot, Pboot, QF and QT functions",nothing
1115,"The interval-censored survival data appear very frequently, where the event of interest is not
observed exactly but it is only known to occur within some time interval. In this article, we propose
a location-scale heteroscedastic regression model based on the odd log-logistic generalized gamma
family of the distribution for modeling interval-censored data. We shall be concerned only with
parametric forms. The proposed model for interval-censored data represents a parametric family of
models that has, as special sub-models, other regression models that are broadly used in lifetime data
analysis.",Odd log-logistic generalized gamma distribution; Generalized gamma distribution; Interval censored data; Maximum likelihood; Regression model.
1422,"We propose to exploit stochastic volatility for statistical identi cation of Structural Vector Autoregressive models (SV-SVAR). We discuss full and partial identi cation of the model and develop e cient EM algorithms for Maximum Likelihood inference. Simulation evidence suggests that the SV-SVAR works well in identifying structural parameters also under misspeci cation of the variance process, particularly if compared to alternative heteroskedastic SVARs. We apply the model to study the interdependence between monetary
policy and stock markets. Since shocks identi ed by heteroskedasticity may not be economically meaningful, we exploit the framework to test conventional exclusion restrictions as well as Proxy SVAR restrictions which are overidentifying in the heteroskedastic model.""",structural vector autoregressions; identification by heteroskedasticity; stochastic volatility; MCMC.
1341,"The database approached in this work is the result of the application of a test for 11 234 individuals. The test consisted of 50 multiple-choice questions, each with five mutually exclusive options. Among the individuals evaluated, 4 small groups became suspected of cheating the exam. As exploratory analysis tools, among other methods, the graph of the characteristic curves, obtained with a Nominal Response Model and the dendrogram, were used, which was useful to determine degrees of similarity between the templates. The probabilistic analysis had the approach via Classical Test Theory, using the indices: K, K1, K2, S1 and S2 and also with a modern approach that is based on the Item Response Theory, in this case, using the indices: GBT and ω. From the probabilistic analysis of the Copy Indices, it can be concluded that the occurrence of fraud among individuals in the suspected groups is very likely, justifying a possible detailed investigation of its members.",Copy Indexes. Item Response Theory (TRI). Nominal Response Model.
1240,"The main characteristic of survival data is the presence of censorship, characterized by incomplete observation of the variable of interest to some individuals in the study, either on their initiative to leave the study, loss of contact during follow-up, termination of the study without the occurrence of event, etc. Thus, this study aimed to evaluate the impact of different censorship proportions on the estimation of the proportional hazards model parameters through a Monte Carlo simulation study. In general, it was observed by the relative bias, coverage probability that combining a higher proportion of failure with a good sample size can bring stability in the estimates of the effects of covariates.",Estimation; Survival; Relative Addiction.
1246,"In this work, we revisit the Wilson-Hilferty distribution. The proposed model is useful to describe devices with high chance of failure in the very early stages, but the hazard function levels off and eventually increases as the system get old.  The maximum likelihood estimators are explored under complete and censoring. Additionally, we present a Bayesian reference analysis for the generalized gamma distribution by using a reference prior, which has important properties such as one-to-one invariance under reparametrization, consistent marginalization, consistent sampling and leads to a proper posterior density. A simulation study compares the performance of the estimators with a clear advantage for the Bayesian approach.","Wilson-Hilferty distribution; Maximum likelihood estimators; Objective Prior, Reference Prior."
1343,"Data with excess zeros are often found in practice, a proper analysis considers the use of models that properly support the counting of null observations as well. In this study, the Inflated Zero Beta Regression Model with Normal Random Effects was applied to model an experimental dataset in order to describe the behavior, over time, of the expected incidence of bacterial citrus leaf canker in orange groves under the influence of genotype and rootstock of origin. The longitudinal model found that both the proportion of uninfected plants and the expected incidence among diseased plants are statistically significantly influenced by the action of time, however the effect caused by the rootstock and genotype significantly influence only the proportion of uninfected plants.",nothing
1204,"Based on microdata from the 2015 National Survey of Schoolchildren&#39;s Health, by the Brazilian Institute of Geography and Statistics, an indicator associated with the quality of health of schoolchildren in some Brazilian municipalities was constructed. The Item Response Theory Gradual Response Model was used in order to obtain the individual scores of the students. The model adjustment was performed with the 102,018 students participating in the research who answered at least 3 of the 65 graded items used. The results indicated significant differences in the indicators obtained by geographic region and by age group. It was also possible to identify items with greater and lesser power of discrimination, as well as items with greater and lesser difficulty.","gradual response model, indicator, health of students"
1102,"Locally stationary process is the class of processes that are approximately stationary in a neighborhood of each time point but its structure, such as covariances and parameters, gradually changes throughout the time period. We consider the case of time varying AR(1) (tvAR(1)) processes with $\alpha$-stable innovations. The $\alpha$-stable family of distributions is a generalization of the Gaussian distribution, which includes the possibility of handling asymmetry and thicker tails. Its estimation is difficult since its density function does not have a closed-form. Therefore, the usual estimation methods do not work. We propose the indirect inference, which is an intensive computationally simulation based method, to estimate $\alpha$-stable tvAR(1). In this paper, we obtain some theoretical results of the process and present simulation study of the estimation method.",locally stationary process; stable distributions; indirect estimation
1171,"In the framework of censored regression models the random errors are routinely assumed to have a
normal distribution, mainly for mathematical convenience. However, this method has been criticized
in the literature because of its sensitivity to deviations from the normality assumption. Here, we first
establish a new link between the censored regression model and the class of assymmetric distributions
studied by Ferreira et al (2015). Skew scale mixtures of normal distributions are often used for statistical
procedures involving asymmetric data and heavy-tailed. The main virtue of the members of this
family of distributions is that they are easy to simulate from and they also supply genuine expectationmaximization
(EM) algorithms for maximum likelihood estimation. In this work, we extend the EM
algorithm for linear censored regression models and we develop diagnostics analyses via global and
local influence. The EM-type algorithm has been discussed with an emphasis on the skew Student-tnormal,
skew slash, skew-contaminated normal and skew power-exponential distributions. Finally, to
examine the performance of the proposed model, case-deletion and local influence techniques are developed
to show its robust aspect against outlying and influential observations. The proposed methods
are verified through the analysis of several simulation studies and applying in real datasets.","Censored regression model; Heavy tails; Skew scale mixtures of normal distributions; EMtype
algorithm; Case-deletion model; Local influence."
1116,"We present a Bayesian methodology for estimating mixture regression models and propose the use of model selection criteria with a Bayesian approach, DIC and EBIC, to estimate the number of mixture components. We applied the proposed methodology to educational data, studying the relationship between the Basic Education Development Index and some socioeconomic and demographic data.","Regression Mixture Models, Bayesian Inference, MCMC, DIC, EBIC, IDEb"
1035,"In a model of stochastically disturbed processes, the observations of the original process can be disturbed, at each instant of time, by random noise. In this way, the observed process may no longer be a sample of the original process. So, the questions to be asked are: Is it possible to know whether or not a given sample is disturbed by some random noise? Is it possible to measure the degree of disturbance and discover the true law from which the data was generated? Is it possible to recover the original law of the data for any degree of disturbance? In this work we present methodologies to estimate the parameters of some stochastic disturbed models based on the models proposed by Galves et al and Garcia and Moreira. We assume that the original, hidden process is a variable-range Markov chain. This class of processes allows many applications because it is parsimonious in relation to the number of parameters and also quite malleable, encompassing the class of fixed-order Markov chains. We propose an adaptation of the Baum-Welch algorithm and a BIC Bootstrap estimator for the parameters of the analyzed models, whose convergence was demonstrated, and through simulations, we show that the proposed methodology is capable of recovering very well the true context tree of a chain. Markov curve with stochastically disturbed variable range, as well as the transition probabilities associated with this tree, within a range of disturbance levels. We were also able to recover the degree of disturbance whatever it was.",nothing
1346,"In this paper, we propose a flexible cure rate survival model by assuming that the number of competing causes of the
event of interested follows the power series distribution and the time to event follows a Weibull distribution. Indeed, we
introduce a new regression models denotedWeibull-Power-Series distribution (WPS) with cure rate, which can be used in
order to model survival data when the hazard rate function is increasing, decreasing and some non-monotonous shaped.
The new compounding regression model includes as special cases several well-known cure rate models discussed in the
literature. The model parameters are estimated by maximum likelihood. Further, for different parameter settings, sample
sizes, and censoring percentages, some simulations are performed. We derive the appropriate matrices for assessing
local influences on the parameter estimates under different perturbation schemes and present some ways to assess local
influences.
The properties of the proposed distribution class are discussed such as quantiles, moments and order statistics. Special
distributions are studied in some detail. Applications to real data sets are given to show the usefulness of the new
distribution class.","Weibull Power Series Distribution, Survival Analysis, Cure fraction modelling, Local influence diagnostics,
Latent failure causes."
1350,"Cook (1986) introduced a methodology to assess the influence of small perturbations in the data or problem formulation. Considering a regression model with errors in the multivariate variables with null intercept, we will use the local influence of Cook(1986) to develop a methodology to detect observations that are influential and masked.","regression model with variable errors, local influence, influence graph, normal curvature, ``forward&#39;&#39; search"
1033,"Multidimensional item response theory (MIRT) models use data from individual item responses to estimate multiple latent traits of interest, making them useful in educational and psychological measurement, among other areas. Practical situations may require a flexible multidimensional structure, different from the usual structure where all items evaluate the multiple latent traits considered. Existing MIRT models do not have in their composition a practical way for the user to define the items that evaluate each of the latent traits considered. To present and evaluate the idea of incorporating Q-matrices in MIRT models, we first formulate the multidimensional 2 parameter logistic model with the Q-matrix (M2PL-Q model). Then, we performed two simulation studies. The first simulation study examined the parameter recovery of the M2PL-Q, and the second simulation study examined the impact of the misspecified Q-matrix on the parameter estimates. In order to explore the effects on the estimation of the parameters caused by the incorporation of the Q-matrix in MIRT models, we performed an applied study with a real dataset with 2,922 individuals who answered 28 items on the Examination for the Certificate of Proficiency in English (ECPE). Incorporating the Q-matrix into the composition of IRT models provides some practical benefits. First, relationships between items and latent traits can be specified directly during test development and incorporated into the model through the Q-matrix in a user-friendly manner. In addition, these constraints provided by the Q-matrix decrease the number of discrimination parameters of the model, simplify the estimation process, increase the accuracy of latent trait estimates, and considerably shorten the estimation time as compared to a fully unrestricted MIRT.",item response theory; MIRT models; Q-matrix; diagnostic measurement; flexible multidimensional structure; multidimensional two-parameter model
1330,"Data visualization consists in representing data in some systematic form including attributes and variables for the unit of information. A simple and quick information can highlight possible errors with data just as it helps uncover interesting trends. There are traditional and new approaches for visualization methods. Data coming from different sources (SIVEP, IBGE, NOAA, MDS) were considered and are potentially useful for the effective understanding of malaria in the Amazon region. Using data of malaria, we illustrated the process of exploratory analysis and traditional visualization tools that can make the evaluation of high dimensional data available and feasible. Exploratory analysis tools (univariate and bivariate) were used to enhance the data visualization techniques. We believe that data integration and the exploration of visualization tools, such as those available using R Shiny, can assist decision making, provide significant contribution for understanding several processes and make massive amounts of available data in several systems become useful.","Visualization 
R-Shiny
Malaria"
1223,"We prove the existence and uniqueness of the invariant distribution of a non linear autoregressive time series of order one with uniformly distributed independent noise. The non linearity is of square root type. We propose an algorithm to determine this distribution. We prove that this distribution must satisfy a functional integral equation. Transforming this integral equation in a functional differential equation with both delay and scaling we were able to prove the existence of a one parameter infinite class of non negative solutions to the integral equation. Within this class there is only one possible choice of the parameter that generates a probability density function, which is the solution of our initial problem.",Invariant distribution; Non linear time series; Functional integral equation.
1354,"The main objective of clustering is, in general, a grouping or segmentation of certain observations into sub-groups (clusters), considering similarity measures as a separation criterion. One of the classic examples is the K-Means which uses the Euclidean distance between observations as a measure of similarity. However, this method has some limitations, especially with regard to data that are not linearly separable. Kernel K-Means are then used to work around this problem. Both methods were implemented in R, and as a result, it was noticed that, in addition to solving well the issue of lack of nonlinearity, Kernel K-Means also presents subtle better results in clustering when compared to classical K-Means.","clustering, machine learning, unsupervised, kernels"
1125,"In this paper, an extension of Poisson distribution was considered to model count data with any discrepancy in the frequency of the observation k, called k-Modified Poisson Distribution. This new distribution was generalized in the regression context. An illustration with data of fetus death notifications in Bahia State was considered.",Count Data;  k-Deflation; k-Inflation; Poisson Distribution
1300,"In this manuscript,  a convenient parametrization of this distribution is proposed in order to develop  regression models.  This distribution, referred to here as L-Logistic distribution, provides great flexibility and includes the uniform distribution as a particular case. Here, we show applications to estimate the vulnerability to poverty and to explain the anxiety are performed.  The  results to applications show that the L-Logistic regression models  provide a better fit than the corresponding beta regression models.","Bayesian analysis,  L-Logistic distribution, Regression analysis,  beta distribution, Sensibility analysis"
1351,"The present work discusses the use of games in the teaching of Statistics. Surveys were carried out regarding the indications of national curriculum parameters (PCN), statistical literacy and the use of technology and games in education, including statistics. In this context, we present the digital game Auction of the lowest bid. Also, we suggest alternatives for using this game in the classroom and comment on the application made with two groups of students, in order to enable this game as an additional teaching tool.",teaching; statistic; games
1224,"The discovery of knowledge in datasets is a semi-automatic process of extracting useful information from a quantity of data that makes it impossible for this process to be performed manually. By semi-automatic it is characterized by the use of computer-based tools for the discovery process, and the analyst&#39;s guidance is indispensable. The information obtained by the discovery process usually takes the form of explanatory patterns, often referred to as models. With the introduction of a new class of models known as support vector machines, or support vector machines (SVM). In short, this class of models is based on the construction of optimal hyperplanes to classify labels - originally binary - as well as using kernealization to increase the flexibility of the model, with Gaussian kernealization having a high predictive capacity and frequently used. This work intends to carry out a comparative study of the predictive capacity using different kernels: Linear, Polynomial, Gaussian, Laplacian and Cauchy. Generality is assessed through replications of the k-Fold cross-validation method.",nothing
1359,"In the present work we approach the estimation of the parameters via maximum likelihood estimators in the promotion time model with Weibull distribution for the failure times of susceptible individuals. The inferences are based on asymptotic approximations that in small samples can present distortions. Therefore, we propose corrections via bootstrap for point estimators and standard bootstrap confidence intervals. We evaluated, via Monte Carlo simulation, the usual punctual and interval estimators and the proposed corrections. The results presented show more reliable estimators when considering the proposed bootstrap corrections. Regarding the interval evaluation, it was verified that as the sample increases, the coverage rate values get closer to the significance levels for the two versions of the considered confidence intervals.",Promotion time model with Weibull distribution; Bootstrap; Maximum likelihood estimators; Inferential improvements.
826,"Accurately measuring epigenetic marks such as DNA methylation and hydroxymethylation at the single nucleotide resolution level has increasingly become an intense focus of research in biological sciences. We introduce the R package MLML2R, which provides maximum likelihood estimates via EM-algorithm of DNA methylation and hydroxymethy-lation proportions when data from the DNA processing methods bisulfite conversion (BS), oxidative bisulfite conversion (ox-BS), and Tet-assisted bisulfite conversion (TAB) are available. For any combination of any two of the methods, we derived the exact constrained maximum likelihood estimate in analytical form, which greatly decreases analytic processing time and computational burden, common bottlenecks when processing high- throughput data. The MLML2R package is flexible as takes as input both, intensities from Infinium Methylation arrays and counts from Next Generation Sequencing technologies, and shows advantages over other available packages. Specially, we have implented for the first time in R the constrained maximum likelihood estimate of DNA methylation and hydroxymethylation from TAB data.","Maximum likelihood, DNA methylation, 5-methylcytosine, 5-hydroxymethylcytosine, MLML2R"
1209,"In 2016, there was a large epidemic of the Zika virus throughout Brazil, causing several clinical and social problems. Zika has become one of the most serious public health problems in developing countries in recent years, and consequently the Aedes aegypti mosquito has regained notoriety for being the vector of viral diseases such as: dengue, yellow fever, chikungunya and Zika. Using time series techniques, this work aims to model the rate of dengue cases collected at the Zoonoses Control Center (CCZ) in the city of Natal/RN and make predictions. The ARIMA(3, 0, 3) model was identified through the auto.arima() function in the R language.",nothing
971,"The probabilistic prediction of the occurrence of extreme winds is of great importance for the planning of projects in agricultural, structural and civil engineering. The need for this planning is to avoid damage and damage that can be caused by the occurrence of high intensity winds. One way to model these phenomena is through generalized extreme value distribution (GEV). Given these facts, the objective of this work was to evaluate the application of Bayesian Inference in predicting the occurrence of maximum semiannual winds, in Bauru-SP, using the generalized distribution of extreme values (GEV). The maximum wind speed data (km h-1) were obtained from the National Institute of Meteorology (INMET), for the period from August 2006 to December 2016. The maximum observed in the last eight semesters were used to assess the accuracy and the average prediction error for different return times. The trivariate normal distribution was used as a priori, and the information about the maximum wind speed in Piracicaba-SP was used to elicit the hyperparameters. In addition, different prior distribution combinations were created, using variance structures by multiplying the covariance matrix by 1, 2, 4 and 8. The application of Bayesian inference with informative prior, based on data from Piracicaba-SP, with A priori variance structure multiplied by eight was the most adequate to predict the behavior of the maximum wind speed in Bauru-SP, providing more accurate and precise results, leading to prediction errors smaller than 3%.",Generalized distribution of extreme values; prior distribution; maximum likelihood method; prediction; turnaround times
1199,"Guarana (Paullinia cupana) is very popular in Brazil, being used for the production of guarana flavored beverages. It has great economic potential, in addition to having astringent and antioxidant properties. Brazilian law (Law No. 8918 of 07/14/1994) does not allow the addition of synthetic caffeine or caffeine from other sources in beverages, so to obtain a beverage with a greater amount of this ingredient it is necessary to obtain a more concentrated guarana extract. Thus, the present work aims to optimize the caffeine extraction process from guarana seeds (P. cupana). For this purpose, a factorial design of orthogonal matrix (OA_9(3^4)) fractional 3^{4-2} experiments was used, performed under the variables of the extraction process, being solvent modifier, time, temperature and extraction pressure . The quantification of caffeine in the extract was analyzed by high performance liquid chromatography (HPLC) and the planning data were analyzed by the R 3.3.4 program, in order to obtain the optimum point by surface response methodology (MSR). The results indicated that the pressure and temperature parameters are significant in the extraction of caffeine, in which a pressure factor of 100 bar together with a temperature of 60º favors the highest percentage of caffeine, given the experiment carried out. The model used was well adapted to the data that were obtained through a fractional experimental design, which provided good results regarding the efficiency of extraction of caffeine, efficiently, quickly and with low laboratory investment.",Experiment Planning; Applied statistics; Regression Model; Caffeine; Response Surface Model.
1045,"This is an ongoing work where our main objective is to propose a new procedure for estimating stochastic volatility models with long memory latent volatility. Our proposal is based on a state space approximation of the log variance process, and the disturbance error is approximated by a mixture of normals. All parameters are estimated by Maximum Likelihood, with the likelihood function expressed in terms of the innovations of a Kalman Filter algorithm. The performance of our proposal is evaluated by a Monte Carlo experiment when the log variance follows an ARFIMA(0,d,0) process","mixtures, non-gaussian errors, long memory"
1203,"In this work we propose a new spatial capture-recapture model for estimating abundance in an open animal population. The model takes into account the birth and death rates during the study period, the spatial location of the captures and recaptures of animals and their possible geographic displacements over the researched region over time. For the inferential process, we adopt the Bayesian approach and present an efficient Markov Chain Monte Carlo (MCMC) algorithm using augmented data techniques in the likelihood function. We analyzed the frequentist properties of Bayesian estimators through a simulation study and applied the proposed methodology to a real dataset of two species of adult harvestmen captured in a cave in southeastern Brazil.","Spatial capture-recapture model, Open population, Bayesian inference."
1034,"Larval stage duration (LSTD), the time interval (t) between egg eclosion and onset of pupal stage, is a key variable in entomological studies for evaluating treatment effects on insect development. This variable is a component of population growth rate in demographic studies, which are important to evaluate suitability of diets, chronical and sub lethal effects of pesticides or growth inhibitors on insect species. LSTD observations, measured in days, are referred to as ‘censored’ whenever the insect die before reaching pupal stage. In this work, we fit Cox model with qualitative predictors to evaluate the influence of host species (cowpea, soybean and weeds) on the time pattern of the onset of pupal stage of Chrysodeixis includens, represented by the cumulative distribution function of LSTD (F[t]=1- P[T>t]). C. includens is an important pest of cowpea, soybean, cotton and sunflower crops but also can survive in some weeds. We performed an exploratory LSTD analysis by fitting F[t] via Kaplan-Meier method and comparing the F[t] among treatments (species) by the Log-Rank test with Sidàk adjustment for multiple comparisons. Evidence of host plat effect on time pattern of pupal onset were quantified using a priori comparisons among F[t] performed via contrasts among Cox model parameters.  Analysis were performed using LIFETEST and PHREG Procedures of the statistical software SAS/STAT®. The proposed methods allowed inferring that the host species cowpea contributes to greater larvae precocity when compared to the remaining species evaluated














































































","Survival analysis, Cox model, larval development, insect life cycle"
1186,"In this work we present the Zero-Modified Poisson Model with Normal random effect for longitudinal data, which is an extension of the Zero-Modified Poisson model. The inclusion of the random effect in the model allows us to deal with count data that have an excess (or deficit) of zeros, and that are correlated over time. A simulation study was elaborated in order to illustrate the developed methodology. Furthermore, in order to illustrate the proposed procedure, we analyzed a set of real data about the self-cleaning behavior of rats. Such a model proved to be flexible to model a dataset with correlated observations without any prior assumption of the frequency of existing zeros.","Zero-Modified, longitudinal data, random effect. zero-inflated, zero-deflated."
1112,"A new distribution family is proposed for modeling data with limited response in the unit range and some properties of the new family are presented. The proposal is based on the composition of a base distribution and the quantile transformation of another family with the same support as the base distribution. Furthermore, the new family is extended to regression models as an alternative to the regression model with unit interval response. Inference procedures based on the likelihood theory are also presented. An application to real data to illustrate the use of the new family is considered.","Normal-Power Distribution, Quantile Function, Regression Models, Generalized Linear Models."
1003,"The present study proposed a bivariate regression model with Weibull distribution via Clayton copula for censored data. To exemplify the use of the regression model, it was applied to a survival database with two events of interest (mild and severe retinopathy), the final model was defined by the Backward method with significant variables at the level of 10%. The model inference was made using the likelihood function described in Lawless (2002) and parameter estimates were found using the optim function of the R software. The model presented a good fit to the retinopathy data.","Clayton copula, bivariate survival, regression model."
1085,"Time series for counting data are records of the relative frequency of occurrence of certain events in successive time intervals, and their main characteristic is the dependence between observations. The Generalized Linear Moving Average Autoregressive Model (GLARMA) is one of the procedures proposed to model series that have this dependency structure. In this work, a Monte Carlo simulation study will be carried out to empirically evaluate the performance of the maximum likelihood method, adopted for the estimation of the model&#39;s parameters. In addition, a real series of respiratory disease will be analyzed using the GLARMA model, with environmental pollution covariates, and its fit will be compared to the fit using the generalized linear model.","generalized linear model, time series, environmental pollution"
1172,"Binary time series models can be used to calculate the probability of occurrence of an event of interest when the probabilities are dependent on past observations in the short term. In this work, we propose a new model for binary time series, which is a modification of the GARMA model. We analyzed a set of real data that contains only indication of the presence or absence of rain in a city located in the central region of the state of São Paulo.","GARMA Models, Binary Time Series, Occurrence of Rain"
1118,"Nonlinear models such as the Logistic model and the Gompertz model are widely used to describe various biological processes through the growth curve given by the model equation. The objective of this work was to adjust the Chanter model, as well as the Logístico and Gompertz model, using a dataset of the cocoa fruit of the clone SIAL - 105 whose length and diameter measurements were taken from Brito e Silva (1983) and correspond to a experiment carried out at the Cocoa Research Center, in Ilhéus-BA. The Chanter model is a hybrid between the Logistic model and the Gompertz model whose parameters can be interpreted similarly. The comparison of the goodness of fit between the models was made using the following statistical measures: the Akaike information criterion (AIC), the Akaike weight criterion, the Bayes information criterion (BIC), the residual standard deviation (DPR ) and the measures of nonlinearity vice of Box and curvature of Bates and Watts. It was found that the Chanter model among the models studied in this work is the most suitable for the adjustment of data on the cocoa tree.","Nonlinear Regression, Chanter Model, Nonlinearity and Cocoa Measures."
1138,"The work introduces the nonparametric model with first-order, normal, and $t$-Student autoregressive symmetric errors. We define the corrected AIC criterion as a form of model selection, and the quantile residuals to assess the adequacy of the conditional error distribution. An application with real data was carried out to illustrate the use of the model.","non-parametric model, first order autoregressive symmetric errors, quantile residues."
1025,"In this work, an extension of the Power Series family of distributions was considered to model count data with any discrepancy in the frequency of observation k, called the Power Series k-Modified family. This new family of distributions was generalized in the regression context. An illustration with data from fetal death notifications in the state of Bahia was considered.",Count Data; Power Series Family; k-Deflation; k-Inflation
907,"Asymmetric distributions have been used in data modeling in several areas. Medical, genetic and financial, among other applications. Asymmetric distributions such as alpha-stable, normal, Student&#39;s t and asymmetric Laplace are the main options. In this work we compare the modeling of these distributions and test the ability of the AIC and BIC criteria to identify the best model for a data set. We also apply this methodology to cryptocurrency data.","Distribuições assimétricas, alpha-estável, normal assimétrica, t de Student assimétrica, Laplace assimétrica, criptomoedas."
932,"Multiplex networks are becoming more and more prevalent in many fields and have emerged as a powerful tool to model the complexity of real networks. There is a critical need to develop inference models for multiplex networks that can take into account the dependency between the different layers of the network, especially when the objective is to detect communities. In this work, this gap is filled with the proposal of an innovative and efficient Bayesian model for the detection of communities in multiplex networks. One of the main features of our model is that it allows communities to vary in different layers, through a hierarchical structure, which differs from many of the existing methods for modeling multiplex networks, which require communities to be fixed for all layers . Marginally, a stochastic block model (MBE) is assumed for each layer. Efficient MCMC algorithms were developed to sample the posterior distribution of unknown model parameters. The developed algorithms were applied to simulated data that demonstrate their good performance. Finally, applications were made to two examples of real data: first considering data on friendship between teenagers in a school in Glasgow, and then considering a multiplex network that represents different types of relationship between employees of a tailor shop in Kapferer.","Multiplex Networks, Community Detection, Block Stochastic Models, Hierarchical Models, Bayesian Inference"
1202,"This article investigates the use of dynamic models to estimate the volatility of time series using the Particle Filter numerical approximation method. In this article, the results of the estimates of the time series Stochastic Volatility model obtained good estimates for the model parameters. This model was applied to daily financial series linked to the assets of the Brazilian stock exchange, IBOVESPA. The result obtained was used to determine the optimal allocation of assets based on the Value-at-Risk risk exposure measure by applying the Monte Carlo Method, with simulation of 10000 portfolios for 252 periods in 10000 simulated scenarios based on the volatility model estimated.",Stochastic Volatility Model; Particles filter ; Monte Carlo Method; Dynamic Models; Portfolio allocation
1111,"Very common problems related to hospitals and health units, in general, it is the cost management and the perceived quality of them. A measure accepted in the literature for perceived quality is associated with the number of deaths. Thus, this study aims to analyze the characteristics of patients that affect the hospital cost of hospitalization as well as the probability of the patient&#39;s death. The results found are in some agreement with the literature when they state that age positively affects these variables and that there is an impact of the severity of each case on the probability of the occurrence of death. The study shows that body mass index, insulin dependent diabetes, cardiopulmonary bypass time, renal failure, mitrial failure, the character and type of surgery are relevant in determining the cost of a patient. When analyzing the probability of death, results were found that indicate that orovalvular surgeries are more likely when compared to other surgeries, in addition to this variable, the elective nature negatively affects this probability when compared to urgent and emergency cases. The results also show that reoperation, stable angina and the index that predicts the mortality of individuals who will undergo cardiac surgery, as well as the age of the patients, are significant to explain the probability of their death.",regression models; hospital cost forecast; occurrence of deaths; logit.
1248,"This work aims to study and analyze the behavior of censored data when the variable of interest is a time series. Censored data is understood when only partial observation of the response variable is obtained, which was interrupted for some reason, or when there is a number of values grouped into a threshold value. On the other hand, a time series is a collection of observations made sequentially over time, where the most important feature of this type of data is that neighboring observations are dependent, that is, the order of the data is crucial for the analysis. The combination of these concepts has a wide application in several areas of knowledge, with more evidence in the environmental, financial and economic areas. A statistical method widely used to study the relationship between two or more variables is the linear regression model, which is considered when there is a linear relationship between the variable of interest and the explanatory variables. The motivation for this work came from Correia (2015), who presented static and dynamic regression models for rates or proportions, that is, a data study with responses at limited intervals. In this case, the interval [0,1]. The inference will be considered under the Bayesian approach.",nothing
1046,"In the present work a study was developed for the occurrence of dengue in the urban area of Rio Claro, São Paulo - Brazil. The Municipal Health Foundation of the municipality provided data on reported cases in 2011. The main objective was to analyze different approaches to spatial models for the distribution of the disease in the municipality, divided by census tracts, and also to relate the disease to socioeconomic factors. A CAR spatial model with a classical approach and a CAR structure for a Bayesian spatial model were used. The results do not indicate notable differences between the analyses, and the classical approach is simpler compared to the Bayesian one. Using thematic maps, the estimates of the models were compared. Analyzes were performed using the R software.","Classic CAR Model, Bayesian CAR Model, Spatial Models."
1327,"This work aims at modeling count data that vary spatially and are inflated from zero. The Poisson log-normal model will be used to accommodate the overdispersion of the data and a mixture to structure the inflation of zeros. For the spatial structure, regional explanatory variables will be used and a spatial structure will be imposed in the modeling. The focus of the work is Bayesian and to infer about the parameters and the latent variable, Monte Carlo methods via Markov chains will be used. Abstract This work aims to model counting data that varies spatially and is inflated from zero. The log-normal Poisson model will be used to accommodate the overdispersion of the data and the mixture to structure the inflation of zeros. For the spatial structure, regional explanatory variables will be used and spatial structure will be imposed in the modeling. The focus of the work is Bayesian and to infer about the parameters and the latent variable Monte Carlo methods via Markov chains will be used.","MCMC, Poisson, log-normal, ZIP, mistura"
1349,"Citrus canker caused by the bacterium \textit{Xanthomonas citri} subsp. \textit{citri} brings several losses to citrus crops and industries. The identification of orange varieties resistant to this pathogen can help to overcome this problem. For this, a study was carried out involving two trials, which were evaluated using generalized mixed models. Mixed models stand out for being able to incorporate the dependence and the correlation structure of the errors, since observations made on the same individual do not have a null correlation. The methodology proved to be adequate, as it enabled the detection of the most susceptible varieties, meeting the necessary assumptions for modeling and providing reliable results.","Citrus Canker, Xanthomonas citri, GLMM, Mistos Models."
1191,"The default rate is a constant concern in companies today. Whatever the size of the company, your attention is always focused on lowering your rate to avoid financial losses. In the real estate sector, rental contracts for residential properties represent the majority of services used by clients, and the main problem with this service is the lack or delay in the payment of contracted rents. A practical solution to this issue is to use statistical models that can predict in advance the probability that a new customer will default or default on a future contract. Most companies already adopt the procedure of filling out a customer record at the time of closing a deal, so the construction of these models can be carried out directly on the registration data of existing customers who have entered into a property rental contract . In this work we present the case of a real estate company for which four forecast models were built: Artificial Neural Networks, Decision Tree, Logistic Regression and Discriminant Analysis. The results obtained in the evaluation of the models were satisfactory and their use will allow the company to respond quickly and be more assertive in the decision to close or not a new contract, thus reducing its default rate.","default, MLP Neural Networks, Logistic Regression, CHAID Decision Tree, Discriminant Analysis, real estate rental."
1285,"In this paper, we approach the estimation of the LGD (Loss Given Default), an important risk component in the use of the IRB (Internal Ratings Based) advanced by financial institutions. We developed and characterized the zero-inflated bimodal beta probability model, created the zero-inflated bimodal beta regression model (RBBZ) and proposed the estimation of the LGD risk component through the RBBZ model. The proposed approach is compared with beta regression models, zero-inflated beta regression and the SVR (Support Vector Machine) algorithm. In this study, we found that the RBBZ model presents satisfactory performance, showing it as a modeling alternative for estimating the LGD that has inflation at zero and bimodality in observations above zero.","Loss Given Default, regressão beta bimodal inflacionado em zero, support vector
regression machines"
1315,"In this work, diagnostic methods in semiparametric models with negative binomial response are presented. The model with only one non-parametric explanatory variable that was fitted using P-splines was considered. The penalized maximum likelihood estimates were obtained through a backfitting-type estimation procedure. Normal probability plots for the residual component of the deviation with simulated confidence bands and diagnosis of local influence were developed to assess the adequacy of the adjusted model and the sensitivity of the estimates. Finally, an illustrative example is presented in which the daily maximum of the average ozone concentration per hour (in ppm) in Los Angeles is adjusted through a model with a negative binomial response.","count data, negative binomial distribution, local influence, non-parametric methods, P-splines."
1271,"The exponentiated gamma distribution is very important in Survival Analysis and whose risk function presents different shapes such as crescent and bathtub shape. This distribution can be applied in several areas of knowledge such as biological and industrial. In the present study, maximum likelihood inferences and Bayesian inferences were applied to estimate the model parameters in the presence of complete data and progressively censored data. Assuming non-informative prior distribution, we present a Bayesian analysis using the MCMC (Markov Chain Monte Carlo) methods. A simulation study was carried out for different sample sizes and Type II Progressive Censoring schemes, in order to compare the performance of the estimators. In addition to the study under simulation, an application of both estimation methods to a real set of literature data was also performed.","Keywords: Exponentiated Gamma, type II progressive censorship, likelihood, Bayesian inference, MCMC."
1254,"We introduce a two-parameter distribution by using the continuous-continuous compounding approach. The new model is an extension of the Weibull distribution, which is obtained by taking the minimum of two continuous independent random variables. The proposed distribution is more flexible than the Weibull, and other competitive two-parameter models, since its density allows bimodal, left-skewed, right-skewed and reversed-J shapes. We show how to perform point estimation on the parameters that index the proposed model by using maximum likelihood method and a Monte Carlo experiment is carried out to evaluate the performance of those estimators for finite sample sizes.","Bimodal distributions, compounding approach, Weibull generalizations."
1244,"In repairable systems, it is very important to consider the choice of the appropriate model to the event time. In a lot of situations the nonhomogenous Poisson process model very well the data. But the heterogeneity between the systems must be included, it can be done by the frailty model, that is, a incorporation of an unobserved random variable multiplying the rate of occurrence of failures. The effect of accumulating event occurrences also must be considered, adding a multiplicative term in the rate of occurrence of failures. The propose of this paper is to include these terms in the model and suggest a method of estimation of the parameters considering the frailty in a nonparametric form. This model will be illustrated by an application with a real data set.",Repairable systems; nonhomogeneous Poisson process; power law process; nonparametric frailty; accumulating event occurrences.
1038,"The item response theory has gained prominence and strength, serving as a basis for statistical approaches to present themselves more refined. Based on this, this study was developed from a bibliometric research in order to expand the knowledge regarding citations and co-citations related to works developed in the scientific community with the theme Item Response Theory. For this, the Web of Science database was used, from 2008 to 2017, where citation and co-citation data were analyzed through text maps made with the Vosviewer software, demonstrating through cluster analysis and links made between authors and keywords.","Item Response Theory, citations, co-citations, bibliometrics."
1009,"In this work we present the regression model for multivariate count data using the COM-Poisson Zero-Inflated type I (ZICOMP type I) distribution. The proposed model is an extension of the Zero-Inflated Poisson type I (ZIP type I) multivariate model. We develop important distribution properties and propose a regression model. In a simulation study, the regression model with the proposed distribution presented very satisfactory results.",nothing
1174,"We study a generalization of the M/M/1 queuing model, coming from the idea of fractional derivatives. We discuss parameter estimation, data simulation and a possible generalization for the M/M/k queuing model.",nothing
1208,"Generalized linear mixed models (GLMM) have been widely applied for analysing correlated data in
the exponential family of distributions in which one may have, for instance, clustered, longitudinal and repeated-
measurement structures for the data. Sensitivity analysis in general requires, under the marginal approach, integral
approximation or numerical methods for obtaining analytical and interpretable expressions for influence measures. In
particular, some authors have discussed the derivation of the normal curvature of local influence in GLMM. For
instance, Ouwens et al. (2001) have presented approximations based on the Bayesian perspective, whereas
Rakhmawati et al. (2016) have discussed numerical solutions for obtaining the solution for the normal curvature in
some GLMM. In this work we apply Laplace approximation jointly with some mathematical procedures, such as the
intermediate value theorem, to derive analytical and intepretable expressions for the normal curvature in random
intercept generalized linear models. We showed under some usual conditions that Laplace approximation can be
used, since the involved functions in the procedures are unimodals. Numerical studies in which the proposed
approximation is compared with the “exact” value of the normal curvature, obtained by the adaptative Gauss-Hermite
quadrature, present excelent results. We illustrate the proposed methodology with at least one application to real data
sets.","Local Influence, Generalized linear mixed models, maximal curvature."
891,"The objective of the work is to present a composite criterion function that optimizes the prediction variance taking into account the number of degrees of freedom for the pure error in factorial experiments with fixed block effects. The efficiencies of the constructed designs are illustrated and compared with the efficiencies of the classic I-, IP-, D- and DP-optimal designs through an example that investigates the best pasta recipe.","Optimal designs, I-optimality, Lack of fit"
1126,"Linear regression models with errors in the skew-t family, which includes normal, Student-t and skew normal distributions as particular cases, have been considered a robust alternative to the normal model. However, presence of outliers can affect estimates. The main goal of this work is to identify situations in which skew-t models are more vulnerable, robustness proprieties related to skew normal and skew-t models and to adapt a robust estimator for this class of models. We could find that outliers in the right tail have stronger impact on the estimates of shape parameters. By a simulation study, it was possible to see that the effect on parameters depend on the proportion of contaminants and how far they are from the mass of the distribution. At last, we tested an adapted robust estimator that proved to have good behavior to outliers in datasets with moderate skewness.","Statistical Inference, Robustness, MLE, M-Estimators, Skew-t models"
1164,"For more than two years, the Public Ministry of the State of São Paulo has been dedicated to the knowledge of the phenomenon of the disappearance of people. Through the Program for Locating and Identifying Missing Persons –PLID, the need to adopt a public policy on the subject was demonstrated. The program allowed the investigation of the disappearance of vulnerable people and the creation of a database at the State level. In this study, we analyzed a total of 24,261 disappearance complaints filed in 2013 and 2014, according to the Prodesp system of the Government of São Paulo. We use text mining techniques to work with Police Reports, Naive Bayes model to automatically classify possible causes of disappearances. As a result, we observed that individuals have different patterns of disappearance ages in relation to sex and cause of disappearance. With the results, effective action strategies will be defined in combating organized crime and locating the missing.","missing; data mining, police report"
1041,"Abstract This work was carried out with the objective of evaluating the profile of the graduates of the Administration Integrated to High School course at the Federal Institute of Education, Science and Technology of Rio Grande do Sul - Campus Canoas. The feedback from students graduated in the course is extremely important for the Institution, as in addition to having completed the course in IFRS and being able to evaluate it more broadly, they have already empirically experienced the contributions of the course in their professional and academic life. Analyzing this information, it was possible to know the situation of students who graduated from the IFRS and identify points to be improved both in the Institution and in the Administration course. A questionnaire was applied using the Google Forms tool for classes that graduated in 2014, 2015 and 2016, in which we sought to know what was the relationship of the graduate with IFRS while at the institution, what is their current situation, what is their current situation. its evaluation of the course and the Institution and on the IFRS communication. The results found demonstrate a unanimous interest of the graduates in continuing their studies, in addition to an excellent evaluation of the teaching, teachers and the basic curriculum of the Administration course. In addition, it demonstrates that the Institution managed to develop most of the skills expected for the student who graduated from the Technical course in Administration Integrated to High School from IFRS - Campus Canoas.",Graduates. Integrated High School. Assessment
1113,"In many practical situations the relationship between a response variable and one or more covariates is curved. Among the different ways to represent this curvature, Royston and Altman (1994) proposed an extensive family of functions called Fractional Polynomials (PF). Bové and Held (2011) implemented the Bayesian paradigm for PF under the assumption of normality of errors. Its methodology is based on a hyper−g a priori distribution, which, in addition to many interesting asymptotic properties, guarantees a consistent Bayesian weighting of models (BMA). In this work, the effectiveness of this technique is verified from simulations where the underlying function is PF1 (degree one) or PF2 (degree two) and the errors are normal. In a scenario of only one covariate, for different values of n, R2 and β, the simulation results show good success percentages. However, the classical approach showed better results when the underlying function was given by PF1, whereas, in the Bayesian approach, the results were better when the function was given by a PF2.","Priori hyper-g, underlying function, simulation."
1256,"This article aims to measure the participation of the main economic variables related to inflation through a dynamic model. In view of the interventions on prices monitored between 2013 and 2015, which were followed by the detachment of inflation from its target, as well as the influence perceived by the food and beverage group on inflation, we seek to verify which the impact of these two segments on inflationary dynamics for the period between 2007 and 2017. The suggested results highlight the significant impact of regulated prices on Brazilian inflationary dynamics.",Inflation; Price Groups; VAR.
1215,"Machine learning techniques stand out for the versatility employed in many business areas that involve data prediction and characterization problems. This work approaches a classification model using Bayesian Networks as a focus, a technique that is increasingly requested and studied in the scenario of machine learning and “BigData”. The model, Naïve-Bayes, considers independence among all covariates in the study, which can rarely be proven. This is one of the models that are used to predict Dota2 matches, in which periodic data collection was performed using the API available by the game manufacturer. The performance of the model was acceptable for future predictions, however, the superior performance of other models is notorious, such as: TAN (Tree Argumented Naïve-Bayes) (Friedman, N. et. al.(1997)) compared to the model Naïve-Bayes.","Prediction, Dota2 Matches, Naïve-Bayes"
1352,"Digital image processing means the manipulation of an image by a computer so that the input and output of the process are images. Image compression is the application of data compression on digital images. In effect, the objective is to reduce data redundancy in order to store or transmit that same data efficiently. Thus, this article aims to study the effects of using the numerical approximation technique called rounded cosine transform (RCT) (Bayer and Cintra, 2010). Additionally, the particular case of the proposed transformation in 8 × 8 matrices and compare it to exact DCT.","processamento digital de imagens, transformada discreta do cosseno, transformada arredondada do cosseno"
910,"This article aims to apply stochastic models to simulate the dynamics of an area of a Mixed Tropical Forest area located in the Tapajós National Forest, in the municipality of Belterra, in the state of Pará, Brazil. Data are from 25 permanent plots of 62.5 ha, with annual measurements of all individuals with DBH greater than or equal to 5 cm obtained in 1981 and 1983. Three methods based on Markov chain were used to model and predict the distribution diametric of individuals in 1987. Finally, statistical criteria were used to present the most appropriate model for the phenomenon being studied and the most appropriate for modeling the dynamics of the area.","Processos Estocásticos, Cadeia de Markov, Floresta Nacional do Tapajós"
1228,"This work has as its theme the application of main components to help the construction of models that perform automatic recognition of handwritten digits. Multinomial response models were adjusted through neural networks, using the principal components referring to the data as explanatory variables, testing an increasing number of components and checking the accuracy of each model by 5-fold cross-validation. It was concluded that the optimal number of main components to be used was 21, which results in an accuracy of approximately 88%. At the end, the marginal utility measure is proposed, which represents the utility of each pixel by itself in the classification of the number, which can be used in contexts of data compression, for example.",nothing
1211,"In order to unify several important discrete models in a single conceptual framework, Cordeiro et al. (2009) proposed a new class of models in generalized nonlinear power series (MSPNLG). Silva et al. (2017) obtained a correction for the second-order bias of the maximum likelihood estimators of the parameters in this class of models. In this work, expressions in matrix notation were obtained from the Bartlett and Bartlett-type correction factors to the likelihood ratio, score, and gradient statistics, respectively, in the MSPNLG. We evaluated and numerically compared the performance of the proposed tests through Monte Carlo simulation in relation to size and power, in finite samples.",Bartlett&#39;s correction; Type-Bartlett Correction; Chi-square distribution; Likelihood Ratio Test; Test Score; Gradient Test.
1014,"In this work we developed the zero-inflated bimodal beta distribution, we presented the hope and the variance for this distribution, we defined maximum likelihood estimators and built the regression model for this probabilistic model, we performed a simulation study to evaluate the performance of the maximum likelihood estimators (obtained through the EM algorithm initialized with K-means method) of the parameters of the bimodal beta regression model inflated to zero.","distribution mixing, zero-inflated beta bimodal regression, EM algorithm, K-means."
1169,"A model of democracy in the world, Cape Verde, which also faces crime as a social problem common to any nation, is segmented into an archipelago of volcanic origin with 10 islands (9 inhabited) and 8 islets, and the administration of each region takes place by the congregation of &quot;Parishes&quot; that form the &quot;Counties&quot;, equivalent to Municipalities. With the need for greater attention from public policies to the activity of this phenomenon throughout the territory, for the maintenance of order, it becomes relevant to investigate the patterns of criminal actions in these Municipalities. This exploratory study analyzes the relationship between Cape Verdean Municipalities with the categorized types of crimes that occurred in the country in the period from 2013 to 2016, which after applying the chi-square test, criterion β for confirming dependence between the categories of qualitative variables, through the multivariate statistical technique Correspondence analysis, the results of the disregard of urban borders and a large concentration of population were obtained, showing the predominance of crimes such as Robbery and Homicide in Regions below the Capital, Praia, which he related up with the crimes of Damage and Corporal Offense, characteristic of the peculiar phenomenon of juvenile delinquency in the country, Thugs.","Relation, County, Municipality, Cape Verde, Crime, Correspondence Analysis"
947,"The main objective was to analyze the relationship between the Infant Mortality Rate (TMI) and the Total Fertility Rate (TFT) for the microregions of the Brazilian semiarid region in 2010. For this purpose, the clusterwise regression technique was used for the 137 microregions with the purpose of grouping data elements based on the linear relationship between TMI and TFT. Regression models were validated for each cluster found, verifying the fulfillment of the assumptions through the following techniques: analysis of model residues, diagnosis of homoscedasticity by the Breusch-Pagan test and the Shapiro-Wilks normality test. Averages weighted by the population size of the TMI microregions estimated by the Ministry of Health&#39;s Active Search Project were calculated with those estimated by the Human Development Atlas (ATLAS). TFTs were extracted from ATLAS. The clusterwise regression model pointed to three clusters, whose assumptions were met. The formation of clusters allowed the identification of regional blocks within the semiarid region with distinctive characteristics such as higher TMI and TFT in one block and variations in these rates in the others. These characteristics can assist in monitoring, optimizing and setting interregional priorities regarding investments in the reproductive health of women and children, as well as in socioeconomic development, as the latter has been repeatedly associated with these rates in the literature.","Demographic indicators, clusterwise regression, semiarid."
1231,": In this work, the usual income of the main job per annualized hour worked was statistically modeled for the employed population aged 18 to 65 years in Brazil in the 3rd quarter of 2017. For this purpose, the survey package of the R e statistical software was used. PNADC data. The following predictive variables were used in the model: contributors or not with a social security institute in relation to the main job; sex; highest level of education achieved; age; and color or race. As a result, it was obtained, according to the model used, that for employed persons aged 18 to 65 years old in Brazil in the 3rd quarter of 2017: not being a contributor in relation to the main job implies a reduction on average of 37.68% in the usual income of the main job per hour worked; being a woman implies an average reduction of 19.86% in the usual income from the main job per hour worked; having a higher level of education attained higher education or equivalent implies an increase on average of 141.86% in the usual income of the main job per hour worked; being in the age group of 31 to 65 years implies an increase on average of 27.08% in the usual income of the main job per hour worked; having a black color implies an average reduction of 20.76% in the usual income of the main job per hour worked; being brown implies an average reduction of 22.13% in the usual income of the main job per hour worked; having an indigenous race implies an average reduction of 21.55% in the usual income from the main job per hour worked; among other important results.",Performance; Main job; Occupied population; Regression model; Taxpayer; Pension; Sex; Instruction level; Age; Color or race.
1320,"This paper proposes a robust estimation method of the sample autocovariance and autocorrelation functions in the presence of additive outliers. The robustness property is achieved by replacing the standard Fourier transform by its robustified version obtained by substituting the least square procedure in the harmonic regression by the non-linear M-regression. Simulation experiments are conducted to assess the performance of the estimators under contaminated and non-contaminated scenarios.
",Robust estimation; outliers; autocorrelation; spectral density
1316,"Decision Theory and Bayesian Inference have an important role to solve some common problems in research and practice in the medical field. These decisions may be from different natures and can consider several factors, such as the costs to carry out the study and each sample unit and, especially, the risks for the patients involved. Here, the estimation of sample size calculation considers the cost of sampling units and clinically relevant size of credible interval for difference between groups. By fixing a probability to the HPD region, the Bayes’ Risk is calculated for each sample size possible and it is chosen the optimal sample size, which minimizes the risk. In addition, a second solution is presented by setting the amplitude of the credible interval, leaving its probability free. It is considered a Normal distribution for data with unknown mean and fixed variance (Normal prior) and also the case where both mean and variance are unknown (Normal-Inverse Gamma prior). It is presented a solution considering the statistical distribution of sufficient statistics. In scenarios with no analytical solutions, the optimal sample sizes are presented using Monte Carlo methods.","decision theory, bayesian statistics, monte carlo, sample size."
1094,"The branching process, or Galton-Watson process, is a special type of stochastic process with a lot of application in areas such as biology, physics, computing, among others. In the present work, the basic concepts related to this theory will be studied and a complete study will be carried out on the survival and extinction results of one of its variants: the branching process in variable medium. This will be done through a bibliographic review of the existing results in the literature and the formulation of examples and counterexamples in order to compare them. Different applications will also be discussed.",nothing
1189,"Context Tree Models (MAC) were first introduced in Information Theory by Rissanen (1983) as a parsimonious generalization of Markovian models. Since then, they have been used extensively in applied probability and statistics and have recently been applied to data modeling in different scientific areas such as biology, linguistics and music. The idea behind MACs is that for each past, only a finite past suffix, called context, is sufficient to probabilistically define the next symbol. Stochastic processes, defined over a finite alphabet, that allow a context tree representation are called variable-length memory strings (CMTV). Given a sample of this chain, the estimation of the context tree and the transition probabilities that generate the sequence of symbols in the sample is an issue widely addressed in the literature. Historically, this estimation is performed on different versions of the Context Algorithm, also introduced by Rissanen, and which selects the smallest tree capable of generating the sample. In this sense, we present in this work a brief description of such algorithms and explore the issue of consistency of these estimators.",Context Trees; Context Algorithm; Model Selection; Consistent Estimation.
986,"A typical problem when dealing datasets with a large amount of covariates compared to
small sample sizes is to satisfactorily estimate the parameters associated with each covariate.
When the number of covariates greatly exceeds the sample size, the parameter estimation be-
comes very dicult. In various areas of application such as text categorization, it is necessary
the task of selecting important covariates and avoiding the overtting of the model.
In this work, we developed a Sparse Bayesian binary regression model with asymmetric
link function for text categorization. In addition, we assign a sparse prior distribution (double
exponential) for regression parameters to favor sparsity and to reduce the number of covariates
in the model. The performance of the proposed model is demonstrated with real data set, the
Reuters R8 corpus. The dataset contains the eight most frequent classes from the Reuters-
21578 collection of newswire articles. The eight classes consist of a minimum of 51 up to 3923
documents and sum up to a total of 7674 texts.
Parameter estimation is performed considering Hamiltonian Monte Carlo estimation method
on No-U-Turn Sampler (NUTS) extension, using the Stan software in the R package.
","Bayesian lasso, Skew link, Sparsity, Text categorization."
1165,"In this work, we propose a flexible Bayesian quantile regression model when the response variable is multivariate, where we are able to define a structured additive framework for all predictor variables. We build on previous ideas considering a directional approach to define the quantiles of a response variable with multiple-outputs (Guggisberg, 2017). We combine this approach with a proposal in the literature to define non-crossing quantiles in every directional quantile model (Rodrigues & Fan, 2017). We define a MCMC procedure for model
estimation, where the noncrossing property is obtained considering a Gaussian process design to model the correlation between several quantile regression models. We illustrate the results of these models using German data from the Socio Economic Panel, where the interest lies in explaining more dimensions of inequality in the population, such as income and health, using the dependence between
these two variables.",Multiple-output response variable; Noncrossing Bayesian quantile regression; Inequality dimensions.
980,"High education systems face dropouts and late graduations and then have to deal with economic and social issues. Our goal is to study students dropout from the Bachelor Applied Mathematics course at University of Sao Paulo, whose dropout rate is increasing and worrisome, and also the covariates associated to the risk of dropout and to the risk of graduation. In our proposal we assume that the (latent) failure time is a continuous variable following the proportional hazard model that can just be observed in discrete times (semesters). Since dropout and graduation are competitive events we assume a competing risk survival modeling. We use the log complementary transformation on the risk function to obtain a linear function of the parameters implying that we can fit a generalized linear model with binomial error structure to estimate the regression coefficients. The Bayesian approach is considered to deal with covariate selection and estimation of the parameters. We conclude that the risk of graduation is higher for those who was admitted by first call, that is the risk of dropout is higher for those who was admitted after the first call. Despite this conclusion, in general the covariates may not explain the events for this course in particular.","Bayesian model averaging, Competing risk, University dropout"
1078,"Studies that prove the superiority of a drug in relation to others already existing in the market are of great interest in clinical practice. Based on them the Brazilian National Agency of Sanitary Surveillance (ANVISA) grants superiority drugs registers which can cure faster or increase the probability of cure of patients, compared to standard treatment. It is of the utmost importance that hypothesis tests control the probability of type I error, that is, they control the probability that a non-superior treatment is approved for use; and also achieve the test power regulated with as few individuals as possible. Tests of hypotheses existing for this purpose or disregard the time until the event of interest occurrence (allergic reaction, positive effect, etc.) or are based on the proportional hazards model and ignore the possibility of cure. In this work we developed and investigated a hypothesis tests for clinical trials of superiority, based on the comparison of survival curves under the assumption that the data follow the proportional survival odds model considering cure fraction. Simulation studies are conducted to analyse the ability to control the probability of type I error and the value of the power of the tests when the data do not satisfy the assumption of the test and when they satisfy, for different sample sizes. We conclude that the probability of type I error is underestimated when the data do not satisfy the assumption of the test. And it is controlled when they satisfy, as expected.","Superiority test, type I error, cure fraction"
1239,"Machine learning models (Machine Learning-ML) have been gaining more and more notoriety in several areas of knowledge, because due to their flexibility and non-distribution assumptions, in addition to their great adaptability, they are able to make classifications and predictions for different types of data. . Support Vector Machines are part of ML. They are a technique that has been receiving increasing attention in recent years, and can be used in various pattern recognition tasks, obtaining results superior to those achieved by other learning techniques in various applications. In this work, a comparison is made to the adjustment of volatility models (GARCH) in which a comparison is made with the Support Vector Machine technique.",Machine Learning; Volatility; Return series; IBOVESPA.
1089,"The Aim this work wasto estimate the mortality from people aged ≥65 years old in three scenarios with different socio-economic background and urbanization process by using flexible parametric survival models.
The three cohorts coming from Brazil (n=365, 8 years of follow-up), Argentina (n=1800, 10 years of follow-up) and Italy (n=2472, 30 years of follow-up) were considered and only people with ≥65 years old included.  Time to death (months) from enrolment and all-causes mortality were considered. Statistical analysis included Frailty Cox’s Model and Flexible Parametric Survival Analysis. Mean (SE) follow-up was 70.8 (2.6) for men and 74.2 (2.0) for women in Brazil, 63.9 (2.1) for men and 67.8 (1.8) for women in Argentina and 298.7 (10.5) for men and 304.9 (11.0) for women in Italy. Frailty Cox model showed significant positive effects of age and an effect modification of High Blood Pressure and Non-communicable diseases but not effects proportionality. Modeling evidenced a positive statistically significant effect of Age, Non-communicable Diseases and Smoking. There was also an effect modification of Non-Communicable Diseases on High Blood Pressure. Overall Flexible Models estimates were more conservative with an increase in their precision.  Conclusion: High Blood Pressure and Non-Communicable Diseases are important causal components and strong risk factors of cardiovascular mortality in these countries. The model obtained indicates a common causal model in three cohorts coming from very different environments.
","Cardiovascular Diseases, Diabetes, Elderly, Hypertension, Survival analysis.
"
1243,"For time data modeling, ARMA models are widely used. However, in time series that contain atypical observations, these models may not be adequate. An alternative to properly model this type of series is to consider more flexible distributions, such as the Student t distribution. Maior &amp; Cysneiros (2018) developed the SYMARMA model that assumes that the variable of interest follows a distribution belonging to a symmetrical class that was less sensitive to atypical observations. In this work, we apply the SYMARMA model to finance data from the Brazilian market, more specifically to the CAPM model (Capital Asset Pricing Model) in which a functional relationship is established between the expected return of a security, the risk-free return and a premium per risk. CAPM&#39;s models have been applied to a variety of situations in finance in the estimation of systematic risk.",nothing
1297,"In this work, we derive a likelihood ratio statistic corrected by a Bartlett correction factor to test parameters of a multivariate normal model with general parameterization, which is a particular case of the general elliptical model. This corrected statistic has a $\chi^2$ distribution up to an error of order $o(n^{-1})$ under the null hypothesis, thus having better performance in small samples compared to the original likelihood ratio statistic.","Bartlett&#39;s Correction, Likelihood Ratio Statistics, Normal Model"
1180,"Item Response Theory (IRT) consists of a set of mathematical models, which aim to estimate or measure latent traits, characteristics or abilities of the individual that cannot be measured directly. The most common One-dimensional models of IRT have as their main assumption that there is a single latent trait, or a dominant latent trait, responsible, for example, for the ability in an area, the degree of severity of a disease or the degree of satisfaction to a product, when applicable. However, they are often faced with situations in which they want to measure more than one latent skill or trait. For example, in a test where the objective is to measure the fluency of an individual in a certain language, several factors can influence the correct answer or not to an item, such as their vocabulary, text interpretation or even reading skills in the language in question. Considering the importance of the theme, this work aims to study multidimensional models (TRIM), developing an application with real data. The analysis is performed using the Bayesian approach and stochastic simulation methods are used to estimate the parameters. The application involves the adjustment of a two-dimensional model, comparing the results to those obtained through a one-dimensional model, for data from the National High School Exam (ENEM) for the year 1999. These data were obtained through the microdata from ENEM, where the answers for each individual participating in the test are found, this one still with 63 items. We opted for the yellow question book and, based on this choice, two thousand individuals were randomly selected to adjust the model. The results showed that the two-dimensional model fits the data better when compared to the one-dimensional model.","Multidimensional Item Response Theory (TRIM), Bayesian Inference, Markov Chain Monte Carlo Methods (MCMC)."
1123,"We propose a new distribution called the exponentiated power exponential (EPE) distribution and introduce a regression model with different regression structures based on the new distribution. We also show that the new regression model can be applied to dispersion data since it represents a parametric family of models that includes as sub-models several widely-known regression models and therefore can be used more effectively in the analysis of real data. We employ a frequentist analysis and derive the appropriate matrices for assessing local influence on the parameter estimates under different perturbation schemes. Some global-influence measurements are also investigated and simulation studies are performed to evaluate the precision of the estimates. We provide an application of the regression model with different regression structures to nursing activities score data in the Unit of the Medical Clinic of University of São Paulo (USP) Hospital. 


",Exponentiated family; diagnostic analysis; maximum likelihood; regression model; residual analysis
1232,"In this work, we introduce an asymmetric extension to the Tobit model by assuming that the error term follows a tilted-normal distribution. The new model, namely tilted-normal Tobit model, can be an useful alternative to other skewed Tobit models, such as the skew-normal and power-normal Tobit models. The method of maximum likelihood is used for estimating the model parameters. We provide some simulation studies for different sample sizes and parameter settings. In addition, we perform residual and influence diagnostic analysis. Finally, we use U.S. food consumption data to illustrate the better performance of the model introduced.","Censored regression model, Influence, Maximum likelihood estimation, Residual analysis, Tilted-normal distribution"
1143,"Longitudinal Item Response Theory (IRT) data occurs when experimental units are submitted to measurement instruments (e.g., cognitive test, psychiatric questionaires, biological essays among others) along different assessment conditions, as different time points. Very often, in this kind of study, we are interested in the so-called latent variables (or latent traits) and their behavior along these conditions, including the modeling of their inter-dependency structure. In this work we use some stationay and nonstationary time series and multilevel models to represent longitudinal IRT data. More specifically, we consider first order autoregressive (AR(1)), first order moving average (MA(1)), first order autoregressive moving average (ARMA(1,1)), antedependence (AD) time series models as well as the Uniform and Hankel dependency structures, induced by appropriate multilevel models. These structures are studied under a time-homocedastic and time-heteroscedastic fashions. We developed a Bayesian inference framework, which includes parameter estimation, model fit assessment and model comparison, through MCMC algorithms. Simulation studies are conducted in order to measure the parameter recovery and model comparison tools. A real data analysis, concerning a longitudinal cognitive study of Mathematics achievement, conducted by the Federal Brazilian government, is performed. All computational implementations are made through the WinBUGS program, using the R2WinBUGS package, from R program.
","longitudinal IRT data, Bayesian inference, time-series modeling, multilevel modeling, MCMC algorithms"
1070,"Studies with binary responses with a hierarchical grouping structure arise in several areas of knowledge. When the main focus is on the structure of the average, the GEE method is a popular choice because of its computational simplicity, absence of distributional assumptions, and marginal interpretation of its parameters. When the association structure is of interest, the ORTH method is an extension of the GHG estimator that allows estimating the association structure as a function of covariates. As an alternative to marginal modeling, mixed generalized linear models (GLMM) model the variability between individuals and the dependence between responses through random effects. In this work, we present an analysis of a randomized cluster study that aims to compare two teaching methods in school education. We contrast the marginal (GEE and ORTH) and conditional (GLMM) models. The application of the three methodologies allowed us to draw complementary conclusions regarding the data association structure and the effects of the considered covariates.",Hierarchical studies; binary response; GHG; ORTH; GLMM
1096,"Breast cancer is the second most common type among women in Brazil and if detected in early stages, the chances of treatment and cure increase significantly. With this knowledge and given the importance of this pathology in the context of public health worldwide, this study aims to verify the influence of covariates on the survival time of patients affected with the aforementioned disease. Data were collected from 62 medical records of women diagnosed with this type of neoplasm in the period 2006 to 2011 attended by the MUCAMA project at the Federal University of Alfenas (UNIFAL-MG). To describe the life span of patients with breast cancer as well as the effect of covariates, the exponential, Weibull and log-normal models were fitted. The Weibull model was the one that best fit the data, verifying that patients with metastasis had a higher risk than patients without metastasis. And, thus, it can be concluded that the earlier breast cancer is discovered. The woman&#39;s survival condition will be better and that the presence of metastasis is a primordial factor in the survival issue. Thus, conducting campaigns and early diagnosis favor the survival of these women.","breast cancer, parametric models, survival"
1213,"Giovani Flório, Guilherme Gandin, Bruno Vernaglia Zólio, Giovana Passos Nesterick, Thaís Maíra Gonçalves de Lima, Pedro Ferreira Filho PET Statistics – Department of Statistics - UFSCar Located in the central region of the state of São Paulo, the city of São Carlos can be considered a medium-sized city with a strong educational insertion, due to the presence of two large public universities included among the best in the country (UFSCar and USP). It has a population of approximately 237,000 inhabitants, with a ratio of 208.42 inhabitants/km2. Like any city of this size, it faces problems, under different aspects, related to public safety, among them the theft and robbery of vehicles. According to IBGE data, in 2016 the city had approximately 168,000 vehicles, a high value compared to the city&#39;s population. The annual growth rate of this fleet is estimated at 6% in recent years. It is important to remember that theft is characterized by the taking of material goods, vehicles in this case, without violence or threat against the victim. Theft is characterized by the fact that the victim is not present or does not notice the action and is considered a common crime. On the other hand, robbery consists of an act of stealing a material property from someone else by means of violence or threat, that is, in this case the victim is present, and suffers threat or is the target of violence. From the point of view of Public Security, in addition to identifying how vehicle thefts and robberies behave, it is important to verify where they occur so that preventive measures are adopted. In this work, based on data from the Public Security Department, the preliminary results of the study on the occurrence of theft and robbery of vehicles in the city of São Carlos between the years 2015 and 2016 are presented, considering a small set of variables observed.","Vehicle Theft, Vehicle Theft, Statistics"
1298,"Problems involving discrete data modeling have been the focus of several researches nowadays. In the literature we find the proposition of new discrete models such as the Lindley Discrete distribution, which can be used as a possible alternative to the discrete Gamma and Weibull, Poisson and Geometric models. In this work, we initially study the construction of this new probability distribution as well as some of its properties. Subsequently, for inferential purposes, we will carry out a classical approach using Maximum Likelihood and Moment estimation methods. We will show the applicability of the studied model to simulated and real data sets. All computational implementations were performed using the R system.","Discrete Models, Discrete Lindley Distribution, Classical Inference, Simulation."
1222,"The identification of mistakes and proficiency levels are of paramount importance in the educational context. Mistakes are systematic errors about some content, made by individuals during an assessment, which lead to an incorrect answer to the item. When information on the occurrence of misunderstandings is provided to the individuals involved in the assessment, immediately or in the short term it can support and guide learning more effectively. In this sense, we propose in this work a new cognitive diagnosis model to identify mistakes, called reduced Bug-Fusion. The model, in addition to being unprecedented, estimates the probability of the presence of each mistake, penalizing the probability of a correct answer in a given item. The initial simulation study performed showed promising results, with good estimates for the misunderstanding pattern and item parameters involved in the model.","Cognitive diagnostic models, misconceptions, reduced Bug-Fusion."
928,"Double generalized linear models (GLM), unlike generalized linear models (GLM), allow the adjustment of the response variable dispersion parameter as a function of predictor variables, improving the way to model phenomena. This happens because MLGD allows the adjustment of the response variable dispersion parameter as a function of predictor variables in cases where the response variable has a distribution that belongs to the exponential family. Considering our interest in variable selection in this class of models, we studied the two-step variable selection scheme proposed by Bayer and Cribari-Neto (2015) and, based on this method, we developed a procedure for selecting variables in up to &quot;k &quot; steps. To verify the performance of our procedure, we performed simulation studies in MLGD with gamma distribution for the response variable. The results obtained indicate that our procedure for selecting variables presents, in general, better performance than the other studied methodologies.","Information criteria, double generalized linear models, double gamma regression, variable selection, stepwise."
1247,"In this work, copula theory is used to model annual maximum flow data for the purpose of estimating river flow return time for Brazilian dam data. Archimedean copulas were used to model the joint bivariate distribution of maximum annual discharges from the Taquaruçu and Capivara dams located on the Paranapanema River. The copulas were estimated using the Gev distribution as marginals, the copula with the best fit was the Gumbel.",nothing
1176,"In an official bulletin, the National Water Agency (ANA) states that since $2012$ there has been a gradual and intense reduction in rainfall in various regions of the country, which has led to a great loss in the supply of water to the public supply. In this work, we adjust a tv-AR(2) model, proposed by Dahlhaus, 1999 to verify if the stochastic process that generates the temporal series, pluviometry, is stationary or varies in time. Data from the daily rainfall series in the Cantareira reservoir region, available data are from January $2003$ to December $2017$, and also from seven-day accumulated data. We found no difference in the rainfall pattern when analyzing the daily series, but we did notice differences when analyzing the weekly accumulated series.","Autoregressive Model, Locally Stationary Process, Time Series"
1124,"Dropouts in undergraduate Statistics courses offered at different higher education institutions, according to data from the Higher Education Census of the National Institute and Educational Research Anisio Teixeira (INEP), reaches values well above the desired level. In the case of the Statistics Course at UFSCar, the dropout percentage has remained around 50% in recent years. This percentage is still considered high, but lower than that observed in most other Statistics courses in the country. Fernandes, et all (2016) found that dropouts from UFSCar basically occur in the first three semesters of the course and that about 70% of the dropout students had not passed the basic subjects of the first semester of the course (Differential and Integral Calculus 1 and Geometry Analytical). Low academic performance, however, may not be the only element that leads to student dropout. In this sense, this work seeks to present a characterization of the students who dropped out at UFSCar in the period 2006 to 2017. For this, a questionnaire was used, sent and answered by a portion of students who dropped out in this period. Although the return of these questionnaires was not as expected, it was possible through descriptive and exploratory analyses, Moretin and Bussab (2005) and a factorial analysis of multiple correspondences, Lebart, Morineau and Piron (1995) to establish a characterization of the dropout students .","Evasion, Teaching, Statistics"
984,"The simulated envelope is an informal diagnostic analysis method used to verify the plausibility of the assumed hypothesis for the response variable in a regression model. In this work, we describe some procedures to obtain it and, later, we present and propose methods for rejecting the model from the simulated envelope. In order to compare these procedures, we performed a simulation study and obtained good results for the proposed method, since they are very close to the results of methodologies already existing in the literature, however, with a much lower computational cost.","diagnostic analysis, simulated envelope, generalized linear models, Monte Carlo simulation."
983,"In this work, a new distribution family is proposed for modeling data with limited response in the continuous range (0.1) and some structural properties of the new family are presented. The proposal is based on Johnson&#39;s Sb distribution, where the quantile of the generalized extreme value distribution is considered instead of the quantile of the logistic distribution. Furthermore, the new family is extended to regression models as an alternative to the beta regression model. Inference procedures based on the likelihood theory are also presented. An application to real data to illustrate the use of the new family is considered.","Beta Distribution, Johnson SB Distribution, Generalized Linear Models"
1196,"We estimate the average rainfall in the region of Presidente Prudente-SP, in different periods and levels of probability. For this, daily rainfall data collected from Jan/1947 to Dec/2003 at the meteorological station of the UNESP campus in Presidente Prudente were used. In this case, the average monthly rainfall data were fitted to the Gamma distribution in monthly periods. This study explores and compares the performance of Maximum Likelihood and Bayesian estimators to estimate probable monthly rainfall for different probability levels. Bayesian analysis is performed using Markov Chain Monte Carlo (MCMC) methods to sample the posterior distributions.","Gamma distribution, rainfall, maximum likelihood, MCMC, Bayesian analysis."
1095,"The Demographic Census is one of the most relevant sources of sociodemographic information in the country. However, like any research process, its realization is subject to the occurrence of non-sampling errors that should be studied and evaluated whenever possible. The objective of this work is to use the 2010 Census data, that is, the information about the collection operation and the research administration, to study the causes of the divergence between the interviewer and the supervisor in the classification of the household species, in the States of Alagoas, Amazonas, Santa Catarina, Rio de Janeiro and Mato Grosso. The statistical methodology adopted was hierarchical logistic regression modeling with three levels, household, sector and supervisor. The results showed that the random effects of sector and supervisor were statistically significant for all states. Furthermore, it was found that the factors that influence the occurrence of divergences vary according to the State. While in Alagoas and Mato Grosso, only second-level (sector) covariates were significant, in the other States at least one covariate from each hierarchical level was relevant. These results can be used in planning the Census quality improvement process, as well as other household surveys.","Demographic Census, non-sampling error, data sets, hierarchical models."
1178,"We consider the issue of statistical testing in classification problems for high dimensional low sample size data, for which a U-statistics based approach has shown promise. Here, we address One Class Classification (OCC) problems, which consist of verifying whether an object represented by a vector of feature values can be assigned to a target class. To this objective, we consider a U-statistic defined on within and between group distances, and present an extension to consider groups of size one. We derive the asymptotic properties for this extension, and employ it to define a model free classification test.  We also present a simulation study to asses the test's statistical properties and  perform an application to real data.","U-Statistics, Asymptotic Theory, Outliers"
1253,"The presence of uncontrollable random factors and, consequently, fluctuations in commodity prices, drive the search for risk protection mechanisms. In this context, hedge operations are considered an important strategic instrument for price management. In view of the volatility of agricultural commodities and exchange prices, this paper aims to analyze the relationship between volatility and the optimal hedge ratio and its effectiveness for the Brazilian market in the period from 2011 to 2017. This proposition is driven by based on the following issue: Would volatility be the cause of the effectiveness and optimal ratio of the hedge? Specifically, it seeks to analyze the volatility of commodity prices through ARCH class models, and find out how much of the assets should be traded in futures contracts to protect against price variation and what percentage of risk reduction compared to the position unprotected",Volatility; Hedge operations; ARCH models.
1323,"Generally, the  women for whom the duration of labor cannot be observed due to fetal death that generating a proportion of times equal to zero, are often excluded of data analysis of the time until the labor. The objective of this paper is to consider the Generalized Gamma  Zero-Inflated Cure-Rate  model in the context of labor time, including times equal to zero, and to evaluate likelihood-based estimation procedures for the parameters by a simulation study and then apply to a real dataset. In general, the inference procedures showed a better performance for larger samples and low proportions of zero inflation and cure rate.%, also we verify the diagnostic of fitness of the model. To exemplify how this model can be an important tool for investigating the course of the childbirth process, we considered the World Health Organization (WHO) dataset. We thanks WHO for granting us permission to use the dataset.
","childbirth, duration of labor, cure-rate models, survival analysis, zero-inflation."
1173,"The resilience of groups in the university environment and the question about staying at the university led us to interest in developing this work. Due to the large number of young people who drop out of university, we wanted to answer the question of the main factors that make a young person with economic and social difficulties remain in the bachelor&#39;s course in Statistics? Given this fact, a questionnaire based on scales obtained in the literature was applied, in an attempt to understand this difficult reality.","resilience, statistics"
1185,"The work has a focus aimed at evaluating the consumption relations between students and Bachelor of Statistics courses offered by higher education institutions. The main objective is to investigate the possible factors that influence the propensity to retain the undergraduate in statistics. In particular, we will investigate the factors Perceived Quality, Perceived Value, Satisfaction, Integration, Commitment and Loyalty, aiming to understand which relationships minimize the propensity to university dropout, a frequent and serious cause in undergraduate studies in Statistics. For this, a questionnaire was built with the objective of measuring these factors and then the hypotheses of interest were tested through linear regressions. The results obtained for the undergraduate course in Statistics indicate that the factors that most influence the retention of bachelor&#39;s degrees in statistics were Emotional Commitment and Loyalty to the course. Thus, the importance of greater integration between the course&#39;s students is highlighted, that is, when they are better socially integrated, students tend to have a greater emotional commitment, later this improves loyalty and consequently the student to be retained in the course.","Factor Analysis, Statistical Education, University Dropout, Linear Regression."
966,"In order to adjust the length of stay in the public service, based on data provided by the Social Security Institute of Municipal Servants of Cabedelo/PB (IPSEMC) − 2630 civil servants and former civil servants, four covariates (career, age of entry into the public service , exit rules and gender) − and through traditional survival models for censored data (non-parametric, parametric and semiparametric), it was possible to estimate risk and survival functions, in addition to the effect of the four covariates on time of permanence. Among the main results, there is that the length of stay of the public servant differs between the levels of age for entering the public service, rules for leaving the public service and gender; among the models that estimate the effect that covariates have on residence time, the probabilistic Gompertz and Cox semiparametric models were considered adequate.",Social Security Institute of Municipal Servants of Cabedelo/PB. Standard survival analysis for censored data. Length of stay in public service.
880,"Archetypes can be defined as extreme elements that well represent a sample, or population. Through the multivariate technique called Archetypal Analysis (AA) it is possible to find and select archetypes, which are convex combinations of the data. The AA can be applied in several areas and with different uses of the archetypes, including its use in the increase of the sample size. When data sets are characterized as incomplete or don’t have the size required to make a minimal error in the inference procedure, one has the option of increasing that sample. Thus, the data augmentation technique consists of introducing unobserved data or latent variables. The multivariate sample augmentation should consider the probability distributions and the correlation between the variables. The aim of this work was to evaluate the increase of multivariate data through its archetypes. So starting from a bivariate sample of random variables with normal distribution, the sample augmentation was performed from three algorithms to increase data via archetypes (A1, A2 and A3), a gold standard and a control. Then, a simulation study was carried out with six scenarios, evaluating the correlation between the variables (0.2, 0.5 and 0.9) and the proportion of the increase (10 % and 30 % of n). The three algorithms presented similar results: they have allowed to augment 10 % of the sample size without changing the probability distribution, nor the estimates of its parameters. Therefore, it is possible to augment a sample of a normal bivariate via archetypes.","Archetypal Analysis, Augmented data, Multivariate statistics, Simulation."
1075,"In many situations, those working with data analysis are faced with the need to compare factors using data from observational studies instead of those from experiments under controlled conditions. This often occurs in environmental and socioeconomic investigations. It is often part of the research objective to compare exactly how values behave in their natural environment, without interference from the researcher, which limits the possibility of controlling the numerous secondary influential variables. The characteristics of the data collected in this way make it difficult to use the most common statistical inference techniques, which require the fulfillment of several assumptions, impossible to be obeyed in such situations. For example, a study whose objective was to survey the reality of the prices of produce, produced organically or conventionally, sold in fairs or supermarkets, resulted in data for which the application of conventional tests to compare group means was not adequate. This work presents the characteristics of this type of observational data and suggests the use of the bootstrap technique as an alternative for the application of inferential tests.","bootstrap, test of means, surveys, statistical inference"
1258,The aim of this article is to compare the performance of multivariate volatility models with dynamic conditional correlation using normal and t-student distribution for estimating the optimal hegde R$/US$ exchange rate inside and outside the sample for the period of 01/03/ /2001 until 02/06/2018. The results indicate that within the sample the model with the best performance is DCC-IGARCH with normal distribution and outside the sample is the DCC-GJR model with t-student distribution.,"Futures Markets, Optimal Hedge, DCC, Forecast"
1337,"Introduction: Studies show that the health conditions of individuals are influenced by the difference in access to social services such as education, housing, food and income distribution. The analysis of the association between segregation and/or inequality of income and mortality has been growing in recent years. Knowledge of this relationship is essential to understand the population&#39;s health conditions. Objective: This article analyzed the association between standardized mortality ratios due to external causes (RMP_EXT) and income segregation, measured using the Neighborhood Sorting Index (NSI) and the Gini index, in metropolitan regions and municipalities with more than 100 thousand inhabitants of Brazil in the years 2000 and 2010. Methods: The socioeconomic factors of this study were estimated based on data from the 2000 and 2010 Demographic Censuses, using sample microdata, available only in the restricted data access room of the Instituto Brasileiro de Geography and Statistics (IBGE) for the need to identify the census sector. The RMP_EXT were estimated for 2001 and 2011, with information contained in SIM/DATASUS. The effects of the NSI and the Gini index were estimated using Poisson models and generalized additive Poisson models. Results: NSI was a risk factor for the RMP_EXT, whose models only had the NSI. By incorporating Gini and per capita income, the effect was attenuated or not significant. This NSI risk effect was more present in 2010 than in 2000. Regarding the effect of the Gini index, it is noted that it is a risk factor for death from all external causes, in both periods. However, the effect of Gini on mortality decreased between 2000 and 2010. Conclusion: Both the NSI and the Gini index have effects on mortality, but the relationship between Gini and mortality is more consistent than that of the NSI. Furthermore, the effect of the NSI intensified and that of the Gini weakened in the period between 2000 and 2010.",Income segregation. Mortality from external causes. Gini index. Poisson model
804,"Abstract: Since the most distant times, governments of the oldest civilizations already used statistical methods as a way of counting the population, training troops to act in wars, administering the country, managing the production and harvest of different agricultural products, precious metals such as gold and silver, and financial controlling the products that were used as exchange currency such as gold and salt. At the same time, in this period, there were already people with disabilities who received from society the most different types of treatment ranging from extreme piety, abandoning death as a common norm established by the civilization or society of the place where this birth took place. Gradually, societies realized that, in addition to charity and assistance, such people should be included in public programs and policies that could enhance their productive potential. In fact, people with disabilities themselves showed that they could and wanted to study, work and be fully included in society. Race is a social construct used to distinguish people in terms of one or more physical marks that are socially significant. In this work, we used data from the 2010 IBGE Demographic Census and data on visual, hearing, physical and multiple impairments, in addition to the variable on race classification, crossings, homogeneity test and profile graphs were made between each of the disability variables. , race and education level; we implemented a ranking between the different levels of disability, number of disabilities and race; and finally; year-by-year profile chart to assess the evolution of disabilities globally as a function of race and level of education.",nothing
956,"When analyzing data in the form of counts, the assumption of equidispersion is commonly not adequate and, consequently, Poisson regression models are inappropriate. Important advances in the area of counts analysis have been reported in the literature, mainly to model different levels of dispersion, namely, under (mean &gt; variance) and overdispersion (mean &lt; variance). In this article, the COM-Poisson, Gamma-Count, Generalized Poisson and Poisson-Tweedie models are reviewed. The genesis of each model is presented, along with a comparative summary of the distributions. The similarity of the COM-Poisson and Gamma-Count models is highlighted, as well as the flexibility of the Generalized Poisson and Poisson-Tweedie models to model overdispersion. The application of the models is illustrated by analyzing the number of progenies of Sitophilus zeamais, observed in a completely randomized experiment with four maize substrates as treatments and ten replications. The computational implementation is performed in the R software, whose codes are made available in supplementary material.",nothing
985,"Multichannel processes are production processes formed by multiple channels and are present in many industrial production and service provision segments. For statistical control of these processes, Boyd (1950) suggested a Group Control Chart (GCG) to control all channels. Subsequently, the work of Mortell and Runger (1995) brought innovation by proposing the decomposition of the source of variation of a production process into two independent sources: an individual source of variability for each channel and another source, common to all channels. The main objective of this research is to propose Shewhart and EWMA versions of a new GCG to control changes in the dispersion of the individual source of variation of a process channel. Both proposals presented are based on the Mobile Amplitude (MR, Moving Range) of the differences between the values of the quality variable in the channels and the average of the channels at each instant of time. To measure the performance of the proposed graphics, their ARLs (Average Run Lengths), specifically ARL0 (ARL with the process in control) and ARL1 (ARL with the process out of control) were obtained by simulation. The performance metrics of both proposed GCG are compared to the metrics obtained for the Shewhart and EWMA versions of Rt by Mortell and Runger (1995) and to those obtained for the Shewhart and MEWMA versions of S2 by Runger et al. (1996), when exposed to the same changes in the dispersion of a process channel. It is worth emphasizing the originality of the research in proposing a statistical tool to control dispersion in cases of withdrawal of individual observations from process channels. The results showed good sensitivity of the proposed GCG, especially the MR GCG EWMA, which had the lowest ARL1 values among all the alternatives (analyzed control charts) in the case of changes of magnitudes less than or equal to 2.0 in dispersion, in processes composed of 10 or more channels.","Multichannel Processes, Statistical Dispersion Control, Individual Sources of Variation, Group Control Chart, Mobile Amplitude, EWMA"
1032,"This paper aims to present the comparison of the main methods of imputation of missing data in the contextual questionnaires of the SAEB, 2015. For that, we proceeded with the imputation of missing data from different types of treatments. Five methods of replacing missing variables were considered: “Pairwise”, “Listwise”, “Mean Imputation”, Multiple Imputation and “Imputation by proportion”. The second step was to use multiple linear regression analyzes to compare the adjustments of the models, considering the databases with different methods. The results indicated the method that best fit the data in the three subjects evaluated, was multiple imputation, which presented the highest adjusted R² of 20.57% in Portuguese. Next are the Mean Substitution and Listwise methods that fit the data from both subjects and mathematics well with adjusted R² of 15.71% and 14.41% respectively.","SAEB, Data Processing, Missing Data, Methods of imputation, Multiple Regression."
1150,"Football is the sport with the greatest attraction to the Brazilian public. No other sport in Brazil takes as many spectators to sports squares as the Breton game. In addition to in person, football covers millions of Brazilians every week, not only in game broadcasts, but also in sports news, which, due to the interest of the public, is one of the largest segments of the national press. In a sport where emotion is the predominant factor, everyday debates are held to decide who is favorite or not to win a game, or a championship. In this work, we sought, through the teams&#39; retrospectives in the last two editions of the Pará Championship, added to the performance in the current championship, to estimate the probabilities of the matches to be played in the 2018 Pará Soccer Championship. The reliability of this work was made using DeFinetti&#39;s Theory, in addition to a measurement based on the Brier Score, which is configured as a measure of accuracy to qualify probabilistic predictions that pointed to greater probability of events that later occurred.","Football, Probability, Results Forecast"
973,"This study aims to propose a multivariate method for determining the shelf life of foods in non-accelerated studies. The method allows incorporating microbiological, physicochemical and sensory variables. The idea of the method is to keep the two main components most correlated with time (not necessarily the first ones) and regress the score of a borderline sample for these components, predicting their lifetime. In this study, the method was applied to freshly processed eggplant data, resulting in a prediction of 9.6 days of shelf life.",Main components; Linear regression; Prediction; shelf life
925,"This work presents a proposal for the simplification of state transition diagrams in Markovian reliability analyzes for large systems and subsequent calculation of their average unavailability, considering that at least one of the components is aging. The stages method and the supplementary variables method are presented to work around the loss of the Markovian property. The method chosen was that of supplementary variables, as it has the great advantage of a standard solution, requiring only updating the system equations and initial condition. Failure rates will be modeled by the two-parameter Weibull distribution. One of the criteria adopted for the cut study is the derivative of the integral quantity in relation to the non-zero elements of the state transition matrix. Therefore, these derivatives are obtained through the generalized perturbation theory (GPT) which makes extensive use of the principle of conservation of importance. The proposed cut using the GPT formalism can be considered a good approximation of the original diagram for calculating the average unavailability of the system, as the result of the metric was equivalent, considering the negligible error.",Markov Chains; Generalized Perturbation Theory (GPT); Transition Diagram Reduction; Supplemental Variables.
813,"A new three-paramtric Nadarajah-Haghighi distribution is proposed using the family developed in Dias (2017). We obtain same structal properties of the new model including  explicit expressions for the moments, mode, median and quantile function. The method of maximum likelihood is used to estimate the model parameters and a simulation study is realized to verify the asymptotic properties of the parameters. The usefulness of the distribution is illustrated by means of two real data sets.
",Exponential distribution; Nadarajah-Haghighi distribution; Maximum likelihood estimation.
811,"A new three-parametric Nadarajah-Haghighi distribution is proposed using the family developed
in Dias (2017). We obtain some structural properties of the new model including explicit expressions
for the moments and generating function. The method of maximum likelihood is used to estimate
the model parameters. A simulation study is realized to verify the asymptotic properties of the parameters
and the usefulness of the proposed model is illustrated by means of two real data sets.","Maximum likelihood estimation; Nadarajah-Haghighi distribution; Rayleigh distribution;
Simulation study."
965,"As sequencing costs drops with the constant improvements in the field, next-generation
sequencing becomes one of the most used technologies in biological research. Sequencing
technology allows the detailed characterization of events at the molecular level, including
gene expression, genomic sequence and structural variants. Such experiments result in
billions of sequenced nucleotides and each one of them is associated to a quality score.
Several software tools allow the quality assessment of whole experiments. However, users
need to switch between software environments to perform all steps of data analysis, adding
an extra layer of complexity to the data analysis workflow.

We developed Rqc, a Bioconductor package designed to assist the analyst during as-
sessment of high-throughput sequencing data quality. The package uses parallel computing
strategies to optimize large datasets processing, regardless the sequencing platform. We
created new data quality visualization strategies by using established analytical proce-
dures. That improves the ability of identifying patterns that may affect downstream pro-
cedures, including undesired sources technical variability. The software provides a frame-
work for writing customized reports that integrates seamlessly to the R/Bioconductor
environment, including publication-ready images. The package also offers an interactive
tool to generate quality reports dynamically.

Rqc is implemented in R and it is freely available through the Bioconductor project
(http://bioconductor.org/packages/Rqc) for Windows, Linux and Mac OS X operat-
ing systems.","next-generation sequencing, quality assessment, high-performance computing, R"
1043,"Surplus production models (also known as biomass dynamic models) provide simple descriptions of
harvested populations, in terms of annual biomass levels (Bt), the intrinsic growth rate (r), the carrying capacity of the  environment (K) and the efficiency of fishing gear. These models have a long history in fisheries science and have provided a key basis leading to the popularity of Maximum Sustainable Yield (MSY) and its associated biomass (BMSY) as biological reference points for fishing management. The surplus production can be modeled using the state-space approach with linear observation equation and nonlinear state equation. Normality is a standard assumption for state-space models, however in many practical situation this is not a good choice since this is not robust to outliers and may not accommodate asymmetric data. More flexible classes of distributions have been proposed in the literature to deal with the issues of asymmetry and outliers. In particular, the SMSN class of distribution has shown its utility in many applications where the assumption of normality must be relaxed. In this work, we show how Bayesian inference for nonlinear state-space model using elements of this family can be implemented. The methodology is applied to data from population of marine shrimp of the Chilean coast. The presentation will be partly based on Montenegro and Branco (2016) with some extensions that are being developed.","state-space model, scale mixture of skew normal , fishing  management,  biomass dynamic models."
1037,"We propose an extension of the Generalized Autocontour (G-ACR) tests for dynamic specification of in-sample conditional densities and for evaluation of out-of-sample forecast densities. The new tests are based on probability integral transforms (PITs) computed from bootstrap conditional densities that incorporate parameter uncertainty. Then, the parametric specification of the conditional moments can be tested without relying on any parametric error distribution yet exploiting distributional properties of the variable of interest. We show that the finite sample distribution of the bootstrapped G-ACR (BG-ACR) tests are well approximated using standard asymptotic distributions. Furthermore, the proposed tests are easy to implement and are accompanied by graphical tools that provide information about the potential sources of misspecification. We apply the BG-ACR tests to the Heterogeneous Autoregressive (HAR) model and the Multiplicative Error Model (MEM) of the U.S. volatility index VIX. We find strong evidence against the parametric assumptions of
the conditional densities, i.e. normality in the HAR model and semi non-parametric Gamma (GSNP) in the MEM. In both cases, the true conditional density seems to be more skewed to the right and more peaked than either normal or GSNP densities, with location, variance and skewness changing over time. The preferred specification is the heteroscedastic HAR model with bootstrap conditional densities of the log-VIX.",Distribution Uncertainty; Model Evaluation; Parameter Uncertainty; PIT; HAR model; Multiplicative Error Model
1051,"Survivals models incorporating surviving fraction or cure rate are increasingly  becoming very popular in analyzing time-to-event data in survival analysis. This is due to the fact that certain fraction of the population suffering a particular type of disease can obtain cured due to the advances in the medical treatments and health care system. In this paper, we propose a new class survival models for lifetime data in presence of surviving fractions and examine some of its properties. Its genesis is based on the extensions of promotion time cure model, where we add a parameter to control the heterogeneity or unobserved dependence of lifetimes. Besides we extend the model to regression model for evaluating the effect of covariates in the cure fraction. Several former cure survival models can be seen as particular cases of our modelling framework. We discuss inference aspects for the proposed model in a classical approach, where we exploit maximum likelihood tools. Besides, an expectation-maximization algorithm is then developed for determining the maximum likelihood estimates of the model parameters. Finally, the modelling is fully illustrated on a data set on colorectal cancer. From the practical point of view, besides having a more flexible modelling for fitting survival data in presence of cure fraction, questions of medical interesting can be answered. Particularly, treatment comparison can be made straightforwardly. Moreover, we can estimate the proportion of patients disease-free after a determined treatment.",Colorectal cancer; cured fraction; cure rate models; Likelihood function; survival models.
806,"Cure rate models have been widely studied to analyze time-to-event data with a cured fraction of patients. We incorporate frailty into a cure rate model as an alternative approach to the description of such data based on the Birnbaum-Saunders distribution. This distribution has theoretical arguments to model medical data and has shown empirically to be an option to analyze this kind
of data. An important advantage of the proposed model is the possibility to jointly consider the heterogeneity among patients by their frailties and the presence of a cured fraction of them. In addition, the number of competing causes is modeled by the negative binomial distribution, which absorbs several particular cases. We consider likelihood-based methods to estimate the model parameters and to derive influence diagnostics for this model. We assess local influence on the parameter estimates under different perturbation schemes. Diagnostic tools are important in all statistical modeling, which is another novel aspect of the paper when deriving it in frailty-based cure rate models. Numerical evaluation of the proposed model is performed by Monte Carlo simulations and by an illustration with melanoma medical data, which shows its potential applications.",Censored data; Cure rate models; Frailty models; Likelihood methods; Local influence; Medical data; Monte Carlo simulation; Negative binomial distribution.
970,"Clustering method are useful tools for exploiting structures in datasets and have been widely used for unsupervised pattern recognition. Clustering means organizing a set of observations (objects, individuals, genes, pixels, etc.) into cluster so that observations belonging to a given cluster have a high degree of similarity, whereas observations belonging to different cluster have a high degree of dissimilarity. Euclidean distance is the most commonly used in clustering methods. However, methods based on this distance have good performance in data whose cluster are approximately hyperspherical and linearly separable. Because of this limitation, several methods capable of dealing with data whose structure is complex have been proposed, among which, kernel based clustering methods whose essence involves realization $\Phi$ arbitrary nonlinear mapping of the original space $p$- dimensional $X\subset\mathbb{R}^p$  for a dimension space higher (possibly infinite), called the feature space,  $\mathcal{F}$. The most commonly used kernel function is the Gaussian. Despite its good characteristics, this kernel is based on the Euclidean distance, that is, it assumes that the observations are more likely to be distributed in a hyperspherical region (that is, equal variances and zero covariance). However, examples in two different cluster are more likely to be distributed inside two hyperelipsoid regions diferente. The distance from Mahalanobis, which takes into account the correlations between variables and is invariant in scale, is a better choice for dealing with hyperelipsoid regions. We propose, under the approach kernelization of the metric, a algorithm kernel K-means based on an adaptive Mahalanobis kernel. The effectiveness of the proposed algorithm was demonstrated by experiments with simulated data.",nothing
1042,"In this research article, we propose a class of models for positive and zero responses by means of a zero-augmented mixed regression model. Under this class, we are particularly interested in studying positive responses whose dis- tribution accommodates skewness. At the same time, responses can be zero, and therefore, we justify the use of a zero-augmented mixture model. We model the mean of the positive response in a logarithmic scale and the mix- ture probability in a logit scale, both as a function of fixed and random effects. Moreover, the random effects link the two random components through their joint distribution and incorporate within-subject correlation because of the repeated measurements and between-subject heterogeneity. A Markov chain Monte Carlo algorithm is tai- lored to obtain Bayesian posterior distributions of the unknown quantities of interest, and Bayesian case-deletion influence diagnostics based on the q-divergence measure is performed. We apply the proposed method to a dataset from a 24 hour dietary recall study conducted in the city of São Paulo and present a simulation study to evaluate the performance of the proposed methods.",Bayesian inference; gamma distribution; log-normal distribution; mixed models; random effects; usual intake; zero-augmented distributions
1294,"In this paper, we have explored operational risk in Brazil by considering different sectoral indices of the Brazilian economy and the GACH Value-at-Risk (GARCH-VaR) estimation approach. We have carried a statistical evaluation of the eight Brazilian sectoral stock indices during different time ranges so that VaR methodologies could be chosen according to the data. We have analyzed the sectoral Brazilian indices during a common time range where we  have realized VaR backtests using recent data. The results of the study reveals that VaR may be an effective tool on minimizing risk exposure and potentially to avoid losses when trading in the Brazilian stock market. Furthermore, we have showed that different sectors of the Brazilian economy have significantly different risk behavior. In particular, the consumption and industrial sectoral indices presented the best risk performance. In this sense, we highlight that this type of analysis would be useful to small lenders/investors in evaluating the attractiveness of lending/investing on the Brazilian stock market.",Brazilian stock market; GARCH models; Time Series; Value-at-Risk; Volatility
1059,"This  work  focus on  multivariate  Birnbaum-Saunders (BS)   regression model   that  { can  be used} in survival analysis  to analyze correlated log-lifetimes of two or  more  units. This   multivariate BS  regression   models  is studied   through the use of a generalization   multivariate of the  sinh-normal (SN) distribution,  which  is built from
the    multivariate  mixture  scale of  normal (SMN) distribution.  The marginal and  conditional  linear    regression  models   of the resulting   multivariate  BS   linear  regression model  are   generalizations   of  the   BirnbaumâSaunders linear  regression models (Rieck and Nedelman,  1991),  which has  been used  effectively for modelling lifetime data and reliability problems.  We exploit the nice hierarchical representation of the   generalized  multivariate SN  to propose a fast and accurate EM algorithm for computing the maximum likelihood (ML) estimates of the model parameters.  Hypothesis testing is also performed by the use of the asymptotic normality of the ML estimators. 

Finally, the results of simulation studies as well as an application to a real data set are  displayed,  where  also  is  included  robustness feature of the estimation procedure developed here.","Birnbaum-Saunders distribution; Sinh-normal distribution;  EM algorithm;  Maximum likelihood method;
Robust estimation; Scale mixtures of normal distributions; Asymptotic normality; Hypothesis testing."
1207,"Although survival models are widely used in medical research, in obstetrics and gynecology these models are not used very often, particularly when it comes to labor and childbirth. Intrapartum care practices are usually based on the study of time, such as the identification of delays and the decision to accelerate labor. Therefore, it is important to establish criteria regarding the duration of labor, which means knowing the progression of labor so as to be able, if necessary, to intervene and favor a healthy outcome. To establish integrated guidelines, a study of the associated factors should be carried out with the knowledge of how the factors occur and change over time. Then, the variable time until childbirth is of great importance and can be described by survival models. An issue that should be considered in the modelling of these studies is the inclusion of women for whom the duration of labor cannot be observed due to fetal death, generating a proportion of times equal to zero. In addition, we consider that another proportion of women's time may be censored due to some intervention. In this context, the objective of this paper is to consider the Log-Normal Zero-Inflated Cure-Rate regression model in the context of labor time and to evaluate likelihood-based estimation procedures for the parameters by a simulation study and then apply to a real dataset. In general, the inference procedures showed a better performance for larger samples and low proportions of zero inflation and cure rate. To exemplify how this model can be an important tool for investigating the course of the childbirth process, we considered the Better Outcomes in Labour Difficulty project dataset and showed that parity and educational level are associated with the main outcomes. We acknowledge World Health Organization for granting us permission to use the dataset.","Childbirth, duration of labor, cure-rate models, survival analysis, zero-inflation"
1097,"This article proposes an extension of the Geostatistical model under Preferential Sampling in order to accommodate possible local repulsion effects. This local repulsion can be caused by the researcher who, when in possession of a measurement of a stochastic process in a given location, would not seek to collect new samples in nearby locations. Proceeding in this way, the sampling plan carried out would, in practice, include a repulsion window centered on each sampling observation, even if the researcher was planning the sample preferentially. This perturbation of the Geostatistical model under Preferential Sampling can be modeled from the definition of a non-homogeneous Bernoulli stochastic process over a partition of M sub-regions of the study area, thus avoiding the observation of more than one sample in each sub- region. Simulations are presented to evaluate the effect of this perturbation on the model and aspects related to the difficulties in estimating the model parameters, performed under the Bayesian approach, are then discussed. The results obtained support the idea that the proposed methodology corrects the distortions caused by this disturbance, mitigating the effects on inference and on spatial prediction.",Geostatistics; preferential sampling; punctual processes; repulsion; Bayesian inference
885,"Disasters are part of a complex relationship between society and nature. Such phenomenon involves both a physical event and a social construction. In Brazil, there is a formal representation and recognition of the disaster through the decree of emergency situation or state of public calamity. The objective of this work is defined by analyzing the occurrence of disasters caused by meteorological events, in the micro-regions of the states of the southern region of Brazil (Paraná, Santa Catarina and Rio Grande do Sul), in the period from 2010 to 2015. More specifically, it searches to investigate such occurrences from the perspective of the vulnerability of the population residing in these areas. For this, an indicator of social vulnerability was obtained, constructed by factor analysis. It was observed that this indicator was more correlated with the year with the most decrees (2014). Due to the importance of the year for the distribution of decrees, a longitudinal analysis of the data was chosen. It was identified that the microregions presented distinct trajectories within the period from 2010 to 2015 and, therefore, a multilevel model was used to consider this heterogeneity. However, the model with the best fit was the one that does not have a hierarchical structure: Negative Binomial generalized linear model. It was concluded that time is an important component in the relationship between decree and social vulnerability. A possible explanation is that when the year has more decrees (perhaps because of the occurrence of more meteorological events in that year), the vulnerability characteristics stand out, bringing the decree closer to the concept of disaster (disruption of the functioning of a society, mainly those that are most vulnerable).",vulnerability; factor analysis; longitudinal analysis; generalized linear model
1340,"Longitudinal studies are quite common in the field of public health and, consequently, adequate statistical methods are required to analyze the temporal evolution of one or more response variables, separately or simultaneously. Specifying the joint density function of all response variables, as well as their correlation structure, in addition to the numerical difficulties found in statistical inference when the size of the problem (ie the number of response variables) increases, are the main obstacles in joint modeling procedures. As an alternative, in this work we present two proposals to deal with multivariate longitudinal data: (i) a univariate approach, with mixed linear models adjusted to each of the response variables separately; and (ii) a joint modeling of these variables, based on copula functions. Both methodologies are applied to a set of real trivariate data regarding the nutritional development of Brazilian children.",Copulations; Longitudinal data; Anthropometric indices; Mixed linear models.
915,"Since the popularization of the use of the internet, e-commerce has shown significant growth rates, enabling the creation of new businesses, new opportunities, however this commerce model has not yet reached its full potential, some factors are pointed out as possible inhibitors such as, the risk of lost merchandise, financial risks, product performance risks, privacy risk, among others. This research sought to understand the impact of the perceived risk of privacy and transaction and social influence on the adoption of electronic commerce. For this, a survey was carried out with 213 students from a public university through a structured questionnaire applied to a non-probabilistic sample . Data were analyzed using Structural Equation Modeling, where, according to the analysis, the hypotheses that the perception of transaction risk influences the adoption of e-commerce and the correlation of social influence with the perceived risk were supported.","e-commerce; applied statistics, perceived risk, structural equation modeling"
1071,"Due to the current national crisis, especially in some states such as Rio de Janeiro and Porto Alegre, the Government chose to cut budgets, especially in the science and technology sector. Thus, the demand for sustainable indicators is essential to observe technological production in Brazil. This article prepares a study of public universities in the Southeast region and associates each one with the concepts evaluated by the Coordination for the Improvement of Higher Education Personnel (CAPES) in the last three years available.","Public Universities, Southeast Region, Master and Doctoral Courses, Correspondence Analysis."
1257,"Although the knowledge of Statistics is intensely linked both to the formation of a professional who is aware and able to read the world, and aware of the process of developing a research, it is clear that undergraduate and graduate students in the areas of Health, Biology, Humanities and even from Exact Sciences, start the discipline of Statistics unmotivated and considering that the knowledge to be acquired is not related to their area of training and often finish it without clarity on how they will use it in practice. This is due to the gap between the set of concepts and theoretical assumptions in the field of Statistics and those needed by those who need or intend to use them in their research or in their professional life. Among them, the emphasis on the use of real data from problems related to the student&#39;s area, the collection and analysis of data with computer programs, the reduction of mathematical formalism, the emphasis on data interpretation and visualization and the realization of projects . However, there is still a large gap between these reflections and research that lead to new methodological proposals and the more usual practice in the classrooms of Brazilian Higher Education. Thus, it is clear that at least two issues related to learning Statistics need to be better answered or agreed upon among Statistics professors to expand their reach in the faculty community: i) which concepts and skills are essential for a student and professional in a non-exact area; ii) how to evaluate this learning based on new paradigms. This article presents some results and reflections on these issues from the use of two qualitative methodologies applied to Statistics teachers, with a view to identifying ways that can advance the dialogue between teachers of the so-called service courses.",Statistical Education Assessment Delphi Method Focus Group
1364,"The purpose of this work is to measure the impact of Pibic on postgraduate training and formal employment of ex-scholarship holders. An impact assessment at the national level proved to be unfeasible, however, as a result of a partnership with the Universidade Estadual Paulista, it became feasible to carry out an assessment on a smaller scale. Five databases were used: (i) Pibic scholarship holders (CNPq), (ii) graduate students from Unesp; (iii) master&#39;s and doctoral graduates in Brazil (Capes/MEC); (iv) PhD graduates abroad (Lattes/CNPq) and (v) formal employment (Rais/MTb). Individuals&#39; CPF was the key to crossing all these banks. As Pibic is a non-random observational study, it was necessary to have a control group. For this purpose, the propensity score matching methodology was used. After selecting the control group, the effects were estimated using linear, logistic and ordinal regression models. Among the huge variety of combinations and analyses, the main result, related to postgraduate studies, is that an individual who took Pibic is 2.2 times more likely to complete a master&#39;s degree and 1.39 times more likely to do a doctorate than individuals who did not take Pibic. Related to employability, the chance of Pibic graduates entering the formal market is $0.80$ times that of the control group. The predictability of this result is due to the fact that, by increasing the chance of going to graduate school, entry into the formal labor market is postponed.","Impact Assessment, Scientific Initiation, Pibic/CNPq, Unesp, Propensity Score Matching, Regression Models."
1142,"Step-stress procedure is a quite popular accelerated test used for analyzing lifetime of highly reliable components. This paper presents simple and multiple step-stress accelerated life tests for the cumulative exposure model using complete and censored data. Assuming that the lifetimes of test item follow the Weibull and Gamma distributions, the maximum likelihood estimation and Bayesian approaches are used to estimate the parameters. Some proposed priors are investigated and compared under symmetric and asymmetric loss functions such as squared, linex and general entropy loss functions. It is also considered Type II censoring scheme. A sensitivity analysis of these Bayesian and maximum likelihood estimators are also presented by simulation Monte Carlo study, and a real data set is analyzed for illustrative purposes.","Bayesian, accelerated test, step-stess,Weibull, Gamma, cumulative exposure, maximal entropy prior, loss function, general entropy, Linex, MCMC."
1155,"In poor and more socially deprived areas, economic and social data are typically underreported. As a consequence, quantities of interest for, e.g., political, social and scientific purposes, such as income, rates of death and spread of diseases, tend to be underestimated. The great challenge, in those cases, is to build models able to provide reliable estimates for such quantities, despite the poor quality of data. In this paper we introduce a Bayesian model for mapping data that are subjected to underreporting. The usual practice to overcome the problem is to assume that data are censored. The proposed model considers count data in different areas, modeled using Poisson distributions, whereas prior information is used to build an appropriate distribution for the probability of underreport in each area. To illustrate the use of the proposed model, we map the early neonatal hospital mortality in the Minas Gerais state, a Brazilian state which presents heterogeneous characteristics and a relevant socio-economical inequality. Levels of functional illiteracy are then considered as covariates affecting underreporting. Finally, microregions are clustered according to their mortality rates and findings are compared with other studies.",nothing
960,"In this paper, a new Bayesian stochastic cure rate model, based on the individual frailty as the total number of
descendent cells of each damaged cell (known as the volume of the tumor) up to specic time and the repair mechanism (known as the destructive mechanism),  is formulated as a superposition of pure-birth stochastic processes (Yule processes). The proposed model deals with combined modeling of long-term eects (cure rate) and short-term eects (tumor growth) which can be regarded as a Bayesian alternative to the Cox regression and promotion cure rate models when the proportional hazards assumption is violated, for example, in the case of crossing survival functions (Broet et al., 2001; Yang & Prentice, 2005; Bennett, 1983). A simulation study and an application to a clinical melanoma data, using the Bayesian RStan package, are provided to illustrate the usefulness of the proposed model and also to evaluate the impact of cancer treatment over the long-term eect and the progression stage of the tumor (tumor growth). Besides this interesting new biological interpretation of the tumor growth, the model can be viewed as an extended Bayesian version of the destructive cure rate model of Rodrigues et al. (2010) and an alternative to the two-sample semiparametric model of Yang & Prentice (2005).","Frailty models, Cox regression models, Bayesian estimation, Stan sampler, Survival functions, Destruc-
tive random variables, Pure-birth processes, Yule process, Long-term eects, Short-term effects."
1318,"In this work we introduce the class of beta autoregressive fractionally integrated moving average models for continuous random variables taking values in the continuous unit interval $(0,1)$.
The proposed model accommodates a set of regressors and a long-range dependent time series structure. We derive the partial likelihood estimator for the parameters of the proposed model, obtain the associated score vector and Fisher information matrix.
We also prove the consistency and asymptotic normality of the estimator under mild conditions.
Hypotheses testing, diagnostic tools and forecasting
are also proposed.
A Monte Carlo simulation is considered to evaluate the finite sample performance of the partial likelihood estimators and to study some of the proposed tests.
An empirical application is also presented and discussed.",double bounded time series -  long-range dependence -  partial likelihood - asymptotic theory -  forecast
989,"In this work, we approach probabilistic aspects related to bingo. We carry out simulations that give us an intuition of how many numbers we expect to be drawn until one of the players completes a card and how much simultaneous wins occur. We take advantage of this motivation to present and discuss the use of negative hypergeometric distribution and the advantages of its teaching for an undergraduate probability course.",Statistical education; Negative hypergeometric; Probability; Bingo.
922,"We compare different confidence and credible intervals for the probability of success in a binomial model with respect to the coverage probability and expected length. The comparison is motivated by the similarity of a confidence interval proposed by Agresti and Coull (The American Statistician, 1998) and a Bayesian credible interval based on a Beta(2,2) prior distribution. Keeping in mind that confidence intervals are random and that credible intervals are numeric, we perform the comparison under the same paradigm, considering the Bayesian intervals (central and HPD) as realizations of random intervals or the latter as numeric intervals. The intervals are compared via simulation studies that show a better performance of the Wilson (score) and HPD intervals with uniform prior distribution and some advantages of Bayesian intervals with respect to the expected and posterior length.","Binomial distribution, coverage probability, expected length, Highest posterior density (HPD) intervals."
1130,"Principal component analysis (PCA) is often used in data decorrelation and dimensionality reduction. Due to its characteristic of energy compression in few main components, there are important applications of PCA in the context of image compression. However, its use in real applications is impossible due to the high computational cost required to calculate each input signal. In this context, this work proposes a low computational cost version for PCA through the application of the signal function in the elements of the matrix induced by PCA. As in the context of signal and image processing the PCA is known as the Karhunen-Loève transform (KLT), we call the transform proposed by signed KLT (SKLT). Emphasis is placed on length 4, 8, 16 and 32 transforms due to the use of these transform lengths in image and video coding, as in the JPEG and HEVC standards. Considering lengths 4, 8, 16 and 32, the SKLT was numerically evaluated using total coding and distance measurements for the exact PCA. Experiments in image compression are also presented.",nothing
1210,"Batch processes are often used in the production of a range of items. Such processes have a peculiar data structure and thus receive special attention from researchers in the theoretical development of more sophisticated approaches to their monitoring. During the course of each batch a large mass of sampled time series data from numerous involved variables are available, generating a tree-way data structure (batchs x variables x instants of time). The classical approaches available in the literature unfold this structure in two-way arrangements and use multivariate techniques for process modeling and control. More recent works use time series models, more specifically VAR models. However, the literature is still scarce and many developments remain open. This article presents a simulated study of an approach based on the pioneering work of Choi (2008), including some modifications in the original proposition. The good performance of the proposed approach is illustrated through a simulated batch process with different introduced disturbances.","Multivariate Statistical Control, Vector Autoregressive Model (VAR), Batch Processes"
1153,"Many financial decisions such as portfolio allocation, risk management, option pricing and hedge strategies are based on the forecast of the conditional variances, covariances and correlations of financial returns. The paper shows an empirical comparison of several methods to predict one-step-ahead conditional covariance matrices. The methods are assessed in terms of out-of-sample minimum variance portfolio return measures  on portfolios constructed by stocks from the S&P 500 index traded from January 2, 2000 to November 30, 2017. The results show that the DCC method using composite likelihood provides the best results according the standard deviation measure, the Risk Metrics 2006 method with non-linear shrinkage is the best in terms of information and Sortino's ratios and the Risk Metrics 1994 with non-linear shrinkage is the best according to an overall criterion.","Minimum variance portfolio, risk, shrinkage, S&P 500"
909,"In Brazil, the effects of the increase in the prison population on statistics on the labor market and income are poorly studied. The number of people deprived of liberty in 2014 is 6.7 times greater than in 1990, an average growth of 7% per year, while the Brazilian population grew by 16% in the period (average of 1.1% per year) . In a recent report by the Ministry of Justice, the prison population already exceeds 700 thousand inmates, occupying the third position in absolute numbers in the world. Jayedev and Bowles (2006) use the term guard labor to refer to efforts to protect the right to property and the distribution of advantages in the absence or incompleteness of contracts. They hypothesize that highly unequal or class-polarized economies as well as societies with strong political or ethnic divisions might need more guard labor. Bruce Western (2006) shows that the US penal system has produced invisible inequality in at least three directions: in official labor market data, in which official statistics significantly overestimate the prevalence of young blacks in employment; on the rates of non-occupation among young blacks, which are already high in official statistics, would only be between 20% and 25% of non-occupation if the prison population were considered, and due to the great racial disparity in the USA, the racial inequality in the market is significantly underestimated by official surveys. This work aims to bring to light other interpretations of mass incarceration in Brazil from the connection between the guard labor approach and the hidden population. The main analysis consists of a comparison between: prison population, free population and prison officers, in the 2010 Census sample.","Prison, hidden population, punitiveness, guard labor"
1067,"We proposed a new class of maximum a posteriori estimators for the parameters of the Gamma distribution. These estimators have simple closed-form expressions and can be rewritten as a bias-corrected maximum likelihood estimators presented by Ye and Chen (2017). A simulation study was carried out to compare different estimation procedures. Numerical results reveal that our new estimation scheme outperforms the existing closed-form estimators and produces extremely efficient estimates for both parameters, even for small sample sizes.",Gamma distribution; Bayesian Analysis; Maximum likelihood estimators; Closed-Form estimator.
972,"Motivated by the problem of actuarial risk allocation (the process of splitting the risk of a portfolio amongst its constituents), I will present algorithms for the computation of expectations conditional to rare events. The algorithms will be based on Sequential Monte Carlo methods and recent pseudo-marginal techniques will be used to perform parameter estimation together with the expectation calculation.","insurance, monte carlo, sequential monte carlo, particle filters"
1261,"The present work was developed in partnership with teachers from the Olympic Experimental Gymnasium (GEO), Santa Teresa Unit, aiming to contribute to the training of UNIRIO&#39;s licensors through their participation in activities aimed at teaching Statistics in Basic Education. In 2017, a group of seventh grade students came to our team with the desire to conduct a survey about alcohol consumption at school. Based on this theme, we plan, execute, analyze and are starting to disseminate the results obtained by this study at school. We used the action-research methodology, as we believe that this was an opportunity to make students reflect on the use of alcohol in adolescence. We use the same instrument on alcohol use as the national surveys of schoolchildren, which was developed by CEBRID. A total of 534 students responded to the survey. At the time of the survey, 63% (n=336) of respondents reported having tried alcohol at some point in their lives and 48% drank in the last 12 months. Among the respondents, 33% had already mixed alcoholic drinks with energy drinks. The National School Health Survey (PeNSE, 2016) showed a lower percentage than that found in our study for the proportion of students who have already tried alcohol (54%). An important result of this survey is not reflected in the figures presented above, but in the speeches by the students of 1704. Sometimes, during the research process, they came to confide that after they started the research they had stopped drinking. It was also possible to observe the empowerment of these students towards the school and all the dedication they had during the process of collecting and developing the research. Thus, we conclude that this research fulfilled the objective of developing statistical literacy and also modifying a reality.",nothing
1145,"In this work, based on Pagui et al. (2017), median unbiased estimators up to order n^{-2} in the Kumaraswamy distribution. Monte Carlo simulation studies were carried out to compare the performance of the estimators obtained with the maximum likelihood estimators and with the unbiased estimators obtained by the methods of Cox and Snell (1968) and of Firth (1993). Thanks to FAPEMIG for the financial support",Bias correction; Kumaraswamy distribution; maximum likelihood estimator; median bias
1168,"Generalized fiducial inference for the precision of a measuring instrument without available replications on the observations is our main interest.  In this work, we study two new estimation  procedures  for the precision parameters, the product variability and the mean of the quantity of interest under the Grubbs model considering the two-instrument case. One method is based on a fiducial generalized pivotal quantity and the other one is built on the method of the generalized fiducial distribution.  The behavior of the point and interval estimators is assessed numerically through Monte Carlo simulation studies. Comparisons with two existing frequentist approaches and five existing Bayesian approaches are reported.  Finally, the methodology is applied in the analysis of a data set from a methods comparison study.
","Fiducial distribution, Generalized fiducial inference, Generalized pivotal quantity,  Methods comparison."
1044,"In statistical analysis, particularly in econometrics, the finite mixture of regression models  based on the normality assumption  is  routinely used to analyze censored data. In this work, an extension of  this model is proposed by considering scale mixtures of normal distributions (SMN). This approach allows us to model data with great flexibility, accommodating multimodality and heavy tails at the same time.  The main virtue of considering the finite mixture of regression models for censored data  under the SMN class is that this class of models has  a nice hierarchical representation which  allows easy implementation of inferences. We develop a simple EM-type algorithm to perform maximum likelihood  inference of the parameters in the proposed model.  To examine the performance of the proposed method, we present some simulation studies and analyze a real dataset. The proposed algorithm and methods are implemented in the new R package CensMixReg.","Censoring, EM-type algorithm, Finite mixture of regression models, Scale mixtures of normal distributions"
1146,"In this work we study the problem of controlling the significance level of the Full Bayesian Significant Test (FBST) in the context of probability density estimation. For this, we show a Bayesian method for this probability density estimation and how the FBST should be conducted in this situation. We introduce the definition of the modified e-value which is an alternative way of calculating the FBST evidence measure and finally we present the results of a simulation study with different density distributions analyzing the behavior of the FBST power function and compare it with the test of Kolmogorov-Smirnov (KS). We observed that in unimodal densities the behavior of the power function of the tests is similar, when it comes to data from multimodal distributions, such as mixture distribution or involving trigonometric functions, the power of the FBST depends on the choice of a priori weights, so for some priors the power of the Bayesian test is greater than the KS and not others.","Hypothesis Testing, Bayesian Tests, Precise Hypotheses, Density Estimation, Power Function"
1063,"Functional data clustering procedures seek to identify subsets of curves with similar shapes and estimate representative mean curves of each such subset. In this work, we propose a new approach for functional data clustering based on a combination of a hypothesis test of parallelism and the test for equality of means. These tests use all observations, which come from an underlying functional model, to compute a measure that determines to which smoothed cluster center each subject’s data belongs. This measure is incorporated into a modified k-means algorithm to partition subjects into clusters and find the cluster centers. While competing algorithms require a fixed amount of smoothing for all curves, the proposed test-based procedure performs unsupervised clustering to curves with different degrees of smoothing. Extensive numerical experiments were examined and the results on simulated and real datasets suggest that the proposed algorithm outperforms other clustering approaches in most cases.",B-splines Parallelism Test-based k-means algorithm ANOVA t test
853,"A novel Bayesian nonparametric method is proposed for hierarchical modelling on a set of related density functions, where grouped data in the form of samples from each density function are available. Borrowing strength across the groups is a major challenge in this context. To address this problem, a hierarchically structured prior, defined over a set of univariate density functions using convenient transformations of Gaussian processes, is introduced. Inference is performed through approximate Bayesian computation (ABC) via a novel functional regression adjustment. The performance of the proposed method is illustrated via simulation studies and an analysis of rural high school exam performance in Brazil.",approximate Bayesian computation; nonparametric density estimation; Gaussian process prior; hierarchical models
1360,"
Brazil is ranked among the world’s most unequal countries and has high income disparities. Recent research shows that monetary policy has a redistribution effect that is non-homogeneous across individuals. Although it is true that individuals will save more when the interest rate is higher and will likely spend more otherwise, when one take into consideration that there are different marginal propensities to consume, the outcomes resulting of a monetary shock, for each type of agent, will vary. Besides the traditional channels (aggregate income and substitution channel), interest rates will also affect people depending on their income composition; prices and depending on their unhedged interest rate exposure.
 
In order to investigate the relationship of monetary policy on income inequality in Brazil for the period between 2000 to 2018, we used the capital-labor ratio as well as monthly data for GDP, inflation rate, exchange rate and interest rate in a bayesian time-varying autoregressive vector model with stochastic volatility. A time-varying framework allows to capture both changes in the coefficients, which can occur, for example, due to changes in the monetary authority behavior. Besides, the stochastic volatility setup allows to capture changes in the economy shocks.
 
We found out that the positive response of the capital-labor ratio to monetary shocks is significant and lasts more than a year, i.e., a contractionary shock leads to an increase in income inequality, even when considering for different periods. These results suggest that interest rate shocks have a non-negligible effect on inequality.",nothing
1140,"Basic Statistics disciplines are present in undergraduate curricula in various careers. In general, they comprise topics such as Descriptive, Probability and Inference that are developed in one or two semesters. A point that has been recurrent when evaluating the quality of student learning is the observation that many master the techniques of calculation, but have difficulties in understanding the concepts discussed. As a way to help alleviate these difficulties, I report the experience of including essay questions in different types of assessments, whether short tests or tests during class. Some examples are presented and we comment on the responses obtained.",assessment; basic statistics; statistics teaching
1002,"The objective of this work is to contribute to the evaluation of the comparability of the historical series of basic education indicators of the National Education Plan (PNE) that use IBGE data sources. In this sense, this study provides a comparative analysis, over time and in different geographic domains, of the estimates of some educational indicators calculated based on data from PNAD and PNAD Continuous between 2012 and 2015, a period in which the sample surveys were carried out in the field in parallel. Therefore, a brief presentation of the PNE, its indicators and data sources is made, followed by a presentation of the PNAD and PNAD Continuous, in particular their main differences. Then, the comparative analysis of the PNE educational indicators that can be estimated from the data of the two surveys, in the period 2012 to 2015. In general, in the period analyzed, the estimates obtained from the Continuous PNAD for the geographic domains evaluated show slightly better estimates than those obtained with the PNAD. If at the Brazilian level, the differences in the estimates of some Indicators fit within the confidence intervals, for more granular geographic domains this reality may not be verified. The results found are important in the analysis of the historical series over the 2000s and for the next few years, because, as discussed throughout the analysis, the differences between the estimates may result from methodological aspects and not from any other assumption that can be made. regarding a possible impact of government action. Thus, considering the above, it is expected that the effort of this work has provided subsidies to assist in the consistent monitoring of the PNE goals, allowing the assessment of possible discontinuities in the trajectory of the indicators, possibly attributing them to methodological issues.",Educational indicators; PNE; PNAD; PNAD Continuous;
920,"In most surveys there is no response. In the case of household surveys, this happens when the individuals or households selected for the sample do not provide the required information. This phenomenon needs to be considered when evaluating research quality. The R indicator – representative of the sample – allows us to assess the impact of non-response on the survey results. In this study, the R indicator is used to assess and analyze the representativeness of the sample of the Research on the use of Information and Communication Technologies in Brazilian Households (TIC Households), carried out by NIC.br, for the production of statistics by Federation Unit (UF). The research sample on ICT Households is originally planned to provide results by Major Regions, and it is not possible to ensure the quality of the sample for estimating statistics by FU. To fulfill the objective, the R Indicator methodology was used, which has the usual objective of quantifying the distance between the sample of respondents and the planned/selected sample to assess whether, even in the planning phase - before the collection operation, the The drawn sample was sufficiently informative and of quality for the production of statistics by FUs. The study presents comparative analyzes of the R indicator for different territorial levels and between different databases. The results of the estimated indicators showed that the sample of respondents and the planned sample of TIC Households can be considered representative at the level of Brazil and Major Regions. At the Federation Unit level, some FUs, however, show a gap between the planned samples and/or respondents and the target population. The results of this work will be incorporated into an estimation study in small areas for the ICT Household research focusing on the production of estimates for Federation Units.",Non-response. Indicator R. Representativeness of the sample. ICT Households.
974,"Joint models for longitudinal and time-to-event data are a powerful tool that take
into account these two data types simultaneously into a single model, allowing to
infer about the dependence and association between the longitudinal biomarker (e.g.
prostate-specific antigen, PSA) and time-to-event, for a better assessment of the effect
of a treatment. These models are useful for studies in the field of health that aim at
understanding the disease (e.g. prostate cancer), considering its development over
time and the amount of time until the patient reaches the absorbent state (e.g. death).
The most used joint models, that result from a combination of a longitudinal model
and a survival analysis, do not allow to monitor the link between the longitudi-
nal biomarker and the transitions between the multiple states of the disease until it
reaches the absorbent state. In order to better understand this link between the lon-
gitudinal biomarker and the transitions between the multiple states, in this paper we
use a joint model that combines the longitudinal model and the multi-state Markov
model. An application is presented where a data set from prostate cancer is consid-
ered. The parameters of the model are estimated by maximum likelihood, which is
performed in two stages: (i) in the first stage the fixed and random effects are esti-
mated based on the longitudinal biomarker PSA; and (ii) in the second stage those
estimates are used to link the longitudinal model with the multi-state Markov model,
allowing the measurement of the impact for the risk of death, considering demo-
graphic covariables, in each transition between the states of the disease along time.
In this way, the model is able to assess the biomarker’s trajectory, to define the risks
of transitions between health states, and to quantify the impact of the PS",Joint model; Longitudinal model; Muti-state Markov model; Prostate cancer
1181,"In this paper we introduce the Kumaraswamy autoregressive moving average models (KARMA), which is a dynamic class of models for time series taking values in the double bounded interval $(a,b)$ following the Kumaraswamy distribution. The Kumaraswamy family of distribution is widely applied in many areas, especially hydrology and related fields. Classical examples are time series representing rates and proportions observed over time. In the proposed KARMA model, the median is modeled by a dynamic structure  containing  autoregressive and moving average terms, time-varying regressors, unknown parameters and a link function. We introduce the new class of models and discuss conditional maximum likelihood estimation, hypothesis testing inference, diagnostic analysis and forecasting. In particular, we provide closed-form expressions for the conditional score vector and conditional Fisher information matrix. An application to environmental real data is presented and discussed.",ARMA; Double bounded data; Dynamic model; Forecasts; Kumaraswamy distribution
1086,"This work aims to investigate the relationship between perceived risk, in its performance and privacy dimensions, and the use of mobile banking by young people. To investigate the relationship, a set of 25 items was developed and applied to a sample of 350 young mobile banking users. Data were analyzed using exploratory and confirmatory factor analysis techniques. It was identified that the final model is formed by two factors, each one representing a dimension of the perceived risk. As a result, it was identified that items such as time to perform transactions, access to the banking application and internet connection have a high influence on risk perception among respondents. Understanding the factors that influence the risk perceived by customers can help banking institutions to design more efficient strategies to reduce risk perception.",Mobile banking. Perceived risk. Use of technology. Exploratory Factor Analysis. Confirmatory Factor Analysis.
1358,"Despite technological advances, landmine actions still affect the lives of many people and the search for efficient methods for detecting these mines becomes a relevant issue. Information from the UN Mine Action Service in 2014 indicated that an average of ten people were killed or mutilated by a landmine each day and there was still one mine for every 17 children in the world, posing a serious risk to the population. In addition, one in three victims of landmine explosions was a child. Thus, this dissertation proposes the Multivariate Image Analysis technique for the detection of landmines using thermal infrared (IR) images. Once the images are captured and processed, digital image processing and the Principal Component Analysis model, a tool used to optimize information processing, can be built. The design of the experiment was based on comparing the terrain, first with only sand and then adding the mine and other objects. Additionally, it was proposed to monitor the mine and objects through Hotelling&#39;s T² control chart. Decorrelation Enhancement Techniques were also used to try to reinforce the identification of the mine and other objects in the images and thus facilitate their respective identification. Experimental results showed that the proposed methods were efficient for both detection and monitoring of landmines.","Land mine, detection, multivariate image analysis"
988,"Single-base profiling of 5-methylcytosine (5-mC) and 5-hydroxymethylcytosine (5-hmC), requires combining data from DNA processing methods such as traditional (BS), oxidative (oxBS) or Tet-Assisted (TAB) bisulfite conversion. While other R packages provide 5-mC and 5-hmC maximum likelihood estimates (MLE) only for oxBS+BS combination, the R package MLML2R provides MLE also for TAB combinations. For combinations of any two of the methods, we derived the exact constrained MLE in analytical form, which greatly decreases computational time. For the three methods combination, we implemented the iterative method by Qu et al. (2013), and a non iterative approximation using Lagrange multipliers. The MLML2R package is freely available at https://CRAN. R- project.org/package=MLML2R.","DNA methylation, DNA hydroxymethylation, EM-algorithm, Maximum Likelihood Estimation, R package"
1234,"Proper treatment of rainfall data is important when identifying typical meteorological phenomena in a region, as it makes it possible to recognize patterns that can help in its management of water resources. In this sense, this work aims to fill in missing data for the Northeast Region of Brazil, from the period between 1993 and 2016, in this region there is little rainfall during the year, so it is necessary to model the behavior of these environmental data. For this, the estimation of missing data was made from the gamma mixture model with GPD tail varying over time (MGPDLMk), proposed by Nascimento (2016) and effective for this problem. Through the partial results of data imputations on the data series, it was possible to see that the model is effective for small intervals of missing data.","Missing data, Northeast region, Bayesian estimation, MCMC method."
1312,"The behavior of mortality decay has changed in recent years and hypotheses used by statistical models used for their modeling may need revisions. The Lee-Carter Model (Lee &amp; Carter, 1992), and its Bayesian version proposed by Pedroza (2006), assumes that the relative speed of mortality decline between different ages remains constant over time, which does not seem to be a reasonable hypothesis when considering recent data for mortality rates in the male population of the United States of America. This work proposes a model based on Pedroza (2006) that incorporates the temporal dynamics for these decay rates. This work also proposes a methodology for approximate estimation of this model, which is applied to data on the male population of the United States of America and whose results are compared with the methodology already existing in the literature.","Mortality, Bayesian Inference, Linear Dynamic Models"
1221,"The modeling of continuous variables belonging to an interval of type (0.1) is necessary in several applications, and allows the treatment of data such as proportions, rates and fractions. The development of regression models that fit these characteristics has been the main focus of many works published in recent decades. In the context of joint modeling for longitudinal and survival data, few studies pay attention to the use of adequate distributions for the longitudinal component, being commonly used the use of data transformations. In this work, we propose an extension of this type of joint modeling, through the use of beta regression models with mixed effects to explain the associated longitudinal measures. We started from a motivation from a study on quality of life and survival time, carried out in two Brazilian public hospitals specialized in cancer treatment: the Dr. Octávio Frias de Oliveira Cancer Institute (ICESP) and the Pio XII Foundation (Hospital do Cancer of Barretos). We used this data set to adjust the parameters of the proposed model, obtaining, among other results, an estimated measure of association between the two observed responses, currently considered an essential factor in the indication of surgeries or medical procedures.",Mixed beta model; joint models; Gauss-Hermite square; quality of life.
1131,"In this work we introduce a Weibull Extended Power Series (WSPE) regression model that is broader than the one approached by Morais and Ferrari (2017), modeling a shape parameter, in addition to the scale parameter. We present some properties of the WSPE distribution that justify the modeling of this shape parameter. We also consider censored data, more specifically right censorship, which is more frequent in practical situations. We have a discussion about long-lasting models. We applied the regression model to two real databases, one of them with censorship, to exemplify its potential uses. In the second application, we introduce a quantile ratio curve to interpret the results.","Regression, Survival Analysis, Quantil, Weibull"
1141,"Abstract – The Brazilian Institute of Geography and Statistics (IBGE) regularly produces the Annual Services Survey (PAS), an important survey of economic and financial information on the tertiary sector in the country. PAS produces sample estimates for the Federation Units of the South and Southeast regions at the four-digit level (class) of the National Classification of Economic Activities (CNAE). However, for some economic activities in the North, Northeast and Center-West regions, only direct estimates at the three-digit level (group) are released. Direct class-level estimates for these regions are not of acceptable precision. Furthermore, the strong asymmetry of economic data, with potential outliers, is an issue that must be addressed. To get around this problem, estimates in small domains based on Bayesian models are proposed, based on the methodology used by Moura et al (2017). In this article, the variable of interest is the gross revenue from services of companies in the right stratum of PAS (in the article by Moura et al (2017), the analysis is restricted to the sampled stratum). The model applied is the normal skew, identified in the article cited as the one with the best performance. The auxiliary variable used is the total salary from the PAS Basic Selection Registry. It should be noted that, although the right stratum is census, in some domains the non-response rate reaches high percentages, which justifies the use of estimation techniques in small domains for this case. After applying the model and analyzing the results, the conclusion is that the asymmetric normal model produces estimates with greater precision than direct estimation, for most of the domains in the right stratum.",nothing
1331,"As most pregnancy-related deaths and morbidities are clustered around the time of child birth, the quality of care during this period is crucial for mothers and their babies. To monitor the women at this stage, the partograph has been the central tool used in recent decades and, because of its simplicity, is frequently used in low-and middle-income countries. However, its use is highly questioned due to lack of evidence to justify a contribution to labor. In this paper we propose the use of multi-state models to explore the progression of the first stage of spontaneous labour, considering augmentation of labour as an intermediate state between vaginal delivery and cesarean section. Also, we considered information about cervical dilatation, membranes status, previous parity, comorbidity or pre pregnancy complications and if the woman was referred from another health facility. Due to the repeated assessments of cervical dilatation over time in labour monitoring, this variable was considered as fixed and time-dependent approaches.","BOLD project, labor modeling, markov process, multi-state model, time dependent covariates"
875,"In this work, we propose a new particle filtering algorithm for estimating the volatility of financial market prices according to the model by Harvey et al. of stochastic volatility with leverage. The proposed method is based on the algorithm of Djuric et al. which employs a Rao-Blackwellized particle filter, differing from this one in that it uses an approximation for the function of optimal importance, since the latter is intractable. The performance of the new method was evaluated through numerical simulations using synthetic data, in which an average performance superior to that of the Djuric method was observed.","Stochastic Volatility with Leverage, Particle Filter, Stochastic Filtration, Econometrics."
1031,"We proposed a new residual to be used in linear and nonlinear beta regressions. Unlike the residuals that had already been proposed, the derivation of the new residual takes into account not only information relative to the estimation of the mean submodel but also takes into account information obtained from the precision submodel. This is an advantage of the residual we introduced. Additionally, the new residual is computationally less intensive than the weighted residual. Recall that the computation of the latter involves an $n\times n$ matrix, where $n$ is the sample size. Obviously, that can be a problem when the sample size is very large. In contrast, our residual does not suffer from that. It can be easily computed even in large samples. Finally, our residual proved to be able to identify atypical observations as well as the weighted residual. We also propose new thresholds for residual plots and a scheme for the choice of starting values to be used in maximum likelihood point estimation in the class of nonlinear beta regression models. We report Monte Carlo simulation results on the behavior of different residuals. We also present and discuss two empirical applications; one uses the proportion of killed grasshopper  at an assays on a grasshopper Melanopus sanguinipes with the insecticide carbofuran and the synergist piperonyl butoxide, which enhances the toxicity of the inseceticid and other uses simulated data. The results favor the new methodology we introduce.",Beta regression; Diagnostic analysis; Empirical thresholds; Residual; Starting values;
913,"In this paper, we analyse the recent principal volatility components analysis procedure. The procedure overcomes several difficulties in modelling and forecasting the conditional covariance matrix in large dimensions arising from the curse of dimensionality. We show that outliers have a devastating effect on the construction of the principal volatility components and on the forecast of the conditional covariance matrix and consequently in economic and financial applications based on this forecast. We propose a robust procedure and analyse its finite sample properties by means of Monte Carlo experiments and also illustrate it using empirical data. The robust procedure outperforms the classical method in simulated and empirical data.",Conditional covariance matrix; Constant volatility; Curse of dimensionality; Jumps; Outliers; Principal components
1158,"There are many challenges to predicting the outcome of an election publicly. The Polling Data website (http://www.pollingdata.com.br/) was launched in 2014 for this purpose, using a polling aggregation model to make predictions. The first election monitored by the website was the 2014 Brazilian presidential election. At Sinape 2016, the difficulties encountered on that occasion and how they were resolved were discussed. However, one of the most relevant issues in an election with little information, as in the case of Brazil, is the methodological bias of each research institute. When there are few institutes publishing estimates at the national level, the weight of each institute in the final forecast of the model is great. In this way, institutes with a large bias towards a candidate can contribute a lot to the failure of the prediction model. In this presentation, we will discuss in more detail how to estimate the methodological bias of the institutes, and how to use this analysis to improve the Polling Data aggregation model predictions for the 2018 Brazilian presidential election. A by-product of this analysis is the creation of a ranking of institutes of Brazilian research. We will also discuss here the importance of publishing this ranking.","election polls, forecasting, Bayesian inference"
929,"The phenomenon of monotone likelihood is observed in the fitting process of
a Cox model when the likelihood converges to a finite value while at least one parameter
estimate diverges to infinity. Monotone likelihood primarily occurs in samples with sub-
stantial censoring of survival times and associated to categorical covariates. In particular
and more frequent, it occurs when one level of a categorical covariate has not experienced
any failure. A solution suggested by Heinze and Schemper (2001) is an adaptation of a
procedure by Firth (1993) originally developed to reduce the bias of maximum likelihood
estimates. The method leads to finite parameter estimates by means of penalized maxi-
mum likelihood estimation. In this case, the penalty might be interpreted as a Jereys
type of prior well known in Bayesian inference. However, this approach has some draw-
backs, especially biased estimators and high standard errors. In this paper, we explore
other penalties for the partial likelihood function in the  flavor of Bayesian prior distribu-
tions. An empirical study of the suggested procedures confirms satisfactory performance
of both estimation and inference. We also explore a real analysis related to a melanoma
skin data set to evaluate the impact of the different prior distributions as penalizations.
","Firth correction, MCMC, Partial likelihood, Survival analysis."
821,"The use of matching techniques to integrate data from two or more sources has become popular in the last 20 years. If, on the one hand, we saw the improvement of the probabilistic method proposed by Fellegi and Sunter and the development of a variety of other methods; on the other hand, the negligence of pairing errors and their impacts on the quality of derived estimates was asserted. Solutions that take into account some measure of matching error for the correction of bias in the estimation using paired data have been proposed, but have not yet been incorporated into the daily practice of paired data users, so many analyzes based on paired data are potentially misleading. An empirical study, including pairing of data from agricultural establishments and estimation of agricultural production values, reveals the presence of bias due to the pairing and shows the performance of regression methods that reduce such bias. Data from the Central Register of Companies of the IBGE and the State Secretariat of Finance of the States of Ceará, Maranhão, Paraíba and Santa Catarina were used. Two matching methods were used (Fellegi-Sunter and classification trees), for two levels of error in the data. Complete (census data) and incomplete (sample data) pairing was performed. Three estimation methods were compared: ordinary least squares (MQO), best linear unbiased estimator (BL) and maximum likelihood (EMV). The results correspond to average values in 200 simulations. Bias was observed in all matching scenarios, being, in most cases, smaller when the paired data results from the method based on classification trees and when the level of error is lower. For complete matching, BL leads to the smallest mean square error whenever the error level is greater; and for incomplete pairing, this measure is always smaller when using the EMV.",Agricultural statistics; pairing; Fellegi-Sunter; classification tree; pet; regression model.
895,"We introduce a broad and flexible class of multivariate distributions obtained by both scale and shape mixtures of multivariate skew-normal distributions. We present the probabilistic properties of this family of distributions in detail and lay down the theoretical foundations for subsequent inference with this model. In particular, we study linear transformations, marginal distributions, selection representations, stochastic representations and hierarchical representations. We also describe an EM-type algorithm for maximum likelihood estimation of the parameters of the model and demonstrate its implementation on a wind dataset. Our family of multivariate distributions unifies and extends many existing models of the literature that can be seen as submodels of our proposal.","EM-algorithm, Scale mixtures of normal distributions, Scale mixtures of skew-normal distributions, Shape mixtures of skew-normal distributions, Skew-normal distribution, Skew scale mixtures of normal distributions."
894,"In this article Stirling's formula is proved using the chi-square distribution and the central limit theorem. The purpose of this text is to present a short and simple demonstration of Stirling's formula from well-known probabilistic facts avoiding a long and tedious demonstration from purely mathematical arguments. Thus, this is a proof of Stirling's formula accessible to undergraduate students in the first year of statistics.",Central limit theorem; factorial approximation; Gamma function; normal distribution.
1200,"This article compares Value-at-Risk (VaR) and Expected Shortfall (ES) predictions obtained through the GAS model proposed by Creal et al. (2013) with alternative models. First, an in-sample investigation of the semiparametric GAS of Blasques et al. (2014) and a methodology for choosing the bandwidth. The empirical part is carried out through two distinct studies. The first focuses on exploring the predictive capacity of the semiparametric specification against parametric models. The second compares the various parametric models to assess whether increasing model complexity has a direct impact on the quality of predicting risk measures. The empirical study is conducted with eight indices, four international currency exchange rates and two crypto-currency exchange rates. The models are compared in terms of Quantile Loss by Gonzales-Rivera (2014) and using the MCS procedure by Hansen et al. (2011). The results indicate that the semiparametric models improve the prediction quality of the GAS(n) models for VaR and ES. More complex models such as Beta-Skew-t-EGARCH by Surracat and Harvey (2013) produce better results than more basic models. However, using the MCS procedure we find that in most cases the models are statistically equivalent in terms of their ability to predict VaR.","Generalized Autoregressive Score (GAS), GAS Semiparamétrico, Densidade Kernel,
Previsão de Volatilidade, Value-At-Risk (VaR), Expected Shortfall (ES)"
1166,"The relative frailty variance among survivors provides a readily interpretable measure of how the heterogeneity of a population, as represented by a frailty model, evolves over time.We discuss the properties of the relative frailty variance, show that it characterizes frailty distributions and that, suitably rescaled, it may be used to compare patterns of dependence across models and data sets. In shared frailty models, the relative frailty variance is closely related to the cross-ratio function, which is estimable from bivariate survival data. We investigate the possible shapes of the relative frailty variance function for the purpose of model selection, and we review available frailty distribution families in this context. We introduce several new families with contrasting properties, including simple but flexible time varying frailty models.The benefits of the approach that we propose are illustrated with an application to bivariate current status data obtained from serological surveys. A few recent advances will also be indicated. This is joint work with Paddy Farrington and Steffen Unkel.","shared frailty, current status data, relative frailty variance"
955,"Objectives: To model trajectories of visuospatial reasoning measured using Kohs Block Design test under realistic missing data assumptions and evaluate their association with hazard of death. Methods: A joint longitudinal-survival model was used to estimate trajectories of visuospatial reasoning under a missing not at random assumption of participants from the Origins of Variance in the Old–Old: Octogenarian Twins study. Sensitivity analyses to missing data assumptions were conducted. Results: Visuospatial reasoning declined at constant rate. Baseline age, dementia status, education, and history of stroke were associated with visuospatial reasoning performance, but only dementia was associated with its rate of decline. Importantly, our results demonstrated an association between poorer visuospatial reasoning and increased hazard of death. Baseline age and sex were associated with risk of death. Discussion: We confirmed an association between visuospatial reasoning and death under plausible missing data assumptions.","joint longitudinal-survival, missing not at random, cognitive decline"
1010,"In this work we propose a wavelet-based classifier method for binary classification. Basically, based on a training data set, we provide a classifier rule with minimum mean square error. Under mild assumptions, we present asymptotic results that provide the rates of con- vergence of our method compared to the Bayes classifier, ensuring universal consistency and strong universal consistency. Furthermore, in order to evaluate the performance of the proposed methodology for finite samples, we illustrate the approach using Monte Carlo simulations and real data set applications. The performance of the proposed methodology is compared with other classification methods widely used in the literature: support vector machine and logistic regression model. Numerical results showed a very competitive performance of the new wavelet-based classifier.","Generalized Classification, Wavelet Estimation, Nonparametric Regression"
1266,"In this paper we introduce the zero-adjusted Birnbaum-Saunders regression model. This new model generalizes at least seven Birnbaum-Saunders regression models. The idea of this modeling is mixing a degenerate distribution at zero with a Birnbaum-Saunders distribution. Besides the capacity to account for excess zeros, the zero-adjusted Birnbaum-Saunders distribution additionally produces an attractive modeling structure to right-skewed data. In this model, the mean and precision parameter of the Birnbaum-Saunders distribution and the probability of zeros can be related to linear and/or non-linear predictors through link functions. We derive a type of residual to perform diagnostic analysis and a perturbation scheme for identifying those observations
that exert unusual influence on the estimation process. Finally, two applications to real data show the potential of the model.",Birnbaum-Saunders distribution; Nonlinear Regression Models; Reparameterization; Zero-adjusted.
760,"Binary regression models are a popular set of statistical tools to analyze dichotomous responses, in the presence of covariates. 
Different topics are associated with these models, including the choice of appropriate link functions, estimation methods, diagnostic 
and residual analysis and  model comparison/validation tools. In addition, since the classical books of Hosmer and Lemeshow (1989) and Collet (2003) were published,  different extensions to asymmetrical/heavy tailed links, ordinal response, mixed models, item response theory and misclassification problems,  have been developed, even thought they are not all available in a single text. 
In this short course, we introduce frequentist and bayesian approaches to different topics of binary regression models, where applications are showed using R codes, as well as some important extensions are presented. Simulation studies and real data analysis are also discussed.

References
Hosmer, D. W. and Lemeshow, S. (1989). Applied logistic regression. Wiley, New York. 
Collet D. Modelling Binary Data 2nd ed. Chapman & Hall/CRC: Boca Raton, USA, 2003.","binary regression, generalized linear models, link function, residual analysis, model fit assessment tools"
789,"With the advancement of new technologies in obtaining data in different areas of knowledge, high-dimensional data analysis is increasingly frequent, this is when the number of parameters to be estimated is much greater than the number of observations. In general, classical statistical methods are not suitable for inference in this type of situation and new approaches have been developed. In recent years there has been a great methodological, mathematical and computational development that has enabled high-dimensional statistical inference based on certain notions of sparsity. These techniques are general enough to cover a wide range of models, such as linear and non-linear regression, autoregressive models and graphical models, among others. The objective of the short course will be to present current statistical inference techniques for high-dimensional data, providing theoretical foundations and practical application examples. Preliminary content: 1- Sub-Gaussian and sub-exponential random variables. 2- Linear regression model and least squares estimators. 3- High-dimensional linear regression and the “LASSO” estimator. 4- Linear approximations and &quot;oracle&quot; type inequalities. 5- Selection of variables. 6- Graphic models. Bibliography: 1- P. Bühlmann &amp; S. Van de Geer. Statistics for High-dimensional Data. Springer, 2011. 2- T. Hastie, R. Tibshirani &amp; J. Friedman. The Elements of Statistical Learning. 2nd edition. Springer, 2009. 3- C. Giraud. Introduction to High-dimensional Statistics. CRC Press, 2014. 4- I. Rish &amp; G. Grabarnik. Sparse Modeling: Theory, Algorithms and Applications. CRC Press, 2014.","sparsity, linear regression, LASSO, oracle inequalities, graphic models"
768,"The attempt to represent reality through models, mathematical or not, continues to be a great challenge for science which, decade after decade, has always sought to improve such tools. One of the most used mathematical modeling techniques is regression analysis, which has been updated in recent years due to the incorporation of factors that help to explain and understand the phenomena. Among these updates, we highlight the spatial regression treated globally and the spatial regression treated locally, in which the Geographically Weighted Regression (GWR) stands out. The latter differs from the former in that it analyzes the relationships between variables specifically for each unit of study, and not jointly, as is done in a global process. In this case, it is assumed that regions j closer to region i have greater influence on the estimates of the regression coefficients than regions further away. Thus, having a specific adjustment for each area, the final result is a better representation of the process as a whole. What justifies this analysis is the violation of the stationarity premise required by global models, which allows the latter to attribute the same relationship between variables for all units of study. Due to the heterogeneity of Brazilian municipalities, for example, the relationship between two or more variables is hardly the same for all regions of the country. Hence the need to work with more specific tools capable of providing a more detailed analysis, and thus evaluating the existing variability. Thus, the final result of an RGP model is a matrix of (nxk) of estimated parameters, being in the amount of data and k the amount of parameters, making it possible to verify the results through a map. The RGP Model began with the original work by Fotheringham et al. (1996) on movable windows and systematized in Fotheringham et al. (2002). Several other works have been dealing with this technique, such as those by Wheeler and Tiefelsdorf (2005) and Nakaya et al. (2005), which deal specifically with the problems of multicollinearity and counting data (poisson). The work by Silva and Rodrigues (2014) extended the RGP model to count data with overdispersion, using for this the negative binomial distribution, and the work by Silva and Lima (2017) extended the RGP model to interval-restricted data (0, 1) using the beta distribution. An interesting aspect of spatial models (whether local or global) is that in the absence of spatial dependence, the models present exactly the same results as non-spatial models, that is, from a spatial model it is possible to arrive at a non-spatial model, but from a non-spatial model is not possible to arrive at a spatial model. The short course will address the characteristics of the geographically weighted regression model, as well as its advantages and problems, in addition to what is currently being developed on the subject.","Spatial models, local regression, GWR"
798,"In regression models, we are interested in describing the relationship between a specific variable (response) and other characteristics. This relationship is commonly characterized by measures of central tendency, usually by the popular mean. Why not the median? For example, asymmetric data for many applications, such as geochemistry, are best approximated by the log normal distribution. However, it makes no sense to consider the mean on a logarithmic scale, as the additivity property is no longer valid, remembering that the logarithm mean is not the logarithm of the mean. In survival analysis, one rarely speaks of ``average survival&#39;&#39;. The median and quantiles reign in this area. By extending the idea to quantiles, quantile regression allows adjusting any quantile of the response variable as a function of a series of covariates, taking advantage of quantile properties such as robustness, invariance, among others. As a consequence, these models are more robust to the presence of outliers, do not need to make assumptions about the error distribution and offer a better graphical description of the data. The short course will be divided into two parts: in the first, we will explore the theory involving quantile regression models as well as the univariate case, where we will study a robust model considering errors with heavy tail distributions and also the case for interval responses. Quantile regression models of linear and non-linear mixed effects will be discussed in the second part of the short course. Applications for the proposed models will be presented using the packages \texttt{lqr}, \texttt{qrLMM} and \texttt{qrNLMM} available in \texttt{R}.","Regression, Quantile, Robustness, Interval Responses, Mixed Effects Models"
784,"In this workshop we will learn how to produce reports quickly, elegantly and of high quality using RMarkdown. RMarkdown is a text-to-HTML conversion tool that allows the export/transformation of R codes into static and dynamic outputs, including HTML, PDF or Microsoft Word documents, as well as slideshows, Dashboards and others. The reports in RMarkdown facilitate the sharing of data analysis results, as well as aid in the teaching/learning process of statistics, enabling the perfect connection between technical theory and data analysis methods with their practical application. In addition, RMarkdown is an important reporting automation tool allowing the reproduction or “recycling” of codes and encouraging the reproducibility of scientific research. It&#39;s a new way of working with R codes that provides a space for authorship and reproducibility in data science.","RMarkdown, R program, dynamic reports, RStudio"
767,"This workshop aims to introduce the basics of machine statistical machine learning. Fundamental elements of predictive models, such as risk and data splitting, will be discussed, as well as methods for estimating the regression function and for creating classifiers.",Machine Learning; Regression; Classification.
746,"We will present several practical examples showing how to analyze data from the Continuous National Household Sample Survey (PNADc) - carried out by the Brazilian Institute of Geography and Statistics (IBGE). Several tools available in the R system for analyzing survey data using complex sampling plans will be presented. The use of these tools will be illustrated considering the PNADc data to estimate indicators of living conditions for Brazilians. Resources for statistical modeling will also be demonstrated considering data from this important IBGE survey. During the explanation of the content, the importance of knowing: • the metadata that accompany and describe the data from the sample surveys will be reinforced; • the methods used to obtain and weight the samples; and also • the details of the methods required to carry out analyzes that adequately incorporate the effects of complex sampling.",Sampling; R; PNADC;
786,"Statistics, like all science, is dynamic and needs to be constantly updated. Among the various proposals for updating, we must rethink the old ways of presenting results. An example of this process is the disuse of graphics such as branches and leaves and the success of data visualization forms proposed by Hans Rosling in his excellent TED presentation, where he presents 200 years and 200 countries in 4 minutes through moving graphics. Therefore, in this tutorial we will present innovative ways to visualize data using the R program and its interfaces (Rcmdr and Rstudio), through tools such as Rmarkdown and the interface with Google&#39;s Application Programming Interface -Google API. OR is a powerful, free and open source development environment. It has an extremely active and collaborative user community. Among its interfaces, Rcmdr stands out for being very intuitive and Rstudio for being very versatile and multifunctional. OR has a series of data visualization and analysis tools traditionally used by the statistical community and, more recently, it has been making available tools such as Rmarkdown that allows the construction of documents such as HTML, PDF, DOC (word) and presentations. In this meeting, routines of the R program and their interfaces with other tools will be presented. More specifically, we&#39;ll build reporting and presentation using Rmarkdown, explore R&#39;s interaction with GoogleAPI, build motion charts, interactive maps, and other innovative ways to visualize data. Undoubtedly, different ways of presenting results add value to the final work, in some cases it automates the work and often allows a better understanding of the data. Finally, we must say goodbye to static and dull graphics and welcome to interactive and dynamic graphics.","Moving Charts, Interactive Maps, Google API, Pivot Tables, R Program"
790,"It is challenging to work with large databases in R. Depending on the analysis performed, the processing time is unfeasible and, in many cases, just reading the file becomes a problem. sparklyr is a package that integrates R with Spark, one of the most used big data tools today. With this package, it is possible to reduce the processing time, as the data is treated in a distributed manner locally on your PC or in an Amazon EMR cluster, via RStudio Server. In this way, learning to work with Big Data becomes more accessible, even when you don&#39;t have a large infrastructure available to deal with massive amounts of data.","Rstudio, Spark, sparklyr, Big Data e dplyr,"
756,"Abstract

In most practical applications of Bayesian inference it is necessary to use computer intensive and efficient methods as they involve
complex models. Markov chain Monte Carlo (MCMC) methods are now routinely employed but in many situations we need to improve
efficiency of such methods in terms of sampling parameter values close to the target distribution at an acceptable computational cost.

In this session, recent developments in computational methods, methodology for data analysis and applications in Bayesian statistics
will be presented.","Hamiltonian Monte Carlo, Langevin algorithms, shrinkage priors, slice sampling, diffusion processes."
793,"In this session, two works related to stochastic differential equations will be presented, and in one of them the equation studied involves fractional Brownian motion. The Brownian movement is also the subject of the third work, more specifically the Brownian Web, which appears as a limit to the scale of random walks.",Stochastic Differential Equations; Brownian motion; fractional Brownian motion; Brownian Web; scaling limit; random walks
739,"Currently, laboratory techniques in genetics have advanced rapidly, driven by an increasing amount of new types of datasets such as the 3rd generation of genetic markers (SNPs), the DNA sequence of numerous organisms, the genetic declaration information (profile of transcription) and an increasing amount of knowledge about the function of genes. And so, statistical genetics becomes dynamic in a multidisciplinary structure involving bioinformatics, biomathematics, biology, epidemiology and genetics, aimed at new developments such as linkage analysis methods, allelic association tests, analysis of gene indication matrix data, analysis of sequence, comparative genomics, reconstruction of phylogenetic trees, among others. In this context, new statistical challenges have emerged as laboratory techniques advance, pointing to the important role played by statisticians in the planning and analysis of genetic studies. The purpose of this activity is to address current genetics topics and their challenges in terms of statistical methods and models.",nothing
794,"High and infinite dimensional time series have been very popular topics in time series analysis recently. In this session, we will mainly discuss about three modelling approaches related to these topics: factor models, curve time series models and machine learning. In terms of factor models, if on one hand the joint analysis of a large number of time series enriches information about the phenomenon under study, on the other hand difficulties with the curse of dimensionality arise and variable dimension reduction techniques are in order. In this sense, factor models have proven to be a rich and important class of models, and state-of-the-art methods will be considered. Machine learning methods have more recently been adopted in the time series literature to face the challenges brought into play by high dimensional and big data related problems. Another subject that will be explored in this session is curve time series, or infinite dimensional time series, or yet functional time series. This encompasses problems where the data is curves observed over time, opposite to scalars or vectors. This approach leads to new ways of performing statistical analyses and forecasting, which will be here explored.","Time series
Large Dimension
Factor Models
Machine Learning
Functional Analysis
"
776,"Many methodological advances in statistics result from the analysis of healthcare data. As an example, strategies for controlling contagious diseases depend on statistical models that allow capturing the incomplete nature of epidemic data, the high dependence between infection events and also several factors that make it difficult to quickly detect ongoing epidemics. The objective of this thematic session is to present recent methodologies and innovative statistical applications in human and animal health problems, with a focus on disease transmission and survival analysis. In particular, new statistical models for survival data related to liver transplants and for surveillance data for a dengue epidemic will be presented. In addition, a large-scale genetic infection experiment, recently completed in a fish population, will be presented, whose statistical analysis results in the first evidence of genetic variation in the propensity of animals to transmit disease.",nothing
745,"In the modern world applications, time series analysis represents one of the most important topics in statistics, as data is being continuously generated in industry (e.g. sensor data, anomaly detection), social media (online searches, video/image/post comments) and finances (e.g. stock markets, GDP growth), among others. With the increase in the amount of data being collected, challenges such as computational time, data contamination and high dimensionality often arise. In this thematic session we will present a general overview and recent advances for some of the time series methodologies, both parametric and non-parametric. Applications to industry, business and finances are considered.","Time series
Singular spectrum analysis
Hamiltonian Monte Carlo
Adaptive LASSO"
795,"Coordinated by Mauren Porciúncula (FURG), and with the participation of Chris Franklin (ASA), Denise Britz (ENCE) and Lisbeth Cordani (USP), this roundtable aims to highlight the urgency of Statistical Education at all levels of education and in every country in the world. The trajectory of the speakers, their actions in academia, and on behalf of the community where they operate, aim to provide socialization with those present, the possibilities related to Statistical Education, and the urgency of addressing this theme.",Statistic; education
781,"The solid theoretical background of a bachelor in statistics is a great attraction in the job market, however, the advance in the use of large databases requires additional training in computing and brings new managerial challenges to statisticians. How to rethink the statistician&#39;s curriculum ensuring theoretical content and adding computational skills? What kind of additional training can both universities and the market offer?",Big Data Commutation Statistical Job Market
